---
title: 프로그래머스 인공지능 데브코스 2주차
date: 2022-09-26 14:00:00 +0900
categories: [프로그래머스 인공지능 데브코스 4기]
tags: [AI, Deep learning, Machine learning, 프로그래머스, 인공지능 데브코스, K-digital training]
description: 프로그래머스 인공지능 데브코스 2주차 강의 기록
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

## 1. HTTP 요청 주고 받기 - requests

### Jupyter Lab 시작하기

- Interactive한 python 코드 작성 / 공유를 위한 개발 도구
- Jupyterlab을 pip 명령어를 통해서 install
- Code cell (명령 모드), Markdown cell (입력 모드)

```python
### jupyterlab install
pip install jupyterlab

### 마크다운 문법
# 1. Header
# Hello world
## Hello world
### Hello world

# 2. Italic
*Hello world*
_Hello world_

# 3. Bold
**Hello world***
__Hello world__

# 4. Strikethrough (취소선)
~Hello world~

# 5. Unordered List
- Hello
- world

* Hello
* world

# 6. Ordered List
1. Hello
2. world

# 7. Code
`Hello world`

# 8. Code Block
```Hello world```
```

### 인터넷 사용자 간의 약속, HTTP

- 두 컴퓨터를 연결하는 **네트워크 (Network)**의 탄생
- 이 네트워크를 묶어서 **근거리 지역 네트워크 (Local Area Network, LAN)** 탄생
- 범지구적으로 연결된 네트워크인 **인터넷 (Inter Network, Internet)** 탄생
- 인터넷에서 정보를 교환할 수 있는 환경인 **www (World Wide Web)** 탄생
- 즉, 인터넷은 여러 컴퓨터끼리 네트워크를 연결한 것. 웹은 인터넷 상에서 정보를 교환하기 위한 시스템

- 웹에서 정보를 주고 받는 방법
    - 정보를 요청하는 컴퓨터를 **클라이언트 (Client)**, 정보를 제공하는 컴퓨터를 **서버 (Server)**
    - 클라이언트가 서버에게 정보를 요청
    - 요청에 대해서 서버가 작업을 수행
    - 수행한 작업의 결과를 클라이언트에게 응답

- HTTP (Hypertext Transfer Protocol)의 구조
    - 웹 상에서 정보를 주고 받기 위한 약속
    - 클라이언트에서 서버로 정보를 요청하는 것을 **HTTP 요청 (Request)**
    - 요청된 정보에 대해 서버가 클라이언트에게 응답하는 것을 **HTTP 응답 (Response)**

- HTTP로 정보 요청하기
    - GET / HTTP 1.1
    - HOST: www.programmers.com
    - User-Agent: Mozilla/5.0
    - HTTP / 1.1 200 OK

### 웹페이지와 HTML

- 웹 속에 있는 문서 하나는 **웹 페이지**, 웹 페이지의 모음은 **웹 사이트**
- 웹 브라우저는 HTML 요청을 보내고, HTTP 응답에 담긴 HTML 문서를 보기 쉬운 형태로 화면을 그려주는 역할
- 웹 페이지는 HTML 이라는 형식으로 되어 있고, 웹 브라우저는 HTTP 요청을 보내고, 응답받은 HTML 코드를 렌더링

- HTML (HyperText Markup Language) 구조
    - HTML 코드는 Head (문서의 정보 - 제목, 언어 등)와 Body (문서의 내용 - 글, 이미지, 동영상 등)로 나뉨
    - HTML은 여러 태그(Tag)로 감싼 요소(Element)의 집합으로 이뤄짐
    - 태그로 묶어서 글의 형식을 지정할 수 있음
    - 태그는 그에 맞는 속성 (attribute)을 갖을 수도 있음

### 나의 첫 HTTP 통신 코드

- `requests`는 python을 이용해서 간단히 HTTP 통신을 진행할 수 있는 라이브러리
- 정보를 달라고 요청하기, GET
- 정보를 갱신하는 것을 요청하기, POST

```python
### requests install
pip install requests
```
### 윤리적으로 웹 스크래핑, 크롤링 진행하기

- 웹 크롤링, 웹 스크래핑의 차이는?
    - 웹 크롤링은 크롤러 (Crawler)를 이용해서 웹 페이지의 **정보를 인덱싱**하는 것에 초점
    - 웹 스크래핑은 웹 페이지들로부터 우리가 **원하는 정보를 추출**하는 것에 초점

- 올바르게 HTTP 요청하기
    - 웹 크롤링, 웹 스크래핑을 통해 어떤 목적을 달성하고자 하는가? (저작권 이슈)
    - 내 웹 크롤링, 웹 스크래핑이 서버에 영향을 미치지는 않는가?
    - 로봇 배제 프로토콜 (Robots Exclusion Standard, REP): 크롤러들은 이 규칙을 지키면서 크롤링을 진행
    - `robots.txt`를 가져오는 방법은 웹 페이지 주소에 `/robots.txt`를 붙이면 됨
        - `www.naver.com/robots.txt`
        - `www.programmers.com/robots.txt`

## 2. 똑똑한 HTML 분석기 - BeautifulSoup4

### 웹 브라우저가 HTML을 다루는 방법
- Document Object Model (DOM)
    - 문서를 렌더링하는 가장 최초의 단계
    - 브라우저의 렌더링 엔진은 웹 문서를 로드한 후에 **파싱**을 진행

- DOM의 목적
    - DOM: Document, html, head, body, style, ul, li ...
    - 각 노드를 객체로 생각하면 문서를 더욱 편리하게 관리할 수 있음
    - DOM Tree를 순회해서 특정 원소를 추가할 수 있음 / 찾을 수 있음

- 브라우저는 왜 HTML을 DOM으로 바꿀까?
    - 원하는 요소를 동적으로 변경해줄 수 있음
    - 원하는 요소를 쉽게 찾을 수 있음
    - 브라우저는 HTML을 파싱해서 DOM을 생성하므로, 이를 바탕으로 요소를 찾을 수 있음
    - 따라서 파이썬으로 HTML을 분석하는 HTML Parser가 필요함!

### HTML을 분석해주는 BeautifulSoup
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- 크롬 브라우저의 개발자 도구를 이용하면, HTML 코드를 확인할 수 있음

### HTML의 Locator로 원하는 요소 찾기
- `tagname`: 태그의 이름
- `id`: 하나의 고유 태그를 가리키는 라벨
- `class`: 여러 태그를 묶는 라벨

## 3. 웹 브라우저 자동화 - Selenium

### 동적 웹 페이지와의 만남

- 정적 웹 사이트와 동적 웹 사이트
    - HTML 내용이 고정된 정적 (Static) 웹 사이트
    - HTML 내용이 변하는 동적 (Dynamic) 웹 사이트 - 인스타그램 등
    - 정적 웹 사이트는 HTML 문서가 완전하게 응답됨
    - 동적 웹 사이트는 응답 후 HTML이 렌더링 될 때까지 **지연시간**이 존재
    - 웹 브라우저에서는 **자바스크립트**라는 프로그래밍 언어가 동작하며, `비동기 처리`로 필요한 데이터를 채움
    - `동기 처리`: 요청에 따른 응답을 기다리는 것 -> 렌더링이 마무리 되어야만 데이터 처리가 됨
    - `비동기 처리`: 요청에 따른 응답을 기다리지 않음 -> 렌더링과 데이터 처리가 같이 이뤄지게 됨
    - 따라서 비동기 처리가 된 경우에, 상황에 따라 **데이터가 완전하지 않는 경우**가 발생할 수 있음

- 웹 브라우저와 파이썬의 만남
    - 웹 브라우저를 자동화하는 라이브러리 `Selenium`
    - 응답 후 **시간**을 지연시킬 수 있음
    - UI와 **상호작용**이 가능함

- 동적 웹 사이트는 응답 후 바로 정보를 추출하기 어려움
- 다양한 키보드 입력과 마우스 클릭 등의 상호작용이 필요함
- 이를 해결하기 위해 웹 브라우저를 Selenium을 이용하여 파이썬으로 조작하는 전략을 취함

### 브라우저를 자동화하기, Selenium

- Selenium은 파이썬을 이용하여 웹 브라우저를 조작할 수 있는 자동화 프레임워크

### Wait and Call

- Selenium은 동적 웹 사이트에 대한 지원을 진행하기 위해 명시적 (Explicit) / 암묵적 (Implicit) 기다림이 있음
- 명시적 기다림 (Explicit wait): 다 로딩이 될 때까지 지정한 시간 동안 기다리기 (다 로딩이 될 때까지 5초 동안 기다려)
- 암묵적 기다림 (Implicit wait): 특정 요소에 대한 제약을 통한 기다리기 (이 태그를 가져올 수 있을떄까지 기다려)

### 마우스 및 키보드 이벤트 처리하기

- [Selenium Documentation](https://www.selenium.dev/documentation/)

## 4. 시각화로 결과 요약하기 - Seaborn

### 시각화 라이브러리, Seaborn

- scraping의 결과가 너무 분산되어 있으므로, **시각화**로 표현해보도록 함!
- matplotlib을 기반으로 하는 시각화 패키지, Seaborn
- 다양한 그래프를 고수준에서 쉽게 그릴 수 있음

### 뭉게뭉게 단어구름, Wordcloud

- 자연어 문장에서 키워드를 추출하여 해당 키워드의 빈도 수를 측정
- 앞에서 전처리한 정보와 Wordcloud 라이브러리를 바탕으로 워드클라우드 생성

## 5. 인공지능 수학

<br/>
<br/>

> 출처: 프로그래머스 인공지능 데브코스 4기 2주차 강의 -> [강의 내용 정리 깃허브 링크](https://github.com/Paul-scpark/AI-dev-course/tree/main/1%EC%A3%BC%EC%B0%A8)