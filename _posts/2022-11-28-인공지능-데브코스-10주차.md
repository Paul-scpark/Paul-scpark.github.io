---
title: 프로그래머스 인공지능 데브코스 10주차 정리 및 후기
date: 2022-11-28 00:00:00 +0900
categories: [Education, 프로그래머스 인공지능 데브코스 4기]
tags: [AI, Deep learning, Machine learning, 프로그래머스, 인공지능 데브코스, K-digital training]
description: 프로그래머스 인공지능 데브코스 10주차 강의 기록
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

이번 글에서는 프로그래머스 인공지능 데브코스의 10주차 강의에 대한 정리입니다. <br/>
딥러닝의 심화 내용과 함께 CNN과 RNN에 대하여 학습합니다.

---

## 1. 딥러닝 기초
- 영상 분류: 과거에는 매우 어렵고, 도전적 문제
- ImageNet: 2만 2천여 부류에 대해 수백~수만장의 사진을 인터넷에서 수집하여 1500만여 장의 사진을 구축 및 공개
- ILSVRC (ImageNet Large Scale Visual Recognition Competition) 대회 - CVPR 학술대회에서 개최
    - 1000가지 부류에 대해 분류, 검출, 위치 지정 문제
    - 120만 장의 훈련 집합, 5만 장의 검증 집합, 15만 장의 테스트 집합
    - 우승: AlexNet (2012) -> GoogleNet & VGGNet (2014) -> ResNet (2015)
- 우승한 모델은 코드와 학습된 가중치를 공개하여 널리 사용되는 표준 신경망이 됨

### AlexNet
- 컨볼루션층 5개와 완전 연결 (Fully Connected, FC) 층 3개
    - 8개 층에 290300-186624-64896-43264-4096-4096-1000개의 노드 배치
- 컨볼루션층은 200만개, FC층은 6500만개 가량의 매개 변수
    - FC층에 30배 많은 매개변수 (향후 CNN은 FC층의 매개변수를 줄이는 방향으로 발전함)
- 구조
    - 당시 GPU의 메모리 크기 제한으로 인해 GPU 1, 2로 분할하여 학습 수행
    - 3번째 컨볼루션층은 두 개의 결과를 함께 사용 (Inter-GPU Connections)
    - 컨볼루션층 큰 보폭으로 다운 샘플링
- 학습에 성공한 요인
    - 외적요인: ImageNet 이라는 대규모 사진 데이터, GPU를 사용한 병럴 처리
    - 내적요인: 활성 함수로 ReLU 사용, 지역 반응 정규화 기법 적용, 과잉적합 방지하는 규제 기법 적용
        - 인간 신경망 측면 억제 모방, ReLU 활성화 규제
        - 데이터 확대 (잘라내기 - Cropping, 반전 - Mirroring으로 2048배로 확대)
        - 드롭아웃 (완전연결층에서 사용함)
    - 테스트 단계에서 앙상블 적용: 입력된 영상을 잘라내기와 반전을 통해 증가시켜서 2~3% 만큼 오류율 감소 효과

### VGGNet
- 3*3의 작은 커널을 사용함
    - GoogleNet의 인셉션 모듈처럼 이후 깊은 신경망 구조에 영향
    - 큰 크기의 커널은 여러 개의 작은 크기 커널로 분해 될 수 있음
    - 매개변수의 수는 줄이면서 신경망은 깊어지는 효과
        - 5 by 5 커널을 2층의 3 by 3 커널로 분해하여 구현
            - 5 by 5 커널의 매개변수는 5 * 5 = 25
            - 3 by 3 커널의 매개변수는 9 + 9 = 18
        - 3 by 3 커널을 1 by 3 커널과 3 by 1 커널로 분해하여 구현
            - 3 by 3 커널의 매개변수는 3 * 3 = 9
            - 1 by 3 커널의 매개변수는 1 * 3 = 3
            - 3 by 1 커널의 매개변수는 3 * 1 = 3
            - 따라서 기존 9개보다 6개로 줄어드는 효과를 볼 수 있음
            - 결국, 최종적으로 n이 클수록 매개변수의 수는 줄어드는 효과가 있음
- 신경망을 더욱 깊게 만듦 (신경망의 깊이가 어떤 영향을 주는지 확인)
- 컨볼루션층 8~16개를 두어 AlextNet의 5개에 비해서 2~3배 깊어짐
- 1 by 1 커널: VGGNet은 적용 실험만 하고, 최종 선택은 안함 (GoogleNet에서는 사용됨)
- 차원 통합, 차원 축소 효과

### GoogleNet
- 핵심은 인셉션 모듈 (Inception) - 총 9개의 인셉션 모듈을 포함
- Conv 레이어를 sparse 하게 연결하고, 행렬 연산은 dense 하게 처리함
- 수용장의 다양한 특징을 추출하기 위해 NIN의 구조를 확장하여 복수의 병렬적인 컨볼루션 층을 가짐
    - NIN 구조는 기존 컨볼루션 연산을 MLPConv 연산으로 대체하는 것
        - 커널 대신 비선형 함수를 활성 함수로 포함하는 MLP를 사용하여 특징 추출에 유리
        - 신경망의 미소 신경망 (Micro NN)이 주어진 수용장의 특징을 추상화 시도
        - 전역 평균 풀링 (Global Average Pooling) 사용
            - FC층 대신 Global Average Pooling을 사용
            - 전 층에서 나온 특징 맵들을 각각 평균 낸 것을 이어서 1차원 벡터 생성
            - FC층을 사용했을 때에 비해, 가중치의 개수를 상당히 줄일 수 있었음
- GoogleNet은 NIN 개념을 확장한 신경망  
    - 인셉션 모듈: 마이크로 네트워크로 MLPConv 대신 네 종류의 컨볼루션 연산 사용 (다양한 특징 추출)
    - 1 by 1 컨볼루션을 사용하여 차원 축소: 매개변수의 수 (특징 맵의 수)를 줄임, 깊은 신경망
    - 3 by 3, 5 by 5 같은 다양한 크기의 컨볼루션을 통해 다양한 특징을 추출
- 매개변수가 있는 층 22개, 없는 층 (풀링) 5개로 총 27개 층
- 완전 연결층은 1개에 불과 함 (1백만 개의 매개변수를 가지고, VGGNet의 완전 연결층에 비하면 1% 수준)
- 두 개의 보조 분류기 추가
    - 네트워크가 깊어지면서 발생하는 기울기 소실 문제를 줄이기 위해 추가
    - 원 분류기의 오류 역전파 결과와 보조 분류기의 오류 역전파 결과를 결합하여 경사 소멸 문제 완화
    - 학습할 때 도우미 역할을 하고, 추론할 때 제거됨

### ResNet
- 잔류 (잔차) 학습이라는 개념을 이용하여 성능 저하를 피하면서 층 수를 대폭 늘림
- 지름길 연결을 두는 이유?
    - 깊은 신경망도 최적화가 가능해짐
    - 단순한 학습의 관점 변화를 통한 신경망 구조 변화
    - 단순 구조의 변경으로 매개변수 수에는 영향이 없음
    - 덧셈 연산만 증가하므로, 전체 연산량 증가도 거의 미비
    - 깊어진 신경망으로 인해 정확도 개선 가능
    - 경사 소멸 문제 해결
- VGGNet과 같은 점: 3*3 커널 사용
- VGGNet과 다른 점: 잔류 학습 사용, 전역 평균 풀링 사용 (FC층 제거), 배치 정규화 적용 (Dropout 필요 없음)


## 2. CNN Models

## 3. 딥러닝 최적화

## 4. RNN


---

<br/>
<br/>

> 출처: 프로그래머스 인공지능 데브코스 4기 10주차 강의 -> [강의 내용 정리 깃허브 링크](https://github.com/Paul-scpark/AI-dev-course/tree/main/10%EC%A3%BC%EC%B0%A8)