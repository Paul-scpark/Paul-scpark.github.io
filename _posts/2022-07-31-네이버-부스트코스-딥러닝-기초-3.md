---
title: 딥러닝 기초 다지기 - 3강. CNN (Convolutional Neural Network)
date: 2022-07-31 01:00:00 +0900
categories: [네이버 부스트코스, 딥러닝 기초 다지기]
tags: [AI, 네이버 부스트코스, 파이썬, Machine learning, 딥러닝, CNN]
description: 네이버 부스트코스 - 딥러닝 기초 다지기 3강
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

* [부스트코스 강좌 링크](https://www.boostcourse.org/ai111/lecture/1156340)

## CNN (Convolutional Neural Network)
- Continuous convolution
- Discrete convolution 
- 2D image convolution
  - 3x3 filter * 7x7 image = output 5x5
- RGB Image convolution
  - 5x5x3 filter * 32x32x3 image = 28x28x1 feature
  - 32x32x3 image * four 5x5x3 filters = 28x28x4 feature

- CNN consists of convolution layer, pooling layer, and fully connected layer
  - Convolution and pooling layers: feature extraction
  - Fully connected layer: decision making (classification)

### Stride, Padding
- Stride가 1이라면 한칸씩 이동하면서, 2라면 두칸씩 이동하면서 데이터를 확인하는 것
- Zero padding이 들어갈수도 있음

### Convolution Arithmetic
- Padding 1, Stride 1, 3x3 Kernel
  - W: 40, H: 50, C: 128
  - Number of parameters: 3 * 3 * 128 * 64 = 73728

### 1x1 Convolution
- 256x256x128 -> 256x256x32 (CONV 1x1x128x32)
  - Dimension reduction: 채널의 수를 줄일 수 있음
  - Convolutional layer를 더 깊게 쌓으면서, parameters의 수를 줄일 수 있음 (bottleneck architecture)

## Modern Convolutional Neural Networks
- 결론적으로는 네트워크의 depth는 점점 깊어지고, parameters의 수는 점점 줄어가고, 성능은 개선됨
- ILSVRC (ImageNet Large-Scale Visual Recognition Challenge)
- AlexNet: 5 Convolutional layers, 3 Dense layers
  - Rectified Linear Unit (ReLU) activation: 
    - Preserves properties of linear models
    - Easy to optimize with gradient descent
    - Good generalization
    - Overcome the vanishing gradient probelm
  - GPI implementation (2 GPUs)
  - Local response normalization, overlapping pooling
  - Data augmentation
  - Dropout
- VGGNet: Repeated 3x3 blocks - Increasing depth with 3x3 convolutional filters (with stride 1)
  - 5x5를 사용하는 것보다, 3x3을 두 번 사용하는게 parameters의 수를 줄이는데 도움이 됨
- GoogLeNet: 1x1 convolution - Inception blocks
  - Reduce the number of parameters
  - 1x1 convolution can be seen as channel-wise dimension reduction
  - 3x3x128x128 = 147,456 vs 1x1x128x32 + 3x3x32x128 = 40,960
  - 1x1 convolution enables about 30% reduce of the number of parameters
- ResNet: Skip connection - Deeper neural networks are hard to train
  - Overfitting is usually caused by an excessive number of parameters
  - But, training error가 더 작음에도 불구하고, testing error가 더 커지는 한계가 존재함
  - Add an identity map after nonlinear activations (skip connection)
    - Simple Shortcut, Projected shortcut
    - Batch normalization after convolutions, Bottleneck architecture
    - Performance increases while parameter size decreases
- DenseNet: Concatenation - It uses concatenation instead of addition
  - Dense block
    - Each layer concatenates the feature maps of all preceding layers
    - The number of channels increases geometrically
  - Transition block
    - Batch Norm -> 1x1 Conv -> 2x2 Avg Pooling
    - Dimension reduction

## Computer Vision Applications

### Semantic Segmentation
- 특정 사진에 대하여 Person, Bicycle, Background를 구분 (자율주행)
- Fully Convolutional Network
  - Image -> Conv -> Conv -> flat -> Dense -> Label
  - Dense layer를 없애고, fully convolutional network를 구축해보자! (Convolutionalization)
  - Transforming FCL into convolution layers enables a classification net to output a heat map
  - While FCN can run with inputs of any size, the output dimensions are typically reduced by subsampling
  - So, we need a way to connect the coarse output to the dense pixels
  - Deconvolution (Conv transpose)

### Detection
- R-CNN: Input image -> Extract region proposals -> Compute CNN features -> Classify regions
- SPPNet: 속도를 빠르게 하기 위해 이미지 안에서 CNN을 한번만 돌리도록 로직을 개선 
- Fast R-CNN: Input image -> Bounding box -> Generated CNN features -> ROI pooling -> Outputs (Class, Bounding box regressor)
- Faster R-CNN: Region Proposal Network + Fast R-CNN
  - Anchor boxes: Detection boxes with predefined sizes
- YOLO: It is an extremely fast object detection algorithm
  - It simultaneously predicts multiple bounding boxes and class probability
    - No explicit bounding box sampling (compared with Faster R-CNN)
  - Given an image, YOLO divides it into SxS grid
    - Each bounding box predicts box refinement, confidence of objectness
    - Each cell predicts C class probability
    - In total, it becomes a tensor with SxSx(B*5+C) size