---
title: LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)
date: 2023-01-18 00:00:00 +0900
categories: [Education, LG Aimers 2기]
tags: [AI, Deep learning, Machine learning]
description: LG Aimers AI 전문가 과정 강의 기록
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

이번 글에서는 LG Aimers의 AI 전문가 과정에서 딥러닝에 대한 기본 개념과 대표적인 모형의 학습 원리를 학습합니다. 특히, 이미지와 언어 모델 학습을 위한 딥러닝 모델과 학습 원리를 배우게 될 것입니다.

---

## 1. Introduction to DNN

- Artificial Intelligence > Machine Learning > Deep learning
- 하나하나의 뉴런들이 모여서, 하나의 신경망을 구성 (여러 Layers를 가지고 있음) = DNN
- DNN 적용을 위해서는 빅데이터, 컴퓨팅 성능, 진보된 알고리즘 모델이 필요함
    - 영상 인식, 이미지 합성, 기계 번역, 챗봇, 자연어 처리, 주식 가격 예측 등에 적용
- 퍼셉트론 (Perceptron): y = f(w0 + w1x1 + w2x2)
    - AND, OR, XOR Gate
    - XOR Gate는 Single layer 퍼셉트론으로는 결과값을 낼 수 없음
    - Input layer - Hidden layer - Output layer (2-layer neural network)
    - Input layer - Hidden layer1 - Hidden layer2 - Output layer (3-layer neural network)
- Forward Propagation, Activation Function (Sigmoid, Softmax), Loss Function (MSE, Cross entropy loss)

## 2. Training NN

- Gradient Descent: Loss Function을 최소화 시킬 수 있는 최적의 파라미터 값을 업데이트 하는 과정
    - Loss function이 복잡한 경우에는, 수렴 속도가 늦어지는 경우도 존재
    - 따라서 Original Gradient Descent 알고리즘을 다양한 형태로 변화시킴 (Momentum, Adagrad, Adam 등)
- NN의 학습 과정
    1. 가장 처음으로는, 학습 데이터에 대해 Random Initialize 된 값으로 Forward Propagation을 수행
    2. 그 상태에서 Loss Function의 값을 최소화 시킬 수 있는 파라미터를 찾아나감 (Backward Propagation)
- Gradient Vanishing을 해결하기 위해 Tanh, ReLU 활성화 함수 등을 사용하게 됨
- Batch Normalization (배치 정규화)
    - 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도, 배치 별로 평균과 분산을 이용해 정규화 해주는 것
    - 활성화 함수의 출력값을 정규화 하여 그 분포를 고르게 해주는 효과

## 3. CNN and 이미지 분류

- Fully Connected NN, Convolution NN (CNN - 이미지 처리에 사용), Recurrent NN (RNN - 시계열 데이터에 사용)
- CNN은 특정 클래스에 존재할 수 있는 작은 특정 패턴들을 정의하고, 패턴들이 주어진 이미지에 있는지를 판단
- 매칭의 정도를 나타내는 결과값을 활성화 지도 (Activation map)이라고 하고, 이는 특정 Convolution Filter를 주어진 입력 이미지에 가능한 모든 위치에 오버랩을 시켜서 매칭되는 정도를 나타낸 것
- Channel, Filter, Pooling Layer (Max, Average)
    - 일반적인 구조: Conv - ReLU - Conv - ReLU - Pooling - Conv - ReLU - Pooling - FC
- Hyperparameters
    - Convolution: Number of filters, Size of filters
    - Pooling: Window size, Window stride
    - Fully Connected: Number of layers, Number of neurons
- CNN 모델은 어느 종류의 layer를 어느 위치에, 어느 순서로 배치하는가가 중요한 문제. 이에 대하여 많은 연구자들이 CNN 모델에 대한 architecture를 사전에 정의한 것들이 있음 = AlexNet, VGGNet, GoogleNet, ResNet
    - VGGNet: 각각의 Conv layer에서 사용하는 필터의 가로, 세로 size를 3 by 3으로 고정하고, layer를 깊게 쌓아서 문제를 해결한 알고리즘
    - ResNet (Residual Network): Conv layer를 통해 나온 output에 layer를 또 추가하는 것. layer를 추가적으로 쌓아서 생기는 비효율성에 대하여 일부 layer는 skip 할 수 있도록 하는 skip connection을 추가한 알고리즘
- CNN 알고리즘의 꾸준한 성능 개선과 함께, 모델의 layer 개수도 꾸준히 증가 됨 (AlexNet은 8개, ResNet은 152개 층)

## 4. Seq2Seq with Attention

- Recurrent Neural Network (RNN)
    - one to one, one to many, many to one, many to many
- RNN 모델의 기울기 소실 및 폭발의 문제를 해결하기 위한 LSTM, GRU 모델 등장
- Seq2Seq, Encoder & Decoder
    - Original Seq2Seq 모델은 매 time step 마다 생성되는 RNN의 각 time step의 Hidden state vector는 같은 dimension으로 이뤄져야 한다는 제약 조건이 있음. Output vector가 다시 Input vector로 사용되기 위해. 이러한 경우에 축적해야 되는 정보가 시간이 길어짐에 따라서 점점 많아지지만, 정보는 항상 똑같은 개수의 dimension에 저장하게 되어 정보를 유실하게 된다는 한계점이 있음
    - 이러한 Bottleneck 문제를 해결하고자 attention 이라는 추가적인 모델이 Seq2Seq에 도입
    - 입력 sequence에 주어지는 데이터를 encoder에서 인코딩 한 후, decoder에서는 encoder의 마지막 time step의 Hidden state vector 만을 입력으로 받지 않고, 그 입력과 더불어 decoder의 각 time step에서 encoder에서 나온 여러 인코딩 데이터 중 필요로 하는 것을 가지고 가서 예측에 사용
    - 기본적 구조는 encoder와 decoder가 존재하고, encoder의 마지막 time step의 Hidden state vector가 decoder의 가장 최초의 Hidden state vector인 h0로 사용됨. 그런데 여기서 어떤 decoder의 각 time step에서 추가적으로 encoder에 있는 여러 Hidden state vector로부터 필요한 정보를 취사선택해서 가장 유관하다고 생각하는 정보를 추가적인 입력으로 사용함

## 5. Transformer

- Transformer 모델은 Seq2Seq with attention 모델의 개선된 버전
    - Seq2Seq with attention 모델에서 Encoder와 Decoder가 RNN 기반의 모델로 구성됨
    - Transformer 모델은 Encoder와 Decoder에서도 attention 기반으로 동작하는 모델
        - RNN 기반의 모델들은 Long-term Dependency 문제가 있음 (오랜 시계열을 거쳐 정보가 소실 될 수 있음)
        - Transformer에서는 가까이 있거나 멀리 있는 정보를 접근해서 필요한 정보를 사용할 수 있게 됨
    - Self-attention, Multi-head Attention

## 6. Self-supervised Learning