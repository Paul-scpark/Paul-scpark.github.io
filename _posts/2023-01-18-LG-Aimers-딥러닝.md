---
title: LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)
date: 2023-01-18 00:00:00 +0900
categories: [Education, LG Aimers 2기]
tags: [AI, Deep learning, Machine learning]
description: LG Aimers AI 전문가 과정 강의 기록
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

이번 글에서는 LG Aimers의 AI 전문가 과정에서 딥러닝에 대한 기본 개념과 대표적인 모형의 학습 원리를 학습합니다. 특히, 이미지와 언어 모델 학습을 위한 딥러닝 모델과 학습 원리를 배우게 될 것입니다.

---

## 1. Introduction to DNN

- Artificial Intelligence > Machine Learning > Deep learning
- 하나하나의 뉴런들이 모여서, 하나의 신경망을 구성 (여러 Layers를 가지고 있음) = DNN
- DNN 적용을 위해서는 빅데이터, 컴퓨팅 성능, 진보된 알고리즘 모델이 필요함
    - 영상 인식, 이미지 합성, 기계 번역, 챗봇, 자연어 처리, 주식 가격 예측 등에 적용
- 퍼셉트론 (Perceptron): y = f(w0 + w1x1 + w2x2)
    - AND, OR, XOR Gate
    - XOR Gate는 Single layer 퍼셉트론으로는 결과값을 낼 수 없음
    - Input layer - Hidden layer - Output layer (2-layer neural network)
    - Input layer - Hidden layer1 - Hidden layer2 - Output layer (3-layer neural network)
- Forward Propagation, Activation Function (Sigmoid, Softmax), Loss Function (MSE, Cross entropy loss)

## 2. Training NN

- Gradient Descent: Loss Function을 최소화 시킬 수 있는 최적의 파라미터 값을 업데이트 하는 과정
    - Loss function이 복잡한 경우에는, 수렴 속도가 늦어지는 경우도 존재
    - 따라서 Original Gradient Descent 알고리즘을 다양한 형태로 변화시킴 (Momentum, Adagrad, Adam 등)
- NN의 학습 과정
    1. 가장 처음으로는, 학습 데이터에 대해 Random Initialize 된 값으로 Forward Propagation을 수행
    2. 그 상태에서 Loss Function의 값을 최소화 시킬 수 있는 파라미터를 찾아나감 (Backward Propagation)
- Gradient Vanishing을 해결하기 위해 Tanh, ReLU 활성화 함수 등을 사용하게 됨
- Batch Normalization (배치 정규화)
    - 각 배치 단위 별로 데이터가 다양한 분포를 가지더라도, 배치 별로 평균과 분산을 이용해 정규화 해주는 것
    - 활성화 함수의 출력값을 정규화 하여 그 분포를 고르게 해주는 효과

## 3. CNN and 이미지 분류

- Fully Connected NN, Convolution NN (CNN - 이미지 처리에 사용), Recurrent NN (RNN - 시계열 데이터에 사용)
- CNN은 특정 클래스에 존재할 수 있는 작은 특정 패턴들을 정의하고, 패턴들이 주어진 이미지에 있는지를 판단
- 매칭의 정도를 나타내는 결과값을 활성화 지도 (Activation map)이라고 하고, 이는 특정 Convolution Filter를 주어진 입력 이미지에 가능한 모든 위치에 오버랩을 시켜서 매칭되는 정도를 나타낸 것
- Channel, Filter, Pooling Layer (Max, Average)
    - 일반적인 구조: Conv - ReLU - Conv - ReLU - Pooling - Conv - ReLU - Pooling - FC
- Hyperparameters
    - Convolution: Number of filters, Size of filters
    - Pooling: Window size, Window stride
    - Fully Connected: Number of layers, Number of neurons
- CNN 모델은 어느 종류의 layer를 어느 위치에, 어느 순서로 배치하는가가 중요한 문제. 이에 대하여 많은 연구자들이 CNN 모델에 대한 architecture를 사전에 정의한 것들이 있음 = AlexNet, VGGNet, GoogleNet, ResNet
    - VGGNet: 각각의 Conv layer에서 사용하는 필터의 가로, 세로 size를 3 by 3으로 고정하고, layer를 깊게 쌓아서 문제를 해결한 알고리즘
    - ResNet (Residual Network): Conv layer를 통해 나온 output에 layer를 또 추가하는 것. layer를 추가적으로 쌓아서 생기는 비효율성에 대하여 일부 layer는 skip 할 수 있도록 하는 skip connection을 추가한 알고리즘
- CNN 알고리즘의 꾸준한 성능 개선과 함께, 모델의 layer 개수도 꾸준히 증가 됨 (AlexNet은 8개, ResNet은 152개 층)

## 4. Seq2Seq with Attention

- Recurrent Neural Network (RNN)
    - one to one, one to many, many to one, many to many
- RNN 모델의 기울기 소실 및 폭발의 문제를 해결하기 위한 LSTM, GRU 모델 등장
- Seq2Seq, Encoder & Decoder
    - Original Seq2Seq 모델은 매 time step 마다 생성되는 RNN의 각 time step의 Hidden state vector는 같은 dimension으로 이뤄져야 한다는 제약 조건이 있음. Output vector가 다시 Input vector로 사용되기 위해. 이러한 경우에 축적해야 되는 정보가 시간이 길어짐에 따라서 점점 많아지지만, 정보는 항상 똑같은 개수의 dimension에 저장하게 되어 정보를 유실하게 된다는 한계점이 있음
    - 이러한 Bottleneck 문제를 해결하고자 attention 이라는 추가적인 모델이 Seq2Seq에 도입
    - 입력 sequence에 주어지는 데이터를 encoder에서 인코딩 한 후, decoder에서는 encoder의 마지막 time step의 Hidden state vector 만을 입력으로 받지 않고, 그 입력과 더불어 decoder의 각 time step에서 encoder에서 나온 여러 인코딩 데이터 중 필요로 하는 것을 가지고 가서 예측에 사용
    - 기본적 구조는 encoder와 decoder가 존재하고, encoder의 마지막 time step의 Hidden state vector가 decoder의 가장 최초의 Hidden state vector인 h0로 사용됨. 그런데 여기서 어떤 decoder의 각 time step에서 추가적으로 encoder에 있는 여러 Hidden state vector로부터 필요한 정보를 취사선택해서 가장 유관하다고 생각하는 정보를 추가적인 입력으로 사용함

## 5. Transformer

- Transformer 모델은 Seq2Seq with attention 모델의 개선된 버전
    - Solving long-term dependency problem
    - Seq2Seq with attention 모델에서 Encoder와 Decoder가 RNN 기반의 모델로 구성됨
    - Transformer 모델은 Encoder와 Decoder에서도 attention 기반으로 동작하는 모델
        - RNN 기반의 모델들은 Long-term Dependency 문제가 있음 (오랜 시계열을 거쳐 정보가 소실 될 수 있음)
        - Transformer에서는 가까이 있거나 멀리 있는 정보를 접근해서 필요한 정보를 사용할 수 있게 됨
    - Self-attention, Multi-head Attention
- Layer Normalization은 각 단어에서 발견된 특정한 dimension으로 이루어진 벡터의 각 원소에 평균과 분산을 계산
    - 그리고 각 단어 내에서 발생된 벡터 각각의 원소 값의 평균과 분산이 0과 1이 되도록 정규화
    - dimension 혹은 layer 내의 각 노드 별로 학습된 trainable parameters로 affine transformation 수행
    - 이는 Batch Normalization과 비슷하게 학습을 조금 더 안정화시키고, 성능을 개선시킬 수 있음
- Positional Encoding은 각 단어의 입력 벡터에 몇 번째 순서에서 나타났다는 알려 줄 수 있는 정보
    - 즉, 각 단어의 순서나 위치를 구분할 수 있게 되는 것
- Transformer 모델은 기존에 RNN 및 Convolution 기반의 Sequence를 인코딩하는 모델보다 좋은 성능을 보임
    - 이렇게 가능하게 된 이유는 Long term dependency를 근본적으로 해결했기 때문
    - 자연어처리 외에도 다양한 도메인에 적용되고 있음
    - 또한 Transformer에서 제한된 Layer 수를 더 많이 늘리되, Block 자체의 설계나 디자인은 그대로 계승하고, Model의 사이즈를 점차 늘리면서 Self-supervised 방법론을 추가로 사용하여 대규모 데이터를 학습한 사전 학습 모델을 활용할 수 있음

## 6. Self-supervised Learning

- Self-supervised Learning (자가지도학습)은 데이터의 Labeling 과정 없이도 Raw data 만으로 모델을 학습
    - 즉, Raw 데이터나, 별도의 추가적인 Label 없이 입력 데이터만으로 입력 데이터 중에 일부를 가려놓고, 가려진 입력 데이터를 주었을 때 가려진 부분을 잘 복원 혹은 예측하도록 하여, 주어진 입력 데이터의 일부를 예측하도록 모델을 학습 (Computer Vision 분야의 Inpainting Task, Zigsaw Puzzle Task 등)
- Transfer Learning의 기본 아이디어는 자가지도학습을 통해 만들어진 (Inpainting) 모델의 앞쪽 Layer는 물체를 잘 인식하기 위해 필요로 하는 유의미한 패턴을 추출할 수 있도록 학습됨. 그리고 뒤쪽의 Layer는 실제 풀고자 하는 문제를 해결할 수 있도록 특화되어 학습이 될 것. 따라서 대규모 데이터로 학습된 모델에 풀고자 하는 문제를 해결할 수 있는 Layer를 뒤에 덧붙여서 모델을 재학습
- BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding)
    - BERT 모델은 Transformer의 Encoder 역할을 수행
    - 입력 문장을 BERT 모델의 입력 Sequence로 제공하되, 입력 데이터의 일부를 가리고 그것을 예측하도록 함
    - 두 개의 문장을 주고, 연속되게 등장하여 두 문장 사이에 어떤 의미 관계가 있는지 판단 (Next Sentence 예측)
    - CLS, MASK, SEP 토큰 추가
- Masked Language Model (MLM)
    - 주어진 입력 문장에 대해, 특정 확률에 따라 각 단어를 Masked token으로 대체할지 전처리 수행
    - 약 15% 비율의 단어를 Mask 단어로 대체하되, 거기서 80%는 Mask 단어, 10% 랜덤한 단어, 10%는 그대로 유지
    - Too little masking: Too expensive to train
    - Too much masking: Not enough to capture the given context
- Next Sentence Prediction (NSP)
    - 두 개의 문장을 SEP token으로 구분하여 제공하고, CLS token으로부터 인코딩 된 Hidden state vector의 Binary classification 결과를 예측
- Further Details of BERT
    - Model Architecture
        - BERT BASE: L = 12, H = 768, A = 12
        - BERT LARGE: L = 24, H = 1024, A = 16
    - Input Representation
        - WordPiece Embedding (30,000 WordPiece)
        - Learned positional embedding
        - CLS (Classification embedding)
        - Packed sentence embedding (SEP)
        - Segment Embedding
    - Pre-training Tasks
        - Masked LM
        - Next sentence prediction
- GPT (Generative Pre-Trained Transformer)
    - GPT 모델은 Transformer의 Decoder 역할을 수행
    - 주어진 텍스트 데이터들에 대해 문장을 가져와서 특정 문장의 일부만 주어졌을 때, 다음에 나타날 단어를 예측. 그리고 그 다음 단어가 주어졌을 때, 그 다음 단어를 예측하는 Word level의 Language modeling task를 학습
    - Zero-shot Summarization으로도 활용 가능
        - 일반적으로 Summarization을 수행하기 위해서는 사전 학습된 모델을 가져와서 target task인 Summarization을 목적으로 입력 지문과 정답 요약 문장인 Labeled 학습 데이터를 가지고 주어진 모델을 Fine-tuning 하는데, GPT는 그런 과정 없이 Summarization을 수행할 수 있어서 Zero-shot이라고 함
    - GPT3는 BERT와 GPT2와 다르게, 훨씬 더 많은 Layer 수를 가지고, 학습에 필요로 하는 1,750억 개의 파라미터
        - Zero-shot, One-shot, Few-shot Learning