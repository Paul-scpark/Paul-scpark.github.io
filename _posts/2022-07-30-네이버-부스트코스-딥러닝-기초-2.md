---
title: 딥러닝 기초 다지기 - 2강. 최적화 (Optimization)
date: 2022-07-30 01:00:00 +0900
categories: [네이버 부스트코스, 딥러닝 기초 다지기]
tags: [AI, 네이버 부스트코스, 파이썬, Machine learning, 딥러닝, 최적화]
description: 네이버 부스트코스 - 딥러닝 기초 다지기 2강
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

* [부스트코스 강좌 링크](https://www.boostcourse.org/ai111/lecture/1156334)

## Optimization
- Gradient descent: First-order iterative optimization for finding a local minimum of a differentiable function
  - 반복적으로 최적화를 시켜서 local minimum 값을 찾는 것을 목표로 함
- Generalization: How well the learned model will behave on unseen data
  - 만든 모델을 일반화 시켜서 다양한 환경 속에서도 적용될 수 있도록 하는 것
  - 일반적으로, 학습이 진행됨에 따라 학습 데이터의 training error는 줄어든다고 할 수 있음
  - 하지만 training error가 0이 되었다고 해서, 원하는 문제의 목표를 100% 달성했다고 할 수는 없음
  - 따라서 **Generalization gap**은 training error와 test error 사이의 차이를 의미함
- Under-fitting vs Over-fitting: 학습 데이터에는 잘 동작하지만, 테스트 데이터에서는 잘 동작하지 않는 상태
- Cross validation: Model validation technique for assessing how the model will generalize to an test dataset
  - train 데이터와 validation, test 데이터를 나눈 상태로 모델의 성능을 측정해보기
  - 학습 데이터로 학습을 시킨 모델이, 학습에 반영 안된 validation 데이터로 테스트를 힐 때 성능이 나오는지 확인해봄
  - train, validation, test 데이터를 나누는 기준의 방법론 중 하나로 cross-validation (K-fold) 방법을 사용
  - 일반적으로, cross-validation 과정을 통해서 최적의 하이퍼 파라미터들을 찾도록 함
  - 그 후에 보정된 하이퍼 파라미터의 조건을 이용하여 모든 데이터를 train 데이터로 활용해 모델 학습 (test 데이터 제외)
- Bias-variance tradeoff: low bias, high bias & low variance, high variance
- Bootstrapping: It is any test or metric that uses random sampling with replacement
  - 학습 데이터 중 일부만 random sampling 해서 모델을 만들어보기
  - 여러 번의 테스트를 통해서 전반적으로 모델의 성과가 어떻게 나오는지 확인 및 비교해봄
- Bagging: Multiple models are being trained with bootstrapping
  - Base classifiers are fitted on random subset where individual predictions are aggregated (voting)
  - 여러 개의 데이터 subset인 부트스트랩을 만들어서 각각의 데이터에 대해 모델을 독립적으로 만들기
- Boosting: It focuses on those specific training samples that are hard to classify
  - A strong model is built by combining weak learners in sequence where each learner learns from the mistake of the previous weak learner
  - 부트스트랩으로 만든 여러 모델들이 독립적이지 않고, 복수의 weak learner가 strong learner가 되도록 가이드

## Gradient Descent Methods
- Stochastic Gradient Descent: Update with the gradient computed from a single sample
  - Single sample 데이터로 접근하여 한 개씩 데이터로 경사하강법 실시
  - 적절한 learning rate를 설정하는 것이 중요
- Mini-batch Gradient Descent: Update with the gradient computed from a subset of data
  - 전체 데이터 중에서 일부 데이터를 활용하여 경사하강법 실시 (대부분 이 방법을 사용함)
- Batch Gradient Descent: Update with the gradient computed from the whole data
  - Whole data로 접근하여 한번에 모든 데이터를 활용해서 경사하강법 실시
- Momentum: 베타라고 불리는 하이퍼파라미터가 포함 (momentum)
- Nesterov accelerated gradient: gradient 계산 시에 Lookahead gradient를 계산
- Adagrad: It adapts the learning rate, performing larger updates for infrequent and smaller updates for frequent parameters
  - 뉴럴 네트워크의 파라미터가 과거부터 지금까지 얼마나 변화했는지를 확인 및 비교
  - 많이 변화가 있었던 파라미터들은 더 적게, 변화가 없었던 파라미터는 조금 더 많이 변화시키도록 가이드
  - 과거부터 현재까지 파라미터의 변화를 담고 있는 Sum of gradient squares
- Adadelta: It extends Adagrad to reduce its monotonically decreasing the learning rate by restricting the accumulation window
  - There is no learning rate in Adadelta
- RMSprop: It is an unpublished, adaptive learning rate method proposed by Geoff Hinton in his lecture
- Adam (Adaptive Moment Estimation): It leverages both past gradients and squared gradients
  - Adam effectively combines momentum with adaptive learning rate approach

## Regularization
- 학습을 방해시키면서 학습 데이터에서만 잘 적용될 뿐 아니라, 테스트 데이터에서도 잘 동작하도록 유도
- Early stopping: Note that we need additional validation data to do early stopping
  - training 과정에서 사용되지 않은 데이터를 활용해 validation error 값이 커지는 시점에 미리 학습을 중단
- Parameter norm penalty: It adds smoothness to the function space
  - 뉴럴 네트워크의 파라미터가 너무 커지지 않도록 규제하는 것 (크기의 관점에서 작게 유도, 부드러운 형태로 변화)
- Data augmentation: More data are always welcomed
  - 데이터의 양이 증가하면 할수록 모델의 성능이 개선될 수 있음
  - 따라서 (label이 훼손되지 않는 한) 데이터의 형태를 다양하게 변화시키는 작업을 실시하여 데이터의 양을 증가시키기
- Noise robustness: Add random noises inputs or weights
  - 입력 데이터에 약간의 노이즈를 추가시켜서 모델을 학습시키기
- Label smoothing: **Mix-up** constructs augmented training examples by mixing both input and output of two randomly selected training data
  - 학습 데이터를 뽑아서 섞어주는 작업을 수행시키기
  - Decision boundary를 부드럽게 만들어주는 효과가 있음
- Dropout: In each forward pass, randomly set some neurons to zero
  - 뉴럴 네트워크의 weight를 0으로 바꿔주는 작업 -> 과적합 방지
- Batch normalization: It compute the empirical mean and variance independently for each dimension layers and normalize
  - 적용하고자 하는 layer의 statistics를 정규화 시키는 것
  - Batch Norm, Layer Norm, Instance Norm, Group Norm
  