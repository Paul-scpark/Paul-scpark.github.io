---
title: LG Aimers 2기 지도학습 - 분류, 회귀 (이화여대 강제원 교수님)
date: 2023-01-13 00:00:00 +0900
categories: [Education, LG Aimers 2기]
tags: [AI, Deep learning, Machine learning]
description: LG Aimers AI 전문가 과정 강의 기록
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

이번 글에서는 LG Aimers의 AI 전문가 과정에서 머신러닝의 한 부류인 지도학습에 대한 기본 개념과 분류 및 회귀의 목적과 차이점에 대해서 학습합니다. 또한 다양한 모델과 방법론을 통해서 언제 어떤 모델을 사용해야 하는지, 왜 사용 해야 하는지, 모델 성능을 향상 시키는 방법 등에 대해 학습합니다.

---

## 1. SL Foundation

### Machine Learning Problems
- 지도학습 (Labeled data): Regression, Classification
    - 데이터 X를 이용하여 정답인 Y로 가는 함수 h를 학습하는 것
    - 새로운 데이터가 들어올 때도 동작 할 수 있도록 하는 함수를 찾아내는 것
    - 지도학습은 모델의 Output과 실제 값의 Error를 줄여가면서 학습이 진행 (Training)
    - 학습 단계에서 보지 못했던 새로운 데이터를 통해서 모델의 성능을 확인 (Testing)
- 비지도학습 (Unlabeled data): Clustering, Dimensional Reduction

### Learning Model
1. Goal: Target function (f: x -> y)
2. Training data
3. Learning model (Feature selection, Model selection, Optimization)
4. Hypothesis, Evaluation

### Model Generalization
- 학습 과정에서 데이터가 제한 됨에 따라, 성능 역시 제한 될 수 밖에 없음
- 따라서 모델의 궁극적 목적은 어느 데이터가 들어와도 동작할 수 있는 **일반화**가 필요함
    - Generalization Error (Training Error, Validation Error, Test Error)
    - Overall Error (Loss Function, Cost Function)
- Test Error를 Training Error로 가까이 가도록 한다면?
    - 실패하는 경우에는 Overfitting (High Variance) -> 정규화 등을 사용
- Training Error가 0에 가까이 가도록 한다면?
    - 실패하는 경우에는 Underfitting (High Bias) -> 조금 더 복잡한 모델 사용
- Variance와 Bias의 Trade-off (Overfitting vs Underfitting)

### Avoid Overfitting
- Data Augmentation, Ensemble
- Regularization to penalize complex models (Lasso 등)
- Cross-validation (CV, K-fold) - Train, Valid, Test Dataset

## 2. Linear Regression

### Linear Models
- Hypothesis set H
- 많은 장점이 있음 (단순함, 해석 가능성, 일반화)
- 주어진 입력에 대해 출력과 선형적 관계를 추론 (단변량, 다변량 문제)
- 선형 모델은 파라미터 (x절편 및 y절편 등)가 달라짐에 따라 데이터 fitting 과정에서 오차가 발생
- 손실 함수가 가장 작게 되도록 하는 모델 파라미터를 찾는 것이 목표 (파라미터 최적화)

### Gradient Descent
- 데이터의 양이 증가할수록 벡터의 차원의 수가 증가함에 따라 복잡도가 늘어나게 됨
- Needs Iterative Algorithm (경사하강법)
    - Iterative 하게 최적의 파라미터를 찾아가는 과정
    - Gradient는 함수를 미분하여 얻는 값으로 해당 함수의 변화하는 정도를 확인할 수 있음
    - 경사하강법에서는 Gradient가 최솟값이 되도록 반복적으로 파라미터를 변화 시킴
    - 적절한 Learning Rate가 필요함 (하이퍼파라미터: 직접 사람이 지정해야 하는 값)
    - Global Optima, Local Optima - 최적화 된 포인트 찾아가기

## 3. Gradient Descent

## 4. Linear Classification

## 5. Advanced Classification

## 6. Ensemble

