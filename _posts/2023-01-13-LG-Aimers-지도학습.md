---
title: LG Aimers 2기 지도학습 - 분류, 회귀 (이화여대 강제원 교수님)
date: 2023-01-13 00:00:00 +0900
categories: [Education, LG Aimers 2기]
tags: [AI, Deep learning, Machine learning]
description: LG Aimers AI 전문가 과정 강의 기록
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

이번 글에서는 LG Aimers의 AI 전문가 과정에서 머신러닝의 한 부류인 지도학습에 대한 기본 개념과 분류 및 회귀의 목적과 차이점에 대해서 학습합니다. 또한 다양한 모델과 방법론을 통해서 언제 어떤 모델을 사용해야 하는지, 왜 사용 해야 하는지, 모델 성능을 향상 시키는 방법 등에 대해 학습합니다.

---

## 1. SL Foundation

### Machine Learning Problems
- 지도학습 (Labeled data): Regression, Classification
    - 데이터 X를 이용하여 정답인 Y로 가는 함수 h를 학습하는 것
    - 새로운 데이터가 들어올 때도 동작 할 수 있도록 하는 함수를 찾아내는 것
    - 지도학습은 모델의 Output과 실제 값의 Error를 줄여가면서 학습이 진행 (Training)
    - 학습 단계에서 보지 못했던 새로운 데이터를 통해서 모델의 성능을 확인 (Testing)
- 비지도학습 (Unlabeled data): Clustering, Dimensional Reduction

### Learning Model
1. Goal: Target function (f: x -> y)
2. Training data
3. Learning model (Feature selection, Model selection, Optimization)
4. Hypothesis, Evaluation

### Model Generalization
- 학습 과정에서 데이터가 제한 됨에 따라, 성능 역시 제한 될 수 밖에 없음
- 따라서 모델의 궁극적 목적은 어느 데이터가 들어와도 동작할 수 있는 **일반화**가 필요함
    - Generalization Error (Training Error, Validation Error, Test Error)
    - Overall Error (Loss Function, Cost Function)
- Test Error를 Training Error로 가까이 가도록 한다면?
    - 실패하는 경우에는 Overfitting (High Variance) -> 정규화 등을 사용
- Training Error가 0에 가까이 가도록 한다면?
    - 실패하는 경우에는 Underfitting (High Bias) -> 조금 더 복잡한 모델 사용
- Variance와 Bias의 Trade-off (Overfitting vs Underfitting)

### Avoid Overfitting
- Data Augmentation, Ensemble
- Regularization to penalize complex models (Lasso 등)
- Cross-validation (CV, K-fold) - Train, Valid, Test Dataset

## 2. Linear Regression

### Linear Models
- Hypothesis set H
- 많은 장점이 있음 (단순함, 해석 가능성, 일반화)
- 주어진 입력에 대해 출력과 선형적 관계를 추론 (단변량, 다변량 문제)
- 선형 모델은 파라미터 (x절편 및 y절편 등)가 달라짐에 따라 데이터 fitting 과정에서 오차가 발생
- 손실 함수가 가장 작게 되도록 하는 모델 파라미터를 찾는 것이 목표 (파라미터 최적화)

### Gradient Descent
- 데이터의 양이 증가할수록 벡터의 차원의 수가 증가함에 따라 복잡도가 늘어나게 됨
- Needs Iterative Algorithm (경사하강법)
    - Iterative 하게 최적의 파라미터를 찾아가는 과정
    - Gradient는 함수를 미분하여 얻는 값으로 해당 함수의 변화하는 정도를 확인할 수 있음
    - 경사하강법에서는 Gradient가 최솟값이 되도록 반복적으로 파라미터를 변화 시킴
    - 적절한 Learning Rate가 필요함 (하이퍼파라미터: 직접 사람이 지정해야 하는 값)
    - Global Optima, Local Optima - 최적화 된 포인트 찾아가기

## 3. Gradient Descent

### Learning Rate
- Learning Rate에 따른 수렴 속도의 변화
- Batch Gradient Descent 알고리즘은 전체 샘플 m개를 모두 고려해야 하는 단점이 존재
    - 데이터가 증가하면 증가할수록 복잡도가 커짐
- Stochastic Gradient Descent 알고리즘은 Batch 경사하강법의 문제점을 해결한 방법
    - 샘플의 개수를 1개로 줄여서 파라미터를 지속적으로 개선시키는 알고리즘
    - Batch 경사하강법에 비해 빠르게 Iteration을 돌 수 있다는 장점
    - 하지만 각 샘플 하나씩 계산하기 때문에 노이즈의 영향이 크다는 단점
- Local Minimum을 피할 수 있는 방법
    - Momentum: 과거 Gradient가 업데이트 되었던 방향 및 속도를 반영하여 현재 포인트에서 Gradient가 0이 되더라도 계속해서 학습을 진행할 수 있는 동력을 제공하게 되는 것 
    - Nesterov Momentum: Look ahead gradient step
    - AdaGrad: Adapts an individual learning rate of each direction
    - RMSProp: AdaGrad의 단점 (Learning rate가 작아지는 부분에서 학습 X)을 보완한 알고리즘
    - Adam (Adaptive Moment Estimation): RMSProp + Momentum 방식

### 모델의 과적합 문제
- 모델이 너무 복잡한 경우, 학습 데이터에서만 좋은 결과를 보이는 Overfitting 된 모델이 나오게 됨
- Overfitting을 피할 수 있는 방법: Features의 개수를 줄이기, Regularization (정규화)

## 4. Linear Classification

- Binary Classification: decision-boundary에 따라 class를 구분하기 (Logistic 모델)
    - Zero-One Loss function
    - Hinge Loss function
    - **Cross-entropy Loss function**
- Multiclass Classification: One-VS-All
- Linear 모델의 장점: 간단함, 해석 가능성이 높음

## 5. Advanced Classification

### SVM (Support Vector Machine) 모델
- 모델의 boundary 사이에서 존재하는 가장 가까운 margin을 갖는 Support Vector를 설정
    - Support Vector는 실제 테스트 시, 성능을 결정할 수 있는 중요한 (민감한) 데이터
    - 따라서 Support Vector의 margin을 최대화 할 수 있는 모델을 구축하는 것이 목표
    - 그렇기 때문에 Outliers에 대해서도 robust 하게 동작이 가능
- Optimization 방법: Hard margin SVM, Soft margin SVM, Nonlinear transform

### ANN (Artificial Neural Network) 모델
- Non-linear classification model
- Activation functions: Sigmoid, ReLU, Leaky ReLU 등
- ANN 모델의 Layer를 더 많이 쌓게 되면, Deep Neural Network (DNN)이 됨
    - DNN을 통해서 더 비선형적인 모델의 결과를 도출 할 수 있게 됨
- Gradient Vanishing, Backpropagation, CNN

## 6. Ensemble
- 앙상블 방식: 머신러닝에서 알고리즘의 종류에 상관 없이 서로 다르거나, 같은 매커니즘으로 동작하는 다양한 머신러닝 모델을 묶어서 함께 사용하는 방식 (예측 모델의 집합을 합쳐서 새로운 모델 만들기)
- 앙상블 모델의 장점
    - 예측 성능을 안정적으로 향상 시킬 수 있다는 것
    - 비교적 간단하게 사용 할 수 있다는 것
    - 모델의 파라미터 튜닝이 많이 필요하지 않다는 것
- Bagging (Bootstrapping + Aggregating): 학습 과정에서 Training samples을 랜덤하게 나눠서 선택하여 학습
    - Bootstrapping: 다수의 Sample data set을 생성하여 학습하는 방식
    - Aggregating: Committee prediction
- Boosting: Sequential 하게 동작하는 앙상블 모델 (Weak classifier의 Cascading)
    - Weak classifier: Bias가 높은 Classifier (비교적 성능이 낮은 모델)
- Random Forest, Adaboost, GBM (Gradient Boosting Machine)
- Performance Evaluation: Accuracy, Confusion Matrix (TN, FN, FP, TP), Precision, Recall, ROC Curve