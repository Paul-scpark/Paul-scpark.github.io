---
title: 딥러닝 기초 다지기 - 1강. 딥러닝 기초
date: 2022-07-30 01:00:00 +0900
categories: [네이버 부스트코스, 딥러닝 기초 다지기]
tags: [AI, 네이버 부스트코스, 파이썬, Machine learning, 딥러닝]
description: 네이버 부스트코스 - 딥러닝 기초 다지기 1강
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

## 간단한 강의 소개
> 이 강의는 네이버 부스트코스의 '딥러닝 기초 다지기' 강좌입니다. \
> 네이버 AI Tech 4기를 지원하기 위해 기초가 되는 이 수업들을 수강하고 기록해 볼 예정입니다. \
> 딥러닝에 대한 기본적인 내용들을 복습하면서 기초를 되돌아보는 시간으로 활용하고자 합니다.
* [부스트코스 강좌 링크](https://www.boostcourse.org/ai111/lecture/1108319)

## Introduction (DL < ML < AI)
- Artificial intelligence: mimic human intelligence
- Machine learning: data-driven approach
- Deep learning: neural network

<br/>
<img src="http://esignal.co.kr/images/ai/PIC8F7E.png" width=800 height=1000>

### Deep Learning에 필요한 지식
- Implementation skills: Tensorflow, PyTorch
- Math skills: Linear algebra, probability
- Knowing a lot of papers

### contents
- Historical review
- Neural networks, multi-layer perceptron
- Optimization methods
- Convolutional neural network
- Modern CNN
- Computer vision applications
- Recurrent neural network (RNN)
- Transformer
- Generative models

## key concepts of deep learning
- The **data** that the model can learn from
  - Data depend on the type of the problem to solve (classification, semantic segmentation, detection, etc)
- The **model** how to transform the data
  - 같은 데이터라도 어떤 모델을 사용하는가에 따라 결과가 다름 (AlexNet, GoogleNet ,ResNet, LSTM, GAN, etc)
- The **loss** function that quantifies the badness of the model
  - The loss function is a proxy of what we want to achieve (Regression task: MSE, Classification task: CE)
  - loss function이 줄어든다고 해서, 항상 모든 문제의 정확도가 높아지는 것은 아님
- The **algorithm** to adjust the parameters to minimize the loss
  - 최적화 알고리즘 (SGD, Momentum, NAG, Adagrad, Adadelta, Rmsprop, etc)
  - 규제화 (Dropout, Early stopping, K-fold validation, Ensemble, etc)

## Historical review
- 2012: AlexNet - 224 * 224 사이즈 이미지가 들어왔을 때, 분류하는 모델 -> 딥러닝이 처음으로 실제적인 성과를 냈던 모델
- 2013: DQN - 딥마인드가 '아타리' 게임을 강화 학습을 이용해서 해결
- 2014: Encoder/Decoder - 특정 언어에 맞는 단어의 시퀀스가 주어졌을 때, 다른 단어로 바꿔주는 형태 (seq to seq)
- 2014: Adam optimizer - 하이퍼파라미터, optimizer 설정 등의 작업들이 필요. But, Adam은 높은 수준의 결과를 도출
- 2015: Generative Adversarial Network (GAN) - 판별기와 생성기가 서로 경쟁하면서 학습시키는 방법
- 2015: Residual Network (ResNet) - layer의 수가 늘어도, test 데이터셋에서 잘 동작하도록 했던 모델
- 2017: Transformer (Attention)
- 2018: BERT - fine tuned NLP models (pre-training, fine-tuning)
- 2019: GPT-X - big language models
- 2020: Self-supervised learning (SimCLR) - 학습 데이터 외에 unlabeled 데이터를 활용해서 모델 개선

## Neural network & Multi-layer perceptron
- 뉴럴 네트워크: 사람의 뇌와 같은 신경망을 모방한 컴퓨팅 시스템 (GoogleNet, ResNet)
  - NN are function approxiators that stack affine transformations followed by nonlinear transformations

### Linear Neural Networks
- 선형회귀 방법론에 따라 y절편과 기울기를 계산하여 회귀식과 회귀선을 도출 (data, model, loss)
- 손실 함수를 최소화 시킬 수 있는 조건 값을 찾는 것이 모델의 목표 
  - We compute the partial derivatives w.r.t the optimization variables
- Gradient descent: Then, we iteratively update the optimization variables
  - stepsize가 너무 큰 경우에는 학습이 제대로 이뤄지지 않음 (너무 크지도, 작지도 않도록 적용해야 함)
- Of course, we can handle multi-dimensional input and output
  - One way of interpreting a matrix is to regard it as a mapping between two vector spaces
- What if we stack more?
- Activation functions (Nonlinear transform): ReLU, Sigmoid, Hyperbolic Tangent

### Multi-layer perceptron
- This class of architectures are often called multi-layer perceptrons
- Of course, it can go deeper
- Loss function: MSE (Regression), CE (Classification), MLE (Probabilistic)
  - 손실 함수가 어떤 성격을 가지고 있는지, 그리고 내가 풀고자 하는 문제를 해결할 수 있는 방법론인지 고민해야 함