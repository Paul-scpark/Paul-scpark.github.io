---
title: 프로그래머스 인공지능 데브코스 9주차 정리 및 후기
date: 2022-11-21 00:00:00 +0900
categories: [Education, 프로그래머스 인공지능 데브코스 4기]
tags: [AI, Deep learning, Machine learning, 프로그래머스, 인공지능 데브코스, K-digital training]
description: 프로그래머스 인공지능 데브코스 9주차 강의 기록
toc: true
toc_sticky: true
toc_label: 목차
math: true
mermaid: true

---

이번 글에서는 프로그래머스 인공지능 데브코스의 9주차 강의에 대한 정리입니다. <br/>

---

## 1. 신경망의 기초 - 다층 퍼셉트론1
### 인공신경망과 생물신경망
- 사람의 뉴런: 두뇌의 가장 작은 정보 처리 단위
- 컴퓨터가 사람 뇌의 정보 처리를 모방하여 지능적 행위를 할 수 있는 인공지능 도전
    - 뉴런의 동작 이해를 모방한 초기 인공 신경망 (ANN) 연구 시작
    - **퍼셉트론**이 고안됨
- 신경망의 종류
    - 전방 (Forward) 신경망, 순환 (Recurrent) 신경망
    - 얕은 (Shallow) 신경망, 깊은 (Deep) 신경망
    - 결정론 (Deterministic) 신경망: 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망
    - 확률론 (Stochastic) 신경망: 고유의 임의성을 가지고 매개변수와 조건이 같더라도 다른 출력을 갖는 신경망
- 퍼셉트론: 절 (Node), 가중치 (Weight), 층 (Layer)과 같은 새로운 개념의 구조 도입
    - 제시된 퍼셉트론 구조의 학습 알고리즘을 제안
    - 깊은 인공신경망 (Deep Learning)을 포함한 현대 인공신경망의 토대
    - 입력 (편향 노드 포함) -> 입력과 출력 사이의 연산 -> 출력
- 일반적인 분류기의 학습 과정
    1. 과업 정의와 분류 과정의 수학적 정의 (가설 설정)
    2. 해당 분류기의 목적함수 정의
    3. 목적함수를 최소화 하는 값을 찾기 위한 최적화 수행
        - 경사하강법을 통해 기울기를 미분하여 반복 탐색해 극값을 찾음

## 2. 신경망의 기초 - 다층 퍼셉트론2
### 다층 퍼셉트론
- 퍼셉트론: 선형 분류기 (Linear Classifier)의 한계
    - OR, AND 분류기는 가능하지만, XOR 문제는 해결하지 못함
- 다층 퍼셉트론의 핵심 아이디어
    - 은닉층을 두어, 특징 공간을 분류하는데 유리한 새로운 특징 공간으로 변환
    - 연성에서는 출력이 연속값이므로 시그모이드 함수를 활성화 함수로 도입
    - 오류 역전파 알고리즘을 사용하여 한 층씩 그레디언트를 계산하고, 가중치를 갱신
- 특징 공간 변환
    - 퍼셉트론 2개를 병렬 결합하면, 원래 공간을 새로운 특징 공간으로 변환 가능
- 추가 퍼셉트론 1개를 순차 결합하면, **다층 퍼셉트론**이 됨

### 활성화 함수
- 딱딱한 공간 분할과 부드러운 공간 분할
    - 계단 함수는 딱딱한 의사결정: 영역을 점으로 변환
    - 그 외에 활성화 함수는 부드러운 의사결정: 영역을 영역으로 변환
        - 로지스틱 시그모이드
        - 하이퍼볼릭 탄젠트 시그모이드
        - Softplus와 Rectifier (ReLU)
    - 활성화 함수에 따른 다층 퍼셉트론의 공간 분할 능력 변화 (경성 부분 변화)
- 일반적으로 은닉층에서 **로지스틱 시그모이드**를 활성화 함수로 많이 사용
    - S자 모양의 넓은 포화 곡선은 경사도 기반한 학습 (오류 역전파)을 어렵게 함
    - 기울기 소실 (Gradient Vanishing) 문제 발생
    - 따라서 깊은 신경망에서는 **ReLU**를 활용
        - 계단 활성화 함수의 범위는 -1과 1
        - 로지스틱 활성화 함수의 범위는 0부터 1
        - 하이퍼볼릭 탄젠트 활성화 함수의 범위는 -1부터 1
        - 소프트플러스, 렉티파이어 (ReLU) 활성화 함수의 범위는 0부터 무한대

### 구조
- 기존에는 입력층 -> 은닉층 -> 출력층의 2층 구조
- 입력층 -> 은닉층 -> 은닉층 -> 출력층의 3층 구조
    - p개의 은닉 노드: p는 하이퍼 매개변수
    - p가 너무 크면 과잉적합, 너무 작으면 과소적합
    - 하이퍼 매개변수 (Hyper-paramenters) 최적화 필요

### 동작
- 특징 벡터 x를 출력 벡터 o로 사상 (Mapping) 하는 함수로 간주할 수 있음
    - 2층 퍼셉트론: o = f2(f1(x))
    - 3층 퍼셉트론: o = f3(f2(f1(x)))
- 은닉층은 특징 추출기
    - 은닉층은 특징 벡터를 분류에 더 유리한 새로운 특징 공간으로 변환
    - 현대 기계학습에서는 특징학습 (Feature Learning, Data-driven Learning) 이라 부름
    - 심층학습은 더 많은 층을 거쳐 계층화 된 특징학습을 함
- 범용적 근사 이론 (Univeral Approximation Theorem)
    - 하나의 은닉층은 함수의 근사를 표현
    - 다층 퍼셉트론도 공간을 변환하는 근사 함수
- 얕은 은닉층의 구조: 일반적으로 깊은 은닉층의 구조가 좋은 성능을 가짐
- 입력층 -> 은닉층 (순방향 전파) -> 오차 계산 -> 은닉층 (역방향 전파) -> 오차 계산
- 학습 알고리즘은 오류 역전파를 반복하여 수행

### 성능 향상을 위한 경험의 중요성
- 순수한 최적화 알고리즘으로는 높은 성능이 불가능
    - 데이터 희소성, 잡음, 미숙한 신경망 구조 등 때문
    - 성능 향상을 위한 다양한 경험 (Heuristics)을 개발하고 공유함
        - 아키텍쳐, 초깃값, 학습률, 활성화 함수

## 3. 신경망의 기초 - 다층 퍼셉트론3
### 목적 함수의 정의
- 훈련집합
    - 특징 벡터 집합 (X)과 부류 벡터 집합 (Y) - 지도학습
    - 부류 벡터는 단발성 (One-hot) 코드로 표현
- 기계학습의 목표: 모든 샘플을 옳게 분류하는 함수 f를 찾는 것
- 목적 함수: 평균 제곱 오차 (Mean Squared Error, MSE)
- 전방 전파와 오류 역전파

### 오류 역전파 알고리즘의 설계
- 연쇄 법칙의 구현: 반복되는 부분식들 (Subexpressions)을 저장하거나 재연산을 최소화
- 목적 함수의 최저점을 찾아주는 **경사 하강법**
- 출력의 오류를 역방향 (왼쪽)으로 전파하여 경사도를 계산하는 알고리즘 (**오류 역전파** 알고리즘)

### 미니배치 확률론적 경사 하강법
- 미니배치 방식
    - 한번에 t개의 샘플을 처리 (t는 미니배치 크기)
    - 미니배치 방식은 보통 수십 ~ 수백
        - 경사도의 잡음을 줄여주는 효과 때문에 수렴이 빨라짐
        - GPU를 사용한 병렬처리에도 유리함
    - 현대 기계학습은 미니배치 기반의 확률론적 경사 하강법을 표준처럼 널리 사용

---

<br/>
<br/>

> 출처: 프로그래머스 인공지능 데브코스 4기 9주차 강의 -> [강의 내용 정리 깃허브 링크](https://github.com/Paul-scpark/AI-dev-course/tree/main/9%EC%A3%BC%EC%B0%A8)
