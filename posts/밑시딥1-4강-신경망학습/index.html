<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="밑시딥1 4강. 신경망 학습" /><meta property="og:locale" content="ko" /><meta name="description" content="밑시딥1 4강 내용 정리" /><meta property="og:description" content="밑시딥1 4강 내용 정리" /><link rel="canonical" href="https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5/" /><meta property="og:url" content="https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5/" /><meta property="og:site_name" content="Paul’s Insights" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2022-10-14T22:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="밑시딥1 4강. 신경망 학습" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-10-26T15:37:15+09:00","datePublished":"2022-10-14T22:00:00+09:00","description":"밑시딥1 4강 내용 정리","headline":"밑시딥1 4강. 신경망 학습","mainEntityOfPage":{"@type":"WebPage","@id":"https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5/"},"url":"https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5/"}</script><title>밑시딥1 4강. 신경망 학습 | Paul's Insights</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Paul's Insights"><meta name="application-name" content="Paul's Insights"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/main_image.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Paul's Insights</a></div><div class="site-subtitle font-italic">Space to learn, to record experiences.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/Paul-scpark" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://www.linkedin.com/in/%EC%84%B1%EC%B0%AC-%EB%B0%95-35a84b219/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['solver.paul','google.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>밑시딥1 4강. 신경망 학습</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>밑시딥1 4강. 신경망 학습</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1665752400" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Oct 14, 2022 </em> </span> <span> Updated <em class="" data-ts="1666766235" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Oct 26, 2022 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/Paul-scpark">Seongchan Park</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="6115 words"> <em>33 min</em> read</span></div></div></div><div class="post-content"><p>이번 글에서는 밑바닥부터 시작하는 딥러닝1 책의 4강에 대한 리뷰를 시작합니다. <br /> 딥러닝의 퍼셉트론과 신경망에 대해 학습한 후, 이번에는 어떻게 학습이 진행되는지 학습합니다. <br /> 손실 함수를 통해 파라미터가 갱신되는 과정을 확인해보며, 그것을 가능케 하는 경사 하강법을 배웁니다.</p><hr /><div class="table-wrapper"><table><thead><tr><th style="text-align: center">Chapter<th style="text-align: center">Title<th style="text-align: center">Main Topics<tbody><tr><td style="text-align: center">1강<td style="text-align: center">헬로 파이썬<td style="text-align: center">파이썬 기초 문법 소개, numpy, matplotlib<tr><td style="text-align: center">2강<td style="text-align: center">퍼셉트론<td style="text-align: center">AND, NAND, OR 게이트<tr><td style="text-align: center">3강<td style="text-align: center">신경망<td style="text-align: center">활성화 함수, 다차원 배열 계산, 출력층 설계, MNIST<tr><td style="text-align: center"><span style="color:red">4강</span><td style="text-align: center"><span style="color:red">신경망 학습</span><td style="text-align: center"><span style="color:red">손실 함수, 경사 하강법</span><tr><td style="text-align: center">5강<td style="text-align: center">오차역전파법<td style="text-align: center">역전파, 활성화 함수 구현<tr><td style="text-align: center">6강<td style="text-align: center">학습 관련 기술들<td style="text-align: center">매개변수 갱신, 배치 정규화, 하이퍼파라미터 값 찾기<tr><td style="text-align: center">7강<td style="text-align: center">합성곱 신경망 (CNN)<td style="text-align: center">합성곱 계층, 풀링 계층, CNN 구현<tr><td style="text-align: center">8강<td style="text-align: center">딥러닝 (Deep learning)<td style="text-align: center">초기 역사, 딥러닝 활용<tr><td style="text-align: center">Appendix<td style="text-align: center">Softmax with loss 계층의 계산 그래프<td style="text-align: center">-</table></div><ul><li><a href="http://www.yes24.com/Product/Goods/34970929">밑바닥부터 시작하는 딥러닝 1</a><li><a href="https://github.com/WegraLee/deep-learning-from-scratch">밑바닥부터 시작하는 딥러닝 1 Github 링크</a></ul><p><br /></p><blockquote><p>참고 - 다크 모드가 아닌 화이트 모드로 보시면 자료를 편하게 확인 가능합니다!</p></blockquote><hr /><h2 id="chapter-4-신경망-학습"><span class="mr-2"><font color="orange">Chapter 4. 신경망 학습</font></span><a href="#chapter-4-신경망-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="41-데이터에서-학습한다"><span class="mr-2">4.1 데이터에서 학습한다!</span><a href="#41-데이터에서-학습한다" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>신경망의 특징은 데이터를 보고, 학습 할 수 있다는 점<li><strong>학습</strong>이란, 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것<li>학습의 목표는 <strong>손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것</strong></ul><h4 id="411-데이터-주도-학습"><span class="mr-2">4.1.1 데이터 주도 학습</span><a href="#411-데이터-주도-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li><strong>기계학습</strong>은 데이터에서 답을 찾고, 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것<li>따라서 기계학습의 중심에는 <strong>데이터</strong>가 존재한다고 할 수 있음<li>기계학습에서는 <code class="language-plaintext highlighter-rouge">사람의 개입을 최소화</code>하면서 수집한 데이터로 패턴을 찾으려 시도<li>MNIST 모델에서 숫자를 인식하는 알고리즘이 동작하기 위해서,<ul><li>이미지에서 특징 (Feature)을 추출하여 그 특징의 패턴을 기계학습 기술로 학습<li><strong>특징</strong>은 입력 데이터에서 본질적인 데이터를 정확히 추출할 수 있도록 설계된 변환기<li>이미지의 특징은 일반적으로 ‘벡터’로 변환하여 학습</ul><li>기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 <strong>기계</strong>가 한다고 할 수 있음<li>사람이 직접 설계하는 것에 비해서 부담은 적지만, 이미지를 벡터로 변환하는 등의 과정은 사람이 해야 함<li>문제에 적합한 특징을 설계하지 (전처리) 못한다면, 좋은 결과를 획득하기도 어렵다고 할 수 있음<li>입력부터 출력까지 사람의 개입 없이 동작하여 <strong>종단간 기계학습 (end-to-end machine learning)</strong>이라고도 함</ul><h4 id="412-훈련-데이터와-시험-데이터"><span class="mr-2">4.1.2 훈련 데이터와 시험 데이터</span><a href="#412-훈련-데이터와-시험-데이터" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>기계학습 문제는 <strong>훈련 데이터 (Training data)와 시험 데이터 (Test data)</strong>로 나눠 학습과 실험을 수행<li>훈련 데이터를 통해서 학습하면서 최적의 매개변수를 찾음<li>그 후에 시험 데이터를 사용하여 앞서 훈련한 모델의 성과를 평가<li>결국 모델링의 궁극적인 목적은 범용적으로 사용할 수 있는 <strong>일반화된 모델</strong>이므로, 훈련 데이터와 시험 데이터로 구분<li>이를 위해서 한번도 보지 못했던 훈련에 포함되지 않은 데이터로 성능을 측정<li>한 데이터셋에만 지나치게 최적화 된 상태를 <strong>오버피팅 (Overfitting)</strong>이라고 함</ul><h3 id="42-손실-함수"><span class="mr-2">4.2 손실 함수</span><a href="#42-손실-함수" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>신경망 학습에서는 현재의 상태를 <code class="language-plaintext highlighter-rouge">하나의 지표</code>로 표현<li>그 지표를 가장 좋게 만들어주는 가중치 매개변수 값을 찾는 것이 목적<li>신경망 학습에서 사용하는 지표는 <strong>손실 함수 (Loss function)</strong>라고 정의<li>일반적으로 <code class="language-plaintext highlighter-rouge">오차제곱합과 교차 엔트로피 오차</code>를 사용<ul><li>손실 함수는 신경망의 성능의 나쁨을 나타내는 지표<li>즉, 현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는지의 성능을 담고 있음<li>따라서 손실 함수의 값이 클수록 좋지 않는 성능을 갖고 있다고 할 수 있음</ul></ul><h4 id="421-오차제곱합-sum-of-square-for-error-sse"><span class="mr-2">4.2.1 오차제곱합 (Sum of Square for Error, SSE)</span><a href="#421-오차제곱합-sum-of-square-for-error-sse" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><br /></p>\[E = \frac{1}{2}{\sum_{k} (y_k - t_k)^2}\]<p><br /></p><ul><li>가장 많이 쓰이는 손실 함수는 <strong>오차제곱합</strong>이고, 수식은 위와 같음<li>$y_k$는 신경망의 출력 (신경망이 출력한 값), $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 나타냄<li>즉, 오차제곱합은 각 원소의 출력과 정답 레이블의 차를 제곱한 후에 그 총합을 구하는 것<li>파이썬 코드로 오차제곱합을 구현하면, 아래와 같음을 알 수 있음<ul><li>MNIST에서 정답이 ‘2’라고 했을 때, 모델의 출력 결과가 맞았을 때와 틀렸을 때의 손실함수 값 확인<li>출력 결과가 맞을 때, 손실 함수 값이 0.0975로 작게 나오는 것을 확인할 수 있음<li>반면, 출력 결과가 틀렸을 때, 손실 함수 값이 0.5975로 크게 나오는 것을 확인할 수 있음<li>즉, <code class="language-plaintext highlighter-rouge">손실 함수의 값이 작을수록 정답에 수렴한다</code>고 할 수 있음</ul></ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">sum_squares_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">t</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># 정답은 '2'
</span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># 예1: '2'일 확률이 가장 높을 때
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="nf">sum_squares_error</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="c1"># 0.09750000000000003
</span>
<span class="c1"># 예2: '7'일 확률이 가장 높을 때
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="nf">sum_squares_error</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="c1"># 0.5975
</span></pre></table></code></div></div><h4 id="422-교차-엔트로피-오차-cross-entropy-error-cee"><span class="mr-2">4.2.2 교차 엔트로피 오차 (Cross Entropy Error, CEE)</span><a href="#422-교차-엔트로피-오차-cross-entropy-error-cee" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><br /></p>\[E = -{\sum_{k} t_klog_ey_k}\]<p><br /></p><ul><li>위 수식은 교차 엔트로피 오차의 수식<li>log는 밑이 e인 자연로그, $y_k$는 신경망의 출력, $t_k$는 정답 레이블이면서 정답 인덱스만 1 (원-핫 인코딩)<li>따라서 실질적으로 정답일 때 추정의 자연로그를 계산하는 식 (다른 경우에는 모두 0이기 때문)<li>즉, <code class="language-plaintext highlighter-rouge">교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 결정</code><li>파이썬 코드로 교차 엔트로피 오차를 구현하면, 아래와 같음을 알 수 있음<ul><li>MNIST에서 정답이 ‘2’라고 했을 때, 모델의 출력 결과가 맞았을 때와 틀렸을 때의 손실함수 값 확인<li>출력 결과가 맞을 때, 손실 함수 값이 0.5108로 작게 나오는 것을 확인할 수 있음<li>반면, 출력 결과가 틀렸을 때, 손실 함수 값이 2.3025로 크게 나오는 것을 확인할 수 있음<li>즉, 위의 오차제곱합과 동일하게 <code class="language-plaintext highlighter-rouge">손실 함수의 값이 작을수록 정답에 수렴한다</code>고 할 수 있음</ul></ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="mf">1e-7</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">t</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span> <span class="o">+</span> <span class="n">delta</span><span class="p">))</span>

<span class="c1"># 정답은 '2'
</span><span class="n">t</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># 예1: '2'일 확률이 가장 높을 때
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="c1"># 0.510825457099338
</span>
<span class="c1"># # 예2: '7'일 확률이 가장 높을 때
</span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>
<span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">t</span><span class="p">))</span> <span class="c1"># 2.3025840929945454
</span></pre></table></code></div></div><h4 id="423-미니배치-학습"><span class="mr-2">4.2.3 미니배치 학습</span><a href="#423-미니배치-학습" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><p><br /></p>\[E = -\frac{1}{N}{\sum_{n}}{\sum_{k} t_klog_ey_k}\]<p><br /></p><ul><li>위 수식은 <strong>훈련 데이터 모두</strong>에 대한 손실 함수 값을 구하는 수식<li>앞선 수식과 비슷하고, 데이터 하나에 대한 손실 함수를 단순히 N개의 데이터로 확장한 것<li>마지막에는 N으로 나눠서 정규화 (평균 손실 함수의 역할)<li>하지만 현실적으로 <strong>빅데이터 안에서 이 모든 데이터를 대상으로 값을 계산하는 것은 비효율적</strong><li>따라서 데이터 일부를 추려 전체의 근사치로 이용하는 <strong>미니배치 (Mini-batch)</strong> 방법을 사용 (일부만 골라 학습)<li>아래 코드는 MNIST 데이터셋에서 np.random.choice 함수를 통해 미니배치로 계산하는 과정을 소개</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">sys</span><span class="p">,</span> <span class="n">os</span><span class="p">,</span> <span class="n">pickle</span>
<span class="n">github_url</span> <span class="o">=</span> <span class="s">'/Users/paul/Desktop/github/deep-learning-from-scratch-master/'</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">github_url</span><span class="p">)</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">dataset.mnist</span> <span class="kn">import</span> <span class="n">load_mnist</span>

<span class="c1">### MNIST 데이터셋 불러오기
</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> \
    <span class="nf">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">x_train</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># (60000, 784)
</span><span class="n">t_train</span><span class="p">.</span><span class="n">shape</span> <span class="c1"># (60000, 10)
</span>
<span class="c1">### 훈련 데이터에서 무작위로 10장만 추출하기
</span><span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
<span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
<span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
</pre></table></code></div></div><h4 id="424-배치용-교차-엔트로피-오차-구현하기"><span class="mr-2">4.2.4 (배치용) 교차 엔트로피 오차 구현하기</span><a href="#424-배치용-교차-엔트로피-오차-구현하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">y</span><span class="p">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">t</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="nf">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">)</span>
        
    <span class="n">batch_size</span> <span class="o">=</span> <span class="n">y</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
<span class="c1">#     return -np.sum(t * np.log(y + 1e-7)) / batch_size # 원-핫 인코딩이 되어 있는 경우
</span>    <span class="k">return</span> <span class="o">-</span><span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">batch_size</span><span class="p">),</span> <span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">))</span> <span class="o">/</span> <span class="n">batch_size</span> <span class="c1"># 원-핫 인코딩이 되어 있지 않은 경우
</span></pre></table></code></div></div><ul><li>배치 데이터를 지원하는 교차 엔트로피 오차 구현<li>y는 신경망의 출력, t는 정답 레이블<li>정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있음</ul><h4 id="425-왜-손실-함수를-설정하는가"><span class="mr-2">4.2.5 왜 손실 함수를 설정하는가?</span><a href="#425-왜-손실-함수를-설정하는가" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>모델의 궁극적인 목적은 높은 정확도를 끌어내는 매개변수 값을 찾아내는 것<li>신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾음<li>이때 매개변수의 미분 (기울기)을 계산하여, 그 값을 단서로 값을 서서히 갱신하는 과정을 반복<li>가중치 매개변수의 손실 함수 미분이라는 것:<ul><li><strong>가중치 매개변수 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하는지</strong>에 대한 것<li>미분 값이 음수면, 그 가중치 매개변수를 양의 방향으로 변화시켜서 손실 함수의 값을 줄일 수 있음<li>반대로 미분 값이 양수면, 가중치 매개변수를 음의 방향으로 변화시켜서 손실 함수 값을 줄일 수 있음<li>하지만 미분 값이 0이면, 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수 값은 변화하지 않음<li>따라서 매개변수 갱신은 중단</ul><li>정확도를 지표로 삼게 되면, 결과값이 연속적으로 변화하지 못하고, 불연속적으로 띄엄띄엄한 값으로 변화<li>한편, 손실 함수를 지표로 삼으면, 매개변수 값이 변할 때마다 그에 반응하여 손실 함수도 연속적으로 값이 변화<li>따라서 활성화 함수로 계단 함수를 사용하지 않는 이유 역시, 미분값이 불연속적으로 계산되기 때문임<li>시그모이드 함수 같은 경우에는, 출력이 연속적으로 변하면서 곡선의 기울기 역시 연속적으로 변화함</ul><h3 id="43-수치-미분"><span class="mr-2">4.3 수치 미분</span><a href="#43-수치-미분" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>경사법에서는 기울기 (경사) 값을 기준으로 나아갈 방향을 정함</ul><h4 id="431-미분"><span class="mr-2">4.3.1 미분</span><a href="#431-미분" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li><strong>미분</strong>은 한순간의 변화량을 표시한 것, <strong>수치 미분</strong>은 아주 작은 차분으로 미분하는 것<li>미분을 수식으로 표현하면 아래와 같은데, 이 뜻은 x의 작은 변화가 함수 $f(x)$를 얼마나 변화시키느냐에 대한 것</ul><p><br /></p>\[\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}\]<p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">numerical_diff</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c1"># 0.0001
</span>    <span class="nf">return </span><span class="p">(</span><span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="o">-</span><span class="n">h</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
</pre></table></code></div></div><h4 id="432-수치-미분의-예"><span class="mr-2">4.3.2 수치 미분의 예</span><a href="#432-수치-미분의-예" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>$y = 0.01x^2 + 0.1x$를 그래프로 표현하면 아래와 같음<li>해당 식에서 x가 5일 때와 10일 때의 미분 결과는 각각 0.2, 0.3 정도</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">matplotlib.pylab</span> <span class="k">as</span> <span class="n">plt</span>

<span class="k">def</span> <span class="nf">function_1</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.01</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="mf">0.1</span><span class="o">*</span><span class="n">x</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">20.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span> <span class="c1"># 0에서 20까지 0.1 간격의 배열 x를 만듦
</span><span class="n">y</span> <span class="o">=</span> <span class="nf">function_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="s">'x'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="s">'f(x)'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="nf">numerical_diff</span><span class="p">(</span><span class="n">function_1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>  <span class="c1"># 0.1999999999990898
</span><span class="nf">numerical_diff</span><span class="p">(</span><span class="n">function_1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span> <span class="c1"># 0.2999999999986347
</span></pre></table></code></div></div><h4 id="433-편미분"><span class="mr-2">4.3.3 편미분</span><a href="#433-편미분" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>$f(x_0, x_1) = x_0^2 + x_1^2$ 라는 식을 파이썬으로 구현하면 아래와 같음<li>이 식을 미분하려고 할때, 주의해야 하는 것은 변수가 2개이므로, 어느 변수에 대한 미분인지가 중요<li>이처럼 변수가 여럿인 함수에 대한 미분을 <strong>편미분</strong>이라고 정의<li>편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구하는 것<li>단, 여러 변수들 중에서 목표 변수 하나에 초점을 맞추고, 다른 변수는 값을 고정함</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">function_2</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>
</pre></table></code></div></div><h3 id="44-기울기-gradient"><span class="mr-2">4.4 기울기 (Gradient)</span><a href="#44-기울기-gradient" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>모든 변수의 편미분을 벡터로 정리한 것<li>아래 그림은 $x_0^2 + x_1^2$의 기울기를 나타내는 그림<li>아래 그림에서 확인할 수 있듯, 기울기는 함수의 <strong>가장 낮은 장소 (최솟값)</strong>를 가리키는 것 같은 모양<li>가장 낮은 곳에서 멀어질수록 화살표의 크기가 커짐<li>하지만 정확하게 말한다면, 기울기는 각 지점에서 낮아지는 방향을 가리킴<ul><li>즉, 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향<li>Global Optima, Local Optima 개념 참고</ul></ul><p><br /></p><p><img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 500 500'%3E%3C/svg%3E" data-src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbADIrW%2Fbtra6IQukSn%2Fu5FHuUcArKf3FKp5V6skik%2Fimg.png" width="500" height="500" data-proofer-ignore></p><p><br /></p><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h</span> <span class="o">=</span> <span class="mf">1e-4</span> <span class="c1"># 0.001
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># x와 형상이 같은 배열 생성
</span>    
    <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">):</span>
        <span class="c1"># f(x + h) 계산
</span>        <span class="n">tmp_val</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">+</span> <span class="n">h</span>
        <span class="n">fxh1</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="c1"># f(x - h) 계산
</span>        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="o">-</span> <span class="n">h</span>
        <span class="n">fxh2</span> <span class="o">=</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="n">grad</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">fxh1</span> <span class="o">-</span> <span class="n">fxh2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">h</span><span class="p">)</span>
        <span class="n">x</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">tmp_val</span> <span class="c1"># 값 복원
</span>    <span class="k">return</span> <span class="n">grad</span>

<span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">]))</span> <span class="c1"># array([6., 8.])
</span><span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]))</span> <span class="c1"># array([0., 4.])
</span><span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]))</span> <span class="c1"># array([6., 0.])
</span></pre></table></code></div></div><h4 id="441-경사법-경사-하강법"><span class="mr-2">4.4.1 경사법 (경사 하강법)</span><a href="#441-경사법-경사-하강법" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>기계학습에서는 학습 단계에서 최적의 매개변수 (가중치와 편향)를 학습에서 찾음<li>최적이라는 것은 손실함수가 최솟값이 될 때의 매개변수 값<li>하지만 손실함수는 복잡하기 때문에, 어디가 최솟값인지 찾기 힘듦<li>따라서 기울기를 잘 이용하여 함수의 (가능한 최대한 작은) 최솟값을 찾으려는 것이 <strong>경사법</strong>의 개념<li>각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 <strong>기울기</strong><ul><li>하지만 기울기가 가리키는 곳에 함수의 최솟값이 있는지 보장할 수는 없음<li>복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없을 수도 있음</ul><li>그렇기 때문에 기울기 정보를 단서로 하여 나아갈 방향을 정하는 경사법 개념이 도입<li>경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한 후, 이동한 곳에서 기울기를 구하고, 이 과정을 반복<li>이렇게 함수의 값을 점차 줄이는 것이 <strong>경사법 (Gradient method)</strong>이고, 기계학습을 최적화 하는데 사용<ul><li>경사법에서 최솟값을 찾는 과정을 <strong>경사 하강법 (Gradient descent method)</strong><li>경사법에서 최댓값을 찾는 과정을 <strong>경사 상승법 (Gradient ascent method)</strong></ul></ul><p><br /></p>\[x_0 = x_0 - \eta \frac{\partial f}{\partial x_0}\] \[x_1 = x_1 - \eta \frac{\partial f}{\partial x_1}\]<p><br /></p><ul><li>경사법을 수식으로 표현하면 위와 같음<li>$\eta$는 갱신하는 양을 나타내고, 신경망 학습에서는 <strong>학습률 (Learning rate)</strong>이라고 정의<li>한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하는지를 정하는 것<li>일반적으로 학습률은 0.01, 0.001 등의 값으로 정하는데, 이 값이 너무 크거나 작으면 좋은 장소로 찾아가기 힘듦<li>학습률과 같은 매개변수를 <strong>하이퍼파라미터 (Hyper parameter)</strong>라고 정의<ul><li>신경망의 가중치 매개변수는 훈련 데이터와 알고리즘으로 자동으로 획득되는 매개변수<li>하지만 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수</ul></ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="c1"># f = 최적화 하려는 함수, init_x = 초깃값, lr = 학습률, step_num = 경사법에 따른 반복 횟수
</span><span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">init_x</span>
    
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">step_num</span><span class="p">):</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">-=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">grad</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="c1"># 학습률에 따른 경사하강법의 결과
</span><span class="n">init_x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="o">-</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">4.0</span><span class="p">])</span>
<span class="nf">gradient_descent</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">init_x</span><span class="o">=</span><span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>   <span class="c1"># array([-6.11110793e-10,  8.14814391e-10])
</span><span class="nf">gradient_descent</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">init_x</span><span class="o">=</span><span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">10.0</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>  <span class="c1"># array([2.34235971e+12, -3.96091057e+12])
</span><span class="nf">gradient_descent</span><span class="p">(</span><span class="n">function_2</span><span class="p">,</span> <span class="n">init_x</span><span class="o">=</span><span class="n">init_x</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-10</span><span class="p">,</span> <span class="n">step_num</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span> <span class="c1"># array([-2.99999994,  3.99999992])
</span></pre></table></code></div></div><h4 id="442-신경망에서의-기울기"><span class="mr-2">4.4.2 신경망에서의 기울기</span><a href="#442-신경망에서의-기울기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>기울기는 가중치 매개변수에 대한 손실 함수의 기울기</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">sys</span><span class="p">,</span> <span class="n">os</span>
<span class="n">github_url</span> <span class="o">=</span> <span class="s">'/Users/paul/Desktop/github/deep-learning-from-scratch-master/'</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">github_url</span><span class="p">)</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">common.functions</span> <span class="kn">import</span> <span class="n">softmax</span><span class="p">,</span> <span class="n">cross_entropy_error</span>
<span class="kn">from</span> <span class="n">common.gradient</span> <span class="kn">import</span> <span class="n">numerical_gradient</span>

<span class="k">class</span> <span class="nc">simpleNet</span><span class="p">:</span> <span class="c1"># x = 입력 데이터, t = 정답 레이블
</span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># 정규분포로 초기화
</span>    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">loss</span>
    
<span class="n">net</span> <span class="o">=</span> <span class="nf">simpleNet</span><span class="p">()</span>
<span class="nf">print</span><span class="p">(</span><span class="n">net</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>             <span class="c1"># 가중치 매개변수
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">])</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">net</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>                 <span class="c1"># [1.25184467 0.34731474 1.30469603]
</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>             <span class="c1"># 최댓값의 인덱스 = 2
</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>  <span class="c1"># 정답 레이블
</span><span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>           <span class="c1"># 2.125447431676497 
</span></pre></table></code></div></div><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">W</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">w</span><span class="p">:</span> <span class="n">net</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="n">dW</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">net</span><span class="p">.</span><span class="n">W</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">dW</span><span class="p">)</span>
</pre></table></code></div></div><h3 id="45-학습-알고리즘-구현하기"><span class="mr-2">4.5 학습 알고리즘 구현하기</span><a href="#45-학습-알고리즘-구현하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>전제: 신경망에는 적용 가능한 가중치와 편향이 있음. 이 값들을 훈련 데이터에 적응하도록 조정하는 것을 학습이라고 정의<li>1단계: 미니배치<ul><li>훈련 데이터 중 일부를 무작위로 가져오기<li>선별한 데이터를 미니배치라고 하고, 그 미니배치의 손실함수 값을 줄이는 것이 목표</ul><li>2단계: 기울기 산출<ul><li>미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기 구하기<li>기울기는 손실함수의 값을 가장 작게 하는 방향 제시</ul><li>3단계: 매개변수 갱신<ul><li>가중치 매개변수를 기울기 방향으로 아주 조금 갱신</ul><li>4단계: 1~3단계를 반복</ul><p><br /></p><ul><li>이 과정이 신경망 학습이 이뤄지는 순서. 경사하강법으로 매개변수를 갱신하는 방법<li>이때 데이터를 미니배치로 무작위로 선정하므로, <strong>확률적 경사 하강법 (Stochastic gradient descent, SGD)</strong>로 정의</ul><h4 id="451-2층-신경망-클래스-구현하기"><span class="mr-2">4.5.1 2층 신경망 클래스 구현하기</span><a href="#451-2층-신경망-클래스-구현하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">sys</span><span class="p">,</span> <span class="n">os</span>
<span class="n">github_url</span> <span class="o">=</span> <span class="s">'/Users/paul/Desktop/github/deep-learning-from-scratch-master/'</span>
<span class="n">sys</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">github_url</span><span class="p">)</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">common.functions</span> <span class="kn">import</span> <span class="n">sigmoid</span><span class="p">,</span> <span class="n">softmax</span><span class="p">,</span> <span class="n">cross_entropy_error</span>
<span class="kn">from</span> <span class="n">common.gradient</span> <span class="kn">import</span> <span class="n">numerical_gradient</span>

<span class="k">class</span> <span class="nc">TwoLayerNet</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">weight_init_std</span><span class="o">=</span><span class="mf">0.01</span><span class="p">):</span>
        
        <span class="c1"># 가중치 초기화
</span>        <span class="n">self</span><span class="p">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_init_std</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">output_size</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">W1</span><span class="p">,</span> <span class="n">W2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span>
        <span class="n">b1</span><span class="p">,</span> <span class="n">b2</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span>
        
        <span class="n">a1</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span>
        <span class="n">z1</span> <span class="o">=</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">a1</span><span class="p">)</span>
        <span class="n">a2</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span>
        <span class="n">y</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">a2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">y</span>
    
    <span class="c1"># x = 입력 데이터, t = 정답 레이블
</span>    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span> 
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="nf">cross_entropy_error</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="nf">float</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="k">return</span> <span class="n">accuracy</span>
    
    <span class="k">def</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">loss_W</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">self</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
        
        <span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="n">grads</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">])</span>
        <span class="n">grads</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">loss_W</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">grads</span>
    
<span class="n">net</span> <span class="o">=</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">net</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W1'</span><span class="p">].</span><span class="n">shape</span> <span class="c1"># (784, 100)
</span><span class="n">net</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b1'</span><span class="p">].</span><span class="n">shape</span> <span class="c1"># (100, )
</span><span class="n">net</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'W2'</span><span class="p">].</span><span class="n">shape</span> <span class="c1"># (100, 10)
</span><span class="n">net</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="s">'b2'</span><span class="p">].</span><span class="n">shape</span> <span class="c1"># (10, )
</span></pre></table></code></div></div><h4 id="452-미니배치-학습-구현하기"><span class="mr-2">4.5.2 미니배치 학습 구현하기</span><a href="#452-미니배치-학습-구현하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>미니배치 학습: 훈련 데이터 중 일부를 무작위로 꺼내고 (미니배치), 그 미니배치에 대해 경사법으로 매개변수를 갱신<li>아래 예시에서는 미니배치 크기를 100으로 설정<li>60,000개의 훈련 데이터에서 임의로 100개 데이터를 추리고, 이 데이터로 확률적 경사하강법으로 매개변수를 갱신<li>갱신 횟수 (반복 횟수)는 10,000번으로 설정하고, 갱신 할때마다 계산되는 손실 함수 값을 배열에 추가<li>결과를 보면, 학습 횟수가 늘어날수록 손실 함수의 값이 줄어드는 것을 알 수 있음<li>이 결과는 신경망의 가중치 매개변수가 서서히 데이터에 적응하면서 학습되고 있다는 것을 뜻함</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">dataset.mnist</span> <span class="kn">import</span> <span class="n">load_mnist</span>
<span class="kn">from</span> <span class="n">ch04.two_layer_net</span> <span class="kn">import</span> <span class="n">TwoLayerNet</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> \
    <span class="nf">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">train_loss_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 하이퍼파라미터
</span><span class="n">iters_num</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># 반복횟수
</span><span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># 미니배치 크기
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">network</span> <span class="o">=</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iters_num</span><span class="p">):</span>
    <span class="c1"># 미니배치 획득
</span>    <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    <span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    
    <span class="c1"># 기울기 계산
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    <span class="c1"># grad = network.gradient(x_batch, t_batch) # 성능 개선판
</span>    
    <span class="c1"># 매개변수 갱신
</span>    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'W1'</span><span class="p">,</span> <span class="s">'b1'</span><span class="p">,</span> <span class="s">'W2'</span><span class="p">,</span> <span class="s">'b2'</span><span class="p">):</span>
        <span class="n">network</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        
    <span class="c1"># 학습 경과 기록
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    <span class="n">train_loss_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
</pre></table></code></div></div><h4 id="453-시험-데이터로-평가하기"><span class="mr-2">4.5.3 시험 데이터로 평가하기</span><a href="#453-시험-데이터로-평가하기" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h4><ul><li>앞서 훈련된 모델은 훈련 데이터에 대한 결과이므로, 다른 데이터셋에서도 동작하는지 확인이 필요함<li>즉, 훈련 데이터에서만 학습한 결과인 오버피팅 되었는지를 확인해봐야 함<li>신경망 학습의 목표는 범용적인 능력을 익히는 것 즉, 일반화라고 할 수 있음</ul><div class="language-python highlighter-rouge"><div class="code-header"> <span data-label-text="Python"><i class="fas fa-code small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">dataset.mnist</span> <span class="kn">import</span> <span class="n">load_mnist</span>
<span class="kn">from</span> <span class="n">ch04.two_layer_net</span> <span class="kn">import</span> <span class="n">TwoLayerNet</span>

<span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">),</span> <span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span> <span class="o">=</span> \
    <span class="nf">load_mnist</span><span class="p">(</span><span class="n">normalize</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">one_hot_label</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">network</span> <span class="o">=</span> <span class="nc">TwoLayerNet</span><span class="p">(</span><span class="n">input_size</span><span class="o">=</span><span class="mi">784</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="c1"># 하이퍼파라미터
</span><span class="n">iters_num</span> <span class="o">=</span> <span class="mi">10000</span> <span class="c1"># 반복횟수
</span><span class="n">train_size</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">100</span>  <span class="c1"># 미니배치 크기
</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="n">train_loss_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">train_acc_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">test_acc_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># 1 에폭 당 반복 수
</span><span class="n">iter_per_epoch</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">train_size</span> <span class="o">/</span> <span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">iters_num</span><span class="p">):</span>
    <span class="c1"># 미니배치 획득
</span>    <span class="n">batch_mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">train_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">)</span>
    <span class="n">x_batch</span> <span class="o">=</span> <span class="n">x_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    <span class="n">t_batch</span> <span class="o">=</span> <span class="n">t_train</span><span class="p">[</span><span class="n">batch_mask</span><span class="p">]</span>
    
    <span class="c1"># 기울기 계산
</span>    <span class="n">grad</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">numerical_gradient</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    <span class="c1"># grad = network.gradient(x_batch, t_batch) # 성능 개선판
</span>    
    <span class="c1"># 매개변수 갱신
</span>    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">(</span><span class="s">'W1'</span><span class="p">,</span> <span class="s">'b1'</span><span class="p">,</span> <span class="s">'W2'</span><span class="p">,</span> <span class="s">'b2'</span><span class="p">):</span>
        <span class="n">network</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">-=</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span><span class="p">[</span><span class="n">key</span><span class="p">]</span>
        
    <span class="c1"># 학습 경과 기록
</span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">loss</span><span class="p">(</span><span class="n">x_batch</span><span class="p">,</span> <span class="n">t_batch</span><span class="p">)</span>
    <span class="n">train_loss_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    
    <span class="c1"># 1 에폭 당 정확도 계산
</span>    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">iter_per_epoch</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">train_acc</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">t_train</span><span class="p">)</span>
        <span class="n">test_acc</span> <span class="o">=</span> <span class="n">network</span><span class="p">.</span><span class="nf">accuracy</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">t_test</span><span class="p">)</span>
        <span class="n">train_acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span>
        <span class="n">test_acc_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">test_acc</span><span class="p">)</span>
        <span class="nf">print</span><span class="p">(</span><span class="s">"train acc, test acc: "</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">train_acc</span><span class="p">)</span> <span class="o">+</span> <span class="s">', '</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">test_acc</span><span class="p">))</span>
</pre></table></code></div></div><h3 id="46-정리"><span class="mr-2">4.6 정리</span><a href="#46-정리" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>손실함수를 기준으로 그 값이 가장 작아지는 가중치 매개변수를 찾아내는 것이 신경망 학습의 목표<li>기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용<li>훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가<li>가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치 값을 갱신 반복</ul><p><br /> <br /></p><blockquote><p>출처: 밑바닥부터 시작하는 딥러닝1 책 리뷰 -&gt; <a href="https://github.com/Paul-scpark/Deep-learning-from-scratch/blob/main/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5.ipynb">강의 내용 정리 깃허브 링크</a></p></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/review-it-book/'>Review - IT Book</a>, <a href='/categories/%EB%B0%91%EB%B0%94%EB%8B%A5%EB%B6%80%ED%84%B0-%EC%8B%9C%EC%9E%91%ED%95%98%EB%8A%94-%EB%94%A5%EB%9F%AC%EB%8B%9D1/'>밑바닥부터 시작하는 딥러닝1</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/%EB%B0%91%EC%8B%9C%EB%94%A51/" class="post-tag no-text-decoration" >밑시딥1</a> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep learning</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >Machine learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=%EB%B0%91%EC%8B%9C%EB%94%A51+4%EA%B0%95.+%EC%8B%A0%EA%B2%BD%EB%A7%9D+%ED%95%99%EC%8A%B5+-+Paul%27s+Insights&url=https%3A%2F%2Fpaul-scpark.github.io%2Fposts%2F%25EB%25B0%2591%25EC%258B%259C%25EB%2594%25A51-4%25EA%25B0%2595-%25EC%258B%25A0%25EA%25B2%25BD%25EB%25A7%259D%25ED%2595%2599%25EC%258A%25B5%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=%EB%B0%91%EC%8B%9C%EB%94%A51+4%EA%B0%95.+%EC%8B%A0%EA%B2%BD%EB%A7%9D+%ED%95%99%EC%8A%B5+-+Paul%27s+Insights&u=https%3A%2F%2Fpaul-scpark.github.io%2Fposts%2F%25EB%25B0%2591%25EC%258B%259C%25EB%2594%25A51-4%25EA%25B0%2595-%25EC%258B%25A0%25EA%25B2%25BD%25EB%25A7%259D%25ED%2595%2599%25EC%258A%25B5%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpaul-scpark.github.io%2Fposts%2F%25EB%25B0%2591%25EC%258B%259C%25EB%2594%25A51-4%25EA%25B0%2595-%25EC%258B%25A0%25EA%25B2%25BD%25EB%25A7%259D%25ED%2595%2599%25EC%258A%25B5%2F&text=%EB%B0%91%EC%8B%9C%EB%94%A51+4%EA%B0%95.+%EC%8B%A0%EA%B2%BD%EB%A7%9D+%ED%95%99%EC%8A%B5+-+Paul%27s+Insights" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="Paul-scpark/Paul-scpark.github.io" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-%ED%9B%84%EA%B8%B0/">프로그래머스 인공지능 데브코스 4기 수료 후기</a><li><a href="/posts/Coursera-Data-Engineering-1%EC%A3%BC%EC%B0%A8/">ETL & Data Pipelines with Shell, Airflow and Kafka 1주차</a><li><a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-1%EC%A3%BC%EC%B0%A8/">프로그래머스 인공지능 데브코스 1주차 정리 및 후기</a><li><a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-2%EC%A3%BC%EC%B0%A8/">프로그래머스 인공지능 데브코스 2주차 정리 및 후기</a><li><a href="/posts/Coursera-Data-Engineering-2%EC%A3%BC%EC%B0%A8/">ETL & Data Pipelines with Shell, Airflow and Kafka 2주차</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/k-digital-training/">K-digital training</a> <a class="post-tag" href="/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4/">인공지능 데브코스</a> <a class="post-tag" href="/tags/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4/">프로그래머스</a> <a class="post-tag" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81/">데이터 라벨링</a> <a class="post-tag" href="/tags/airflow/">Airflow</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/elt/">ELT</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-1%EA%B0%95-%ED%97%AC%EB%A1%9C-%ED%8C%8C%EC%9D%B4%EC%8D%AC/"><div class="card-body"> <em class="small" data-ts="1664600400" data-df="ll" > Oct 1, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>밑시딥1 1강. 헬로 파이썬</h3><div class="text-muted small"><p> 간단한 책 소개 이 책은 신경망과 딥러닝의 기본을 직접 만들면서 그 개념을 소개하고 있습니다. 일반적으로 잘 알려진 딥러닝 프레임워크인 TensorFlow나 PyTorch가 잘 되어 있긴 하지만, 밑바닥부터 딥러닝의 모델을 직접 만들어보는 것은 매우 중요하다고 생각합니다. 그런 부분에 대한 니즈가 항상...</p></div></div></a></div><div class="card"> <a href="/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-2%EA%B0%95-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0/"><div class="card-body"> <em class="small" data-ts="1664931600" data-df="ll" > Oct 5, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>밑시딥1 2강. 퍼셉트론</h3><div class="text-muted small"><p> 이번 글에서는 본격적으로 밑바닥부터 시작하는 딥러닝1 책에 대한 리뷰를 시작합니다. 딥러닝의 가장 기초 개념이라고 할 수 있는 퍼셉트론이 무엇인지에 대해 학습합니다. 또한 AND, NAND, OR 게이트 등을 통해서 퍼셉트론의 구조와 동작 원리도 함께 배울 수 있습니다. Chapter Title ...</p></div></div></a></div><div class="card"> <a href="/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-3%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D/"><div class="card-body"> <em class="small" data-ts="1665061200" data-df="ll" > Oct 6, 2022 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>밑시딥1 3강. 신경망</h3><div class="text-muted small"><p> 이번 글에서는 밑바닥부터 시작하는 딥러닝1 책의 3강에 대한 리뷰를 시작합니다. 앞서 딥러닝의 기초 개념인 퍼셉트론을 학습했는데, 이어서 딥러닝의 중요한 개념인 신경망에 대해 학습합니다. 퍼셉트론과 달리 신경망이 어떻게 동작하는지, 그리고 활성화 함수는 무엇인지 등에 대해서도 배울 예정입니다. 마지막으로는, 손글씨 숫자 이미지 데이터로 유명한 M...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/%EB%8C%80%ED%95%99%EC%9B%90%EC%83%9D-%EB%95%8C-%EC%95%8C%EC%95%98%EB%8D%94%EB%9D%BC%EB%A9%B4-%EC%A2%8B%EC%95%98%EC%9D%84-%EA%B2%83%EB%93%A4/" class="btn btn-outline-primary" prompt="Older"><p>대학원생 때 알았더라면 좋았을 것들 책 Review</p></a> <a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-5%EC%A3%BC%EC%B0%A8/" class="btn btn-outline-primary" prompt="Newer"><p>프로그래머스 인공지능 데브코스 5주차 정리 및 후기</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/Paul-scpark">Seongchan Park</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/k-digital-training/">K-digital training</a> <a class="post-tag" href="/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4/">인공지능 데브코스</a> <a class="post-tag" href="/tags/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4/">프로그래머스</a> <a class="post-tag" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81/">데이터 라벨링</a> <a class="post-tag" href="/tags/airflow/">Airflow</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/elt/">ELT</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
