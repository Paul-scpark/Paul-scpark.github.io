<!DOCTYPE html><html lang="ko" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="generator" content="Jekyll v4.3.2" /><meta property="og:title" content="LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)" /><meta property="og:locale" content="ko" /><meta name="description" content="LG Aimers AI 전문가 과정 강의 기록" /><meta property="og:description" content="LG Aimers AI 전문가 과정 강의 기록" /><link rel="canonical" href="https://paul-scpark.github.io/posts/LG-Aimers-%EB%94%A5%EB%9F%AC%EB%8B%9D/" /><meta property="og:url" content="https://paul-scpark.github.io/posts/LG-Aimers-%EB%94%A5%EB%9F%AC%EB%8B%9D/" /><meta property="og:site_name" content="Paul’s Insights" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2023-01-18T00:00:00+09:00" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-01-25T18:04:26+09:00","datePublished":"2023-01-18T00:00:00+09:00","description":"LG Aimers AI 전문가 과정 강의 기록","headline":"LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)","mainEntityOfPage":{"@type":"WebPage","@id":"https://paul-scpark.github.io/posts/LG-Aimers-%EB%94%A5%EB%9F%AC%EB%8B%9D/"},"url":"https://paul-scpark.github.io/posts/LG-Aimers-%EB%94%A5%EB%9F%AC%EB%8B%9D/"}</script><title>LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님) | Paul's Insights</title><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><meta name="apple-mobile-web-app-title" content="Paul's Insights"><meta name="application-name" content="Paul's Insights"><meta name="msapplication-TileColor" content="#da532c"><meta name="msapplication-config" content="/assets/img/favicons/browserconfig.xml"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" crossorigin><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css"><link rel="stylesheet" href="/assets/css/style.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/magnific-popup@1/dist/magnific-popup.min.css"> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get MODE_ATTR() { return "data-mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } static get ID() { return "mode-toggle"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } let self = this; /* always follow the system prefers */ this.sysDarkPrefers.addEventListener("change", () => { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.notify(); }); } /* constructor() */ get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode === ModeToggle.DARK_MODE; } get isLightMode() { return this.mode === ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer)) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } setDark() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_ATTR, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_ATTR); sessionStorage.removeItem(ModeToggle.MODE_KEY); } /* Notify another plugins that the theme mode has changed */ notify() { window.postMessage({ direction: ModeToggle.ID, message: this.modeStatus }, "*"); } } /* ModeToggle */ const toggle = new ModeToggle(); function flipMode() { if (toggle.hasMode) { if (toggle.isSysDarkPrefer) { if (toggle.isLightMode) { toggle.clearMode(); } else { toggle.setLight(); } } else { if (toggle.isDarkMode) { toggle.clearMode(); } else { toggle.setDark(); } } } else { if (toggle.isSysDarkPrefer) { toggle.setLight(); } else { toggle.setDark(); } } toggle.notify(); } /* flipMode() */ </script><body data-spy="scroll" data-target="#toc" data-topbar-visible="true"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" class="mx-auto"> <img src="/assets/img/main_image.png" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Paul's Insights</a></div><div class="site-subtitle font-italic">Space to learn, to record experiences.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tag ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center align-items-center"> <button class="mode-toggle btn" aria-label="Switch Mode"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/Paul-scpark" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github"></i> </a> <a href="https://www.linkedin.com/in/%EC%84%B1%EC%B0%AC-%EB%B0%95-35a84b219/" aria-label="linkedin" target="_blank" rel="noopener"> <i class="fab fa-linkedin"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['solver.paul','google.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a></div></div><div id="topbar-wrapper"><div id="topbar" class="container d-flex align-items-center justify-content-between h-100 pl-3 pr-3 pl-md-4 pr-md-4"> <span id="breadcrumb"> <span> <a href="/"> Home </a> </span> <span>LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..."> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper" class="d-flex justify-content-center"><div id="main" class="container pl-xl-4 pr-xl-4"><div class="row"><div id="core-wrapper" class="col-12 col-lg-11 col-xl-9 pr-xl-4"><div class="post pl-1 pr-1 pl-md-2 pr-md-2"><h1 data-toc-skip>LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)</h1><div class="post-meta text-muted"> <span> Posted <em class="" data-ts="1673967600" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jan 18, 2023 </em> </span> <span> Updated <em class="" data-ts="1674637466" data-df="ll" data-toggle="tooltip" data-placement="bottom"> Jan 25, 2023 </em> </span><div class="d-flex justify-content-between"> <span> By <em> <a href="https://github.com/Paul-scpark">Seongchan Park</a> </em> </span><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="2651 words"> <em>14 min</em> read</span></div></div></div><div class="post-content"><p>이번 글에서는 LG Aimers의 AI 전문가 과정에서 딥러닝에 대한 기본 개념과 대표적인 모형의 학습 원리를 학습합니다. 특히, 이미지와 언어 모델 학습을 위한 딥러닝 모델과 학습 원리를 배우게 될 것입니다.</p><hr /><h2 id="1-introduction-to-dnn"><span class="mr-2">1. Introduction to DNN</span><a href="#1-introduction-to-dnn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Artificial Intelligence &gt; Machine Learning &gt; Deep learning<li>하나하나의 뉴런들이 모여서, 하나의 신경망을 구성 (여러 Layers를 가지고 있음) = DNN<li>DNN 적용을 위해서는 빅데이터, 컴퓨팅 성능, 진보된 알고리즘 모델이 필요함<ul><li>영상 인식, 이미지 합성, 기계 번역, 챗봇, 자연어 처리, 주식 가격 예측 등에 적용</ul><li>퍼셉트론 (Perceptron): y = f(w0 + w1x1 + w2x2)<ul><li>AND, OR, XOR Gate<li>XOR Gate는 Single layer 퍼셉트론으로는 결과값을 낼 수 없음<li>Input layer - Hidden layer - Output layer (2-layer neural network)<li>Input layer - Hidden layer1 - Hidden layer2 - Output layer (3-layer neural network)</ul><li>Forward Propagation, Activation Function (Sigmoid, Softmax), Loss Function (MSE, Cross entropy loss)</ul><h2 id="2-training-nn"><span class="mr-2">2. Training NN</span><a href="#2-training-nn" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Gradient Descent: Loss Function을 최소화 시킬 수 있는 최적의 파라미터 값을 업데이트 하는 과정<ul><li>Loss function이 복잡한 경우에는, 수렴 속도가 늦어지는 경우도 존재<li>따라서 Original Gradient Descent 알고리즘을 다양한 형태로 변화시킴 (Momentum, Adagrad, Adam 등)</ul><li>NN의 학습 과정<ol><li>가장 처음으로는, 학습 데이터에 대해 Random Initialize 된 값으로 Forward Propagation을 수행<li>그 상태에서 Loss Function의 값을 최소화 시킬 수 있는 파라미터를 찾아나감 (Backward Propagation)</ol><li>Gradient Vanishing을 해결하기 위해 Tanh, ReLU 활성화 함수 등을 사용하게 됨<li>Batch Normalization (배치 정규화)<ul><li>각 배치 단위 별로 데이터가 다양한 분포를 가지더라도, 배치 별로 평균과 분산을 이용해 정규화 해주는 것<li>활성화 함수의 출력값을 정규화 하여 그 분포를 고르게 해주는 효과</ul></ul><h2 id="3-cnn-and-이미지-분류"><span class="mr-2">3. CNN and 이미지 분류</span><a href="#3-cnn-and-이미지-분류" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Fully Connected NN, Convolution NN (CNN - 이미지 처리에 사용), Recurrent NN (RNN - 시계열 데이터에 사용)<li>CNN은 특정 클래스에 존재할 수 있는 작은 특정 패턴들을 정의하고, 패턴들이 주어진 이미지에 있는지를 판단<li>매칭의 정도를 나타내는 결과값을 활성화 지도 (Activation map)이라고 하고, 이는 특정 Convolution Filter를 주어진 입력 이미지에 가능한 모든 위치에 오버랩을 시켜서 매칭되는 정도를 나타낸 것<li>Channel, Filter, Pooling Layer (Max, Average)<ul><li>일반적인 구조: Conv - ReLU - Conv - ReLU - Pooling - Conv - ReLU - Pooling - FC</ul><li>Hyperparameters<ul><li>Convolution: Number of filters, Size of filters<li>Pooling: Window size, Window stride<li>Fully Connected: Number of layers, Number of neurons</ul><li>CNN 모델은 어느 종류의 layer를 어느 위치에, 어느 순서로 배치하는가가 중요한 문제. 이에 대하여 많은 연구자들이 CNN 모델에 대한 architecture를 사전에 정의한 것들이 있음 = AlexNet, VGGNet, GoogleNet, ResNet<ul><li>VGGNet: 각각의 Conv layer에서 사용하는 필터의 가로, 세로 size를 3 by 3으로 고정하고, layer를 깊게 쌓아서 문제를 해결한 알고리즘<li>ResNet (Residual Network): Conv layer를 통해 나온 output에 layer를 또 추가하는 것. layer를 추가적으로 쌓아서 생기는 비효율성에 대하여 일부 layer는 skip 할 수 있도록 하는 skip connection을 추가한 알고리즘</ul><li>CNN 알고리즘의 꾸준한 성능 개선과 함께, 모델의 layer 개수도 꾸준히 증가 됨 (AlexNet은 8개, ResNet은 152개 층)</ul><h2 id="4-seq2seq-with-attention"><span class="mr-2">4. Seq2Seq with Attention</span><a href="#4-seq2seq-with-attention" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Recurrent Neural Network (RNN)<ul><li>one to one, one to many, many to one, many to many</ul><li>RNN 모델의 기울기 소실 및 폭발의 문제를 해결하기 위한 LSTM, GRU 모델 등장<li>Seq2Seq, Encoder &amp; Decoder<ul><li>Original Seq2Seq 모델은 매 time step 마다 생성되는 RNN의 각 time step의 Hidden state vector는 같은 dimension으로 이뤄져야 한다는 제약 조건이 있음. Output vector가 다시 Input vector로 사용되기 위해. 이러한 경우에 축적해야 되는 정보가 시간이 길어짐에 따라서 점점 많아지지만, 정보는 항상 똑같은 개수의 dimension에 저장하게 되어 정보를 유실하게 된다는 한계점이 있음<li>이러한 Bottleneck 문제를 해결하고자 attention 이라는 추가적인 모델이 Seq2Seq에 도입<li>입력 sequence에 주어지는 데이터를 encoder에서 인코딩 한 후, decoder에서는 encoder의 마지막 time step의 Hidden state vector 만을 입력으로 받지 않고, 그 입력과 더불어 decoder의 각 time step에서 encoder에서 나온 여러 인코딩 데이터 중 필요로 하는 것을 가지고 가서 예측에 사용<li>기본적 구조는 encoder와 decoder가 존재하고, encoder의 마지막 time step의 Hidden state vector가 decoder의 가장 최초의 Hidden state vector인 h0로 사용됨. 그런데 여기서 어떤 decoder의 각 time step에서 추가적으로 encoder에 있는 여러 Hidden state vector로부터 필요한 정보를 취사선택해서 가장 유관하다고 생각하는 정보를 추가적인 입력으로 사용함</ul></ul><h2 id="5-transformer"><span class="mr-2">5. Transformer</span><a href="#5-transformer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Transformer 모델은 Seq2Seq with attention 모델의 개선된 버전<ul><li>Solving long-term dependency problem<li>Seq2Seq with attention 모델에서 Encoder와 Decoder가 RNN 기반의 모델로 구성됨<li>Transformer 모델은 Encoder와 Decoder에서도 attention 기반으로 동작하는 모델<ul><li>RNN 기반의 모델들은 Long-term Dependency 문제가 있음 (오랜 시계열을 거쳐 정보가 소실 될 수 있음)<li>Transformer에서는 가까이 있거나 멀리 있는 정보를 접근해서 필요한 정보를 사용할 수 있게 됨</ul><li>Self-attention, Multi-head Attention</ul><li>Layer Normalization은 각 단어에서 발견된 특정한 dimension으로 이루어진 벡터의 각 원소에 평균과 분산을 계산<ul><li>그리고 각 단어 내에서 발생된 벡터 각각의 원소 값의 평균과 분산이 0과 1이 되도록 정규화<li>dimension 혹은 layer 내의 각 노드 별로 학습된 trainable parameters로 affine transformation 수행<li>이는 Batch Normalization과 비슷하게 학습을 조금 더 안정화시키고, 성능을 개선시킬 수 있음</ul><li>Positional Encoding은 각 단어의 입력 벡터에 몇 번째 순서에서 나타났다는 알려 줄 수 있는 정보<ul><li>즉, 각 단어의 순서나 위치를 구분할 수 있게 되는 것</ul><li>Transformer 모델은 기존에 RNN 및 Convolution 기반의 Sequence를 인코딩하는 모델보다 좋은 성능을 보임<ul><li>이렇게 가능하게 된 이유는 Long term dependency를 근본적으로 해결했기 때문<li>자연어처리 외에도 다양한 도메인에 적용되고 있음<li>또한 Transformer에서 제한된 Layer 수를 더 많이 늘리되, Block 자체의 설계나 디자인은 그대로 계승하고, Model의 사이즈를 점차 늘리면서 Self-supervised 방법론을 추가로 사용하여 대규모 데이터를 학습한 사전 학습 모델을 활용할 수 있음</ul></ul><h2 id="6-self-supervised-learning"><span class="mr-2">6. Self-supervised Learning</span><a href="#6-self-supervised-learning" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ul><li>Self-supervised Learning (자가지도학습)은 데이터의 Labeling 과정 없이도 Raw data 만으로 모델을 학습<ul><li>즉, Raw 데이터나, 별도의 추가적인 Label 없이 입력 데이터만으로 입력 데이터 중에 일부를 가려놓고, 가려진 입력 데이터를 주었을 때 가려진 부분을 잘 복원 혹은 예측하도록 하여, 주어진 입력 데이터의 일부를 예측하도록 모델을 학습 (Computer Vision 분야의 Inpainting Task, Zigsaw Puzzle Task 등)</ul><li>Transfer Learning의 기본 아이디어는 자가지도학습을 통해 만들어진 (Inpainting) 모델의 앞쪽 Layer는 물체를 잘 인식하기 위해 필요로 하는 유의미한 패턴을 추출할 수 있도록 학습됨. 그리고 뒤쪽의 Layer는 실제 풀고자 하는 문제를 해결할 수 있도록 특화되어 학습이 될 것. 따라서 대규모 데이터로 학습된 모델에 풀고자 하는 문제를 해결할 수 있는 Layer를 뒤에 덧붙여서 모델을 재학습<li>BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding)<ul><li>BERT 모델은 Transformer의 Encoder 역할을 수행<li>입력 문장을 BERT 모델의 입력 Sequence로 제공하되, 입력 데이터의 일부를 가리고 그것을 예측하도록 함<li>두 개의 문장을 주고, 연속되게 등장하여 두 문장 사이에 어떤 의미 관계가 있는지 판단 (Next Sentence 예측)<li>CLS, MASK, SEP 토큰 추가</ul><li>Masked Language Model (MLM)<ul><li>주어진 입력 문장에 대해, 특정 확률에 따라 각 단어를 Masked token으로 대체할지 전처리 수행<li>약 15% 비율의 단어를 Mask 단어로 대체하되, 거기서 80%는 Mask 단어, 10% 랜덤한 단어, 10%는 그대로 유지<li>Too little masking: Too expensive to train<li>Too much masking: Not enough to capture the given context</ul><li>Next Sentence Prediction (NSP)<ul><li>두 개의 문장을 SEP token으로 구분하여 제공하고, CLS token으로부터 인코딩 된 Hidden state vector의 Binary classification 결과를 예측</ul><li>Further Details of BERT<ul><li>Model Architecture<ul><li>BERT BASE: L = 12, H = 768, A = 12<li>BERT LARGE: L = 24, H = 1024, A = 16</ul><li>Input Representation<ul><li>WordPiece Embedding (30,000 WordPiece)<li>Learned positional embedding<li>CLS (Classification embedding)<li>Packed sentence embedding (SEP)<li>Segment Embedding</ul><li>Pre-training Tasks<ul><li>Masked LM<li>Next sentence prediction</ul></ul><li>GPT (Generative Pre-Trained Transformer)<ul><li>GPT 모델은 Transformer의 Decoder 역할을 수행<li>주어진 텍스트 데이터들에 대해 문장을 가져와서 특정 문장의 일부만 주어졌을 때, 다음에 나타날 단어를 예측. 그리고 그 다음 단어가 주어졌을 때, 그 다음 단어를 예측하는 Word level의 Language modeling task를 학습<li>Zero-shot Summarization으로도 활용 가능<ul><li>일반적으로 Summarization을 수행하기 위해서는 사전 학습된 모델을 가져와서 target task인 Summarization을 목적으로 입력 지문과 정답 요약 문장인 Labeled 학습 데이터를 가지고 주어진 모델을 Fine-tuning 하는데, GPT는 그런 과정 없이 Summarization을 수행할 수 있어서 Zero-shot이라고 함</ul><li>GPT3는 BERT와 GPT2와 다르게, 훨씬 더 많은 Layer 수를 가지고, 학습에 필요로 하는 1,750억 개의 파라미터<ul><li>Zero-shot, One-shot, Few-shot Learning</ul></ul></ul></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/education/'>Education</a>, <a href='/categories/lg-aimers-2%EA%B8%B0/'>LG Aimers 2기</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/deep-learning/" class="post-tag no-text-decoration" >Deep learning</a> <a href="/tags/machine-learning/" class="post-tag no-text-decoration" >Machine learning</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=LG+Aimers+2%EA%B8%B0+%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5+-+%EB%94%A5%EB%9F%AC%EB%8B%9D+%28KAIST+%EC%A3%BC%EC%9E%AC%EA%B1%B8+%EA%B5%90%EC%88%98%EB%8B%98%29+-+Paul%27s+Insights&url=https%3A%2F%2Fpaul-scpark.github.io%2Fposts%2FLG-Aimers-%25EB%2594%25A5%25EB%259F%25AC%25EB%258B%259D%2F" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=LG+Aimers+2%EA%B8%B0+%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5+-+%EB%94%A5%EB%9F%AC%EB%8B%9D+%28KAIST+%EC%A3%BC%EC%9E%AC%EA%B1%B8+%EA%B5%90%EC%88%98%EB%8B%98%29+-+Paul%27s+Insights&u=https%3A%2F%2Fpaul-scpark.github.io%2Fposts%2FLG-Aimers-%25EB%2594%25A5%25EB%259F%25AC%25EB%258B%259D%2F" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fpaul-scpark.github.io%2Fposts%2FLG-Aimers-%25EB%2594%25A5%25EB%259F%25AC%25EB%258B%259D%2F&text=LG+Aimers+2%EA%B8%B0+%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5+-+%EB%94%A5%EB%9F%AC%EB%8B%9D+%28KAIST+%EC%A3%BC%EC%9E%AC%EA%B1%B8+%EA%B5%90%EC%88%98%EB%8B%98%29+-+Paul%27s+Insights" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i id="copy-link" class="fa-fw fas fa-link small" data-toggle="tooltip" data-placement="top" title="Copy link" data-title-succeed="Link copied successfully!"> </i> </span></div></div></div><script src="https://utteranc.es/client.js" repo="Paul-scpark/Paul-scpark.github.io" issue-term="pathname" theme="github-dark" crossorigin="anonymous" async> </script></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted"><div class="access"><div id="access-lastmod" class="post"><div class="panel-heading">Recently Updated</div><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-%ED%9B%84%EA%B8%B0/">프로그래머스 인공지능 데브코스 4기 수료 후기</a><li><a href="/posts/Coursera-Data-Engineering-1%EC%A3%BC%EC%B0%A8/">ETL & Data Pipelines with Shell, Airflow and Kafka 1주차</a><li><a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-1%EC%A3%BC%EC%B0%A8/">프로그래머스 인공지능 데브코스 1주차 정리 및 후기</a><li><a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-2%EC%A3%BC%EC%B0%A8/">프로그래머스 인공지능 데브코스 2주차 정리 및 후기</a><li><a href="/posts/Coursera-Data-Engineering-2%EC%A3%BC%EC%B0%A8/">ETL & Data Pipelines with Shell, Airflow and Kafka 2주차</a></ul></div><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/k-digital-training/">K-digital training</a> <a class="post-tag" href="/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4/">인공지능 데브코스</a> <a class="post-tag" href="/tags/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4/">프로그래머스</a> <a class="post-tag" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81/">데이터 라벨링</a> <a class="post-tag" href="/tags/airflow/">Airflow</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/elt/">ELT</a></div></div></div><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@1.0.1/dist/bootstrap-toc.min.js"></script><div id="toc-wrapper" class="pl-0 pr-4 mb-5"><div class="panel-heading pl-3 pt-2 mb-2">Contents</div><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 pl-3 pr-3 pr-xl-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/LG-Aimers-%EC%84%A4%EB%AA%85%EA%B0%80%EB%8A%A5%ED%95%9C-AI/"><div class="card-body"> <em class="small" data-ts="1674054000" data-df="ll" > Jan 19, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>LG Aimers 2기 Explainable AI (서울대학교 문태섭 교수님)</h3><div class="text-muted small"><p> 이번 글에서는 LG Aimers의 AI 전문가 과정에서 설명가능한 AI에 대하여 학습합니다. 머신러닝은 크고 복잡한 데이터를 이해하고, 이들 간의 관계성을 살펴보는 기술이지만 본질적으로 해석 가능성을 제한하는 블랙박스인 경우가 많습니다. 현실에서 이것을 사용할 때는 그에 따른 해석이 요구되는데, 따라서 그 한계를 보완하기 위해 Explainable A...</p></div></div></a></div><div class="card"> <a href="/posts/LG-Aimers-%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/"><div class="card-body"> <em class="small" data-ts="1674486000" data-df="ll" > Jan 24, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>LG Aimers 2기 인과추론 (서울대학교 이상학 교수님)</h3><div class="text-muted small"><p> 이번 글에서는 LG Aimers의 AI 전문가 과정에서 인과성에 대해 추론하고, 경험적 데이터를 사용해 인과 관계를 결정하는 방법을 익히게 됩니다. 이를 통해 데이터를 생성한 프로세스에 대해 만들어야 하는 필수 가정과 이러한 가정이 합리적인지 평가하는 방법, 마지막으로 추정되는 양을 해석하는 방법을 학습합니다. 1. Causality 인과성...</p></div></div></a></div><div class="card"> <a href="/posts/LG-Aimers-%EC%8B%9C%EA%B3%84%EC%97%B4-%EB%B6%84%EC%84%9D/"><div class="card-body"> <em class="small" data-ts="1674572400" data-df="ll" > Jan 25, 2023 </em><h3 class="pt-0 mt-1 mb-3" data-toc-skip>LG Aimers 2기 시계열 분석 (고려대학교 강필성 교수님)</h3><div class="text-muted small"><p> 이번 글에서는 LG Aimers의 AI 전문가 과정에서 시계열 데이터의 순차적 특성을 고려한 모형과 그 학습 원리를 배웁니다. 모형의 어떠한 특성이 시계열 데이터의 특성을 학습에 반영하게 되는지 그리고 어떻게 모델의 성능을 향상시킬 수 있는지를 배울 수 있습니다. 1. 순환신경망 기반의 시계열 데이터 회귀 Non-Sequential vs Seq...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-16%EC%A3%BC%EC%B0%A8/" class="btn btn-outline-primary" prompt="Older"><p>프로그래머스 인공지능 데브코스 16주차 정리 및 후기</p></a> <a href="/posts/LG-Aimers-%EC%84%A4%EB%AA%85%EA%B0%80%EB%8A%A5%ED%95%9C-AI/" class="btn btn-outline-primary" prompt="Newer"><p>LG Aimers 2기 Explainable AI (서울대학교 문태섭 교수님)</p></a></div></div></div><footer class="row pl-3 pr-3"><div class="col-12 d-flex justify-content-between align-items-center text-muted pl-0 pr-0"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://github.com/Paul-scpark">Seongchan Park</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-sm-11 post-content"><div id="search-hints"><div id="access-tags"><div class="panel-heading">Trending Tags</div><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/ai/">AI</a> <a class="post-tag" href="/tags/deep-learning/">Deep learning</a> <a class="post-tag" href="/tags/machine-learning/">Machine learning</a> <a class="post-tag" href="/tags/k-digital-training/">K-digital training</a> <a class="post-tag" href="/tags/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4/">인공지능 데브코스</a> <a class="post-tag" href="/tags/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4/">프로그래머스</a> <a class="post-tag" href="/tags/%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81/">데이터 라벨링</a> <a class="post-tag" href="/tags/airflow/">Airflow</a> <a class="post-tag" href="/tags/data-engineering/">Data Engineering</a> <a class="post-tag" href="/tags/elt/">ELT</a></div></div></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><script src="https://cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js"></script> <script> $(function() { function updateMermaid(event) { if (event.source === window && event.data && event.data.direction === ModeToggle.ID) { const mode = event.data.message; if (typeof mermaid === "undefined") { return; } let expectedTheme = (mode === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* Re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } let initTheme = "default"; if ($("html[data-mode=dark]").length > 0 || ($("html[data-mode]").length == 0 && window.matchMedia("(prefers-color-scheme: dark)").matches ) ) { initTheme = "dark"; } let mermaidConf = { theme: initTheme /* <default|dark|forest|neutral> */ }; /* Markdown converts to HTML */ $("pre").has("code.language-mermaid").each(function() { let svgCode = $(this).children().html(); $(this).addClass("unloaded"); $(this).after(`<div class=\"mermaid\">${svgCode}</div>`); }); mermaid.initialize(mermaidConf); window.addEventListener("message", updateMermaid); }); </script><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a><div id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-animation="true" data-autohide="false"><div class="toast-header"> <button type="button" class="ml-2 ml-auto close" data-dismiss="toast" aria-label="Close"> <span aria-hidden="true">&times;</span> </button></div><div class="toast-body text-center pt-0"><p class="pl-2 pr-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></div><script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); </script> <script src="https://cdn.jsdelivr.net/combine/npm/magnific-popup@1/dist/jquery.magnific-popup.min.js,npm/lozad/dist/lozad.min.js,npm/clipboard@2/dist/clipboard.min.js"></script> <script src="https://cdn.jsdelivr.net/combine/npm/dayjs@1/dayjs.min.js,npm/dayjs@1/locale/ko.min.js,npm/dayjs@1/plugin/relativeTime.min.js,npm/dayjs@1/plugin/localizedFormat.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script> /* see: <https://docs.mathjax.org/en/latest/options/input/tex.html#tex-options> */ MathJax = { tex: { inlineMath: [ /* start/end delimiter pairs for in-line math */ ['$','$'], ['\\(','\\)'] ], displayMath: [ /* start/end delimiter pairs for display math */ ['$$', '$$'], ['\\[', '\\]'] ] } }; </script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"> </script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.bundle.min.js"></script> <script defer src="/app.js"></script>
