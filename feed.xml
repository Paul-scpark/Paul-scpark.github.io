<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Paul's Insights</title>
    <description></description>
    <link>https://paul-scpark.github.io/</link>
    <atom:link href="https://paul-scpark.github.io/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Thu, 23 Feb 2023 00:25:43 +0900</pubDate>
    <lastBuildDate>Thu, 23 Feb 2023 00:25:43 +0900</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 4기 수료 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스 4기의 수료 후기를 적어보려고 합니다. &lt;br /&gt;
약 5개월 정도 되는 시간 동안 참여했던 이 프로그램을 돌아보고, 나름대로 간단한 회고를 해보고자 합니다. &lt;br /&gt;
부족한 부분들도 많이 있었지만, 많은 분들과 새로운 것들을 배워갈 수 있는 좋은 기회가 되었던 것 같습니다! &lt;br /&gt;
프로그래머스에서 주관하는 데브코스 외에도 인공지능과 관련한 부트캠프에 참여하시는 분들께 도움이 되었으면 좋겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-돌아보다&quot;&gt;1. 돌아보다&lt;/h2&gt;

&lt;p&gt;프로그램 합격 후에 &lt;a href=&quot;https://paul-scpark.github.io/posts/%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%98%EB%A8%B8%EC%8A%A4-AI-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4/&quot;&gt;후기를 담은 글&lt;/a&gt;을 블로그에 올렸던 것이 엊그제 같은데, 벌써 5개월이라는 시간이 지났습니다. &lt;br /&gt;
K-digital training으로 참여했던 이 과정에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;스크랩핑, 머신러닝 및 딥러닝 (NLP, CV, Rec Sys), SQL, Spark 등&lt;/code&gt; 다양한 내용들을 학습할 수 있었습니다. 또한 가장 마지막에는 지금까지 배운 내용으로 최종 프로젝트를 수행하며, 결과물을 만들어보았습니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;약 1년 정도 다녔던 스타트업을 퇴사하고 이 프로그램에 참여하면서 합격 후기 글을 다음과 같이 적었습니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;저는 약 1년 정도 다녔던 핀테크 스타트업을 퇴사하고, 이번 프로그램에 참여하는 것으로 결정했습니다. &lt;br /&gt;
회사를 다니면서 여러가지 고민이 많이 있었는데, 이러한 고민들을 조금 내려놓고 개인의 시간을 가지면서 &lt;br /&gt;
제가 하는 분야에 대한 전문성을 기르고 싶다는 생각을 하게 되었습니다. &lt;br /&gt;
지금도 여전히 고민은 진행중이고, 향후 진로에 대해서도 취업과 대학원 진학, 창업 등으로 고민을 많이 하고 있지만, &lt;br /&gt;
앞서 이야기 했듯 어느 방향으로 가든 제 분야에 대한 전문성과 실력을 기르고 싶다는 마음이 가장 큰 것 같습니다. &lt;br /&gt;
이번 프로그램에서는 아래와 같이 계획과 목표를 세우고 참여해보고 싶습니다.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;딥러닝의 주요한 모델들에 대한 기본적인 이론 및 응용력 학습하기&lt;/li&gt;
    &lt;li&gt;데이터 엔지니어링 분야에 대하여 학습해보고, 해당 분야에 대해 조금 더 구체화 시켜보기&lt;/li&gt;
    &lt;li&gt;팀원들과 함께하는 프로젝트를 통해 유의미한 성과 내보기 (공모전, 논문, 특허 등)&lt;/li&gt;
    &lt;li&gt;내 진로, 향후 계획 등에 대해서 조금 더 진지하게 고민할 수 있는 시간 갖기&lt;/li&gt;
    &lt;li&gt;참여하는 약 40명 정도의 교육생 분들과 이야기 나누고, 교제하며 서로 동기부여 받기&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;프로그램 시작 전, 여러 기대와 꿈을 가지고 고민하며 글을 적었던 것이 기억 납니다. 5개월 이라는 시간이 지난 지금, 과연 이 목표와 계획들은 어떻게 이뤄졌는지 돌아보고 싶습니다. 프로그램에 참여하면서 수업 외적으로 가장 많은 시간을 들여 고민했던 것은 &lt;strong&gt;진로와 직무에 대한 부분&lt;/strong&gt;이었습니다. 이 캠프의 기간에 맞물려, 많은 기업들에서 AI 분야를 비롯하여 개발자 채용의 문이 좁아지고 있다는 소식들을 접하게 되면서 오히려 조금 더 객관적으로 정말로 내가 하고 싶은 것은 무엇인지 계속 고민할 수 있었던 것 같습니다. 개인적으로는, 어떻게 하는지 (Know-how) 보다, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;무엇을, 왜 하는지 (Know-why)&lt;/code&gt;가 더 중요하다고 생각하기 때문에 데이터 분야 안에서도 이 부분을 꾸준히 고민하려고 했던 것 같습니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이러한 생각 때문인지 프로젝트를 하는데 있어서도, 기술적으로 할 수 있는 프로젝트 보다는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;사회적으로 필요한 프로젝트&lt;/code&gt;를 하고 싶었습니다. 사람들의 삶에서 여전히 존재하는 불편함을 데이터로 해결해보고자 하는 목표 속에, 이를 수행할 수 있는 공모전과 프로젝트를 수행할 수 있었습니다. 사람들의 데이터 이용 과정에서 발생하는 불편함을 해소하기 위하여 ‘데이터 아카이브 서비스’를 개발할 수 있었는데, 자세한 내용은 이 &lt;a href=&quot;https://chrome-beryllium-bd4.notion.site/e4280a322baa42fab5499b1f86bf811e&quot;&gt;링크&lt;/a&gt;를 통해 확인할 수 있습니다. 공공 데이터를 사용해보았던 분들이라면, 어느 정도 공감할 수 있는 문제 정의를 바탕으로 감사하게 &lt;strong&gt;2022년도 ETRI OPEN API 공모전에서 우수상을, 데브코스 최종 프로젝트에서 최우수상&lt;/strong&gt;을 받을 수 있었습니다.&lt;/p&gt;

&lt;table align=&quot;center&quot;&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;img src=&quot;../../assets/img/post_img/프로그래머스_수상.png&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;이 외에도 개인적으로 관심 있었던 ‘데이터 엔지니어링’ 분야에 대해서 &lt;a href=&quot;https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka/home/info&quot;&gt;coursera 강의&lt;/a&gt;를 통해 airflow와 kafka를 활용하여 기초적인 데이터 엔지니어링 분야를 학습할 수 있었습니다. 개인적으로 최종 프로젝트에서 이 부분도 고려하여 완성도를 높이고 싶었지만, 시간적인 제약 속에서 아쉽게 적용하지는 못했습니다. 그럼에도 개인적으로 airflow를 통해서 자동화 된 ETL 프로세스를 만들어 볼 계획입니다. 한편, 이번 데브코스의 정식 프로그램의 95% 이상은 온라인으로 진행되었고, 나머지는 팀원들의 재량으로 오프라인으로 모인다면 장소를 지원해주는 형식이었습니다. 저는 프로젝트의 전체적인 관리를 하면서, 팀원 분들과 오프라인으로 프로젝트 할 수 있도록 제안하며 그래도 직접 만났던 분들과는 여전히 좋은 교제를 나누고 있는 것 같습니다. 비록 목표했던 모든 분들과 이야기를 나눠보지는 못했지만, 온라인이라는 제한적인 상황에서 서로의 진로와 꿈, 고민들을 나눌 수 있는 (소수지만) 깊은 관계를 맺을 수 있는 팀원 분들을 알게 되어 정말 감사한 것 같습니다.&lt;/p&gt;

&lt;h2 id=&quot;2-앞으로-나아가다&quot;&gt;2. 앞으로 나아가다&lt;/h2&gt;

&lt;p&gt;길다면 길고, 짧다면 짧았던 지난 5개월을 마무리하면서 가장 많이 들었던 생각은 ‘이제는 정말로 무엇을 하고 싶다’는 생각이었습니다. 현재 뚜렷하게 제 신분을 설명할 수 없다는 한계 속에서 이러한 생각이 들었을 수도 있겠지만, 그럼에도 불구하고, 대학원에서 정말로 내가 하고 싶은 연구나, 가고 싶은 회사에서 새로운 부가가치를 만들어내는 일을 하고 싶다는 생각이 정말로 많이 들었던 것 같습니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 고민 속에서 사람과 기술 사이의 관계를 고민하는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;HCI (Human Computer Interaction)&lt;/code&gt; 이라는 새로운 분야를 알게 되었습니다. 최근 ChatGPT 등과 같이 사람이 생각하는 것보다 더 빠르게 발전하는 여러 기술들 속에, 유용한 기술이라는 것은 사람들에게 더 큰 가치를 제공하는 것임을 깨닫습니다. 즉, 기술 그 자체에 대한 학습 역시 중요하겠지만, 그것을 사용하는 사람에 대해서도 함께 공부할 수 있는 이 분야가 중요할 것이라고 생각합니다. 그래서 아직은 잘 모르지만 이 분야에 대해서 공부하고, 논문을 찾아보며 제가 기여할 수 있는 부분은 어떠한 것인지 고민해보고 있습니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이와 더불어, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;데이터로 새로운 부가가치를 만들어내는데 기여하는 좋은 문화를 갖는 몇몇 기업&lt;/code&gt;들을 찾아보면서 어떤 직무 속에 내가 기여할 수 있을지에 대한 것들을 고민하고 있습니다. 기술로 세상을 더 선하게 변화시키고 싶다는 개인적인 비전을 달성하기 위하여 지금까지 배웠던 것들을 잘 정리하고, 준비하여 새로운 도전을 시작해보겠습니다. &lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 글을 읽으시는 모든 분들의 꿈을 진심으로 응원합니다. 감사합니다!&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Feb 2023 21:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-%ED%9B%84%EA%B8%B0/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-%ED%9B%84%EA%B8%B0/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>ETL &amp; Data Pipelines with Shell, Airflow and Kafka 4주차</title>
        <description>&lt;p&gt;이번 글에서는 &lt;a href=&quot;https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka/home/info&quot;&gt;Coursera의 ETL and Data Pipelines with Shell, Airflow and Kafka (IBM)&lt;/a&gt; 4주차 강의를 정리합니다. &lt;br /&gt;
이 강좌는 ETL 및 ELT 데이터 파이프라인에 대해 학습하며, Airflow와 Kafka 등을 이용해 이를 배우게 됩니다. &lt;br /&gt;
4주차에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Kafka를 이용해서 스트리밍 파이프라인을 구축하는 방법&lt;/code&gt;에 대해 공부합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-데이터-학습-목표&quot;&gt;1. 데이터 학습 목표&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Kafka가 이벤트 스트리밍 플랫폼 (ESP)으로 작동하는 방식 학습&lt;/li&gt;
  &lt;li&gt;Kafka의 핵심 구성 요소 학습&lt;/li&gt;
  &lt;li&gt;Kafka의 Stremas API가 무엇이고, 장점은 무엇인지 등에 대해 학습&lt;/li&gt;
  &lt;li&gt;Kafka-python 클라이언트를 통해 Kafka 서버에서 작업 실행시켜보기&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-분산-이벤트-스트리밍-플랫폼&quot;&gt;2. 분산 이벤트 스트리밍 플랫폼&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Event 라는 것은 주목할 만한 일이 일어나고 있다는 것을 의미&lt;/li&gt;
  &lt;li&gt;Event Streaming 맥락에서, Event는 시간이 지남에 따라 엔티티의 관찰 가능한 상태를 설명하는 데이터 유형
    &lt;ul&gt;
      &lt;li&gt;자동차의 GPS 좌표, 방의 온도, 환자의 혈압 측정, 애플리케이션에서 RAM 사용량 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Event는 특별한 데이터의 타입으로 형식이 서로 다른데, 일반적인 3가지는 다음과 같음
    &lt;ul&gt;
      &lt;li&gt;Primitive (원시 유형): 일반 텍스트, 숫자, 날짜 등&lt;/li&gt;
      &lt;li&gt;Key-value Pairs: List, Tuple, JSON, XML, Bytes 등&lt;/li&gt;
      &lt;li&gt;Key-value with a Timestamp: 이벤트를 타임 스탬프와 연결하여 시간에 민감하게 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Event Source와 Event Destination 사이의 연속적 이벤트 전송을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Event Streaming&lt;/code&gt; 이라고 함
    &lt;ul&gt;
      &lt;li&gt;Event Source에는 Sensors, Devices, Applications 등과 같이 데이터를 만드는 대상&lt;/li&gt;
      &lt;li&gt;Event Destination는 File Systems, Databases 등과 같이 Event Source가 전송되는 대상&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실제로, Event Streaming은 다양하고, 분산된 Event Source와 Destination으로 복잡한 문제
    &lt;ul&gt;
      &lt;li&gt;데이터 전송 파이프라인의 프로토콜은 다음과 같은 것들이 있음
        &lt;ul&gt;
          &lt;li&gt;FTP: File Transfer Protocol&lt;/li&gt;
          &lt;li&gt;HTTP: Hypertext Transfer Protocol&lt;/li&gt;
          &lt;li&gt;JDBC: Java Database Connectivity&lt;/li&gt;
          &lt;li&gt;SCP: Secure Copy&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;또한 Event Destination은 동시에 Event Source가 될 수도 있음&lt;/li&gt;
      &lt;li&gt;다양한 Event Source와 Destination을 처리하기 위해서는, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ESP (Event Stream Platform)&lt;/code&gt;을 사용해야 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://media.geeksforgeeks.org/wp-content/uploads/FTP.jpg&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://www.geeksforgeeks.org/file-transfer-protocol-ftp-in-application-layer/&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;ESP (Event Stream Platform)
    &lt;ul&gt;
      &lt;li&gt;Event Source와 Destination 사이에서 중간 계층 역할을 하면서 이벤트 기반 ETL을 처리하기 위한 인터페이스&lt;/li&gt;
      &lt;li&gt;따라서 Event Source를 개별 Event Destination으로 전달하지 않고, ESP로 전달&lt;/li&gt;
      &lt;li&gt;Event Destination은 ESP에 subscribe만 하고, 개별 Event Source로 받지 않고, ESP에서 데이터를 consume&lt;/li&gt;
      &lt;li&gt;아래 그림에서는 Event Source를 Producer로, Event Destination을 Consumer로 표현&lt;/li&gt;
      &lt;li&gt;Popular ESP는 Kafka, Kinesis, Flink, Spark, Storm 등이 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://miro.medium.com/max/1400/1*QCs3M2PKJV7qSsDlsTzw4w.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://blog.devgenius.io/event-processing-platform-4d950c4ff3e3&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Common Components of an ESP
    &lt;ul&gt;
      &lt;li&gt;Event Broker: ESP에서의 Core Component로, Ingester, Processer, Consumpution을 포함
        &lt;ul&gt;
          &lt;li&gt;Ingester: 다양한 Event Sources로부터 Event를 효과적으로 받을 수 있도록 설계&lt;/li&gt;
          &lt;li&gt;Processer: (De) Serializing, (De) Compressing, Encryption 등 데이터에 대한 작업 수행&lt;/li&gt;
          &lt;li&gt;Consumpution: 이벤트 저장소에서 이벤트를 검색해 이벤트를 효율적으로 배포&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Event Storage&lt;/li&gt;
      &lt;li&gt;Analytics and Query Engine&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-apache-kafka&quot;&gt;3. Apache Kafka&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka는 포괄적인 플랫폼이면서 많은 애플리케이션 시나리오에서 사용 가능&lt;/li&gt;
  &lt;li&gt;원래 Kafka는 사용자의 키보드를 통한 검색, 마우스 클릭, 검색 등과 같이 사용자의 활동을 추적하기 위한 것&lt;/li&gt;
  &lt;li&gt;하지만 지금은 하드웨어 및 소프트웨어 모니터링, 센서, GPS 등과 같이 Metric-Streaming에도 적합&lt;/li&gt;
  &lt;li&gt;Kafka를 이용하여 중앙 저장소에 로그를 수집하고 통합 할 수도 있음 (거래내역을 다루는 은행, 보험 등에서 사용)&lt;/li&gt;
  &lt;li&gt;즉, Kafka를 통해 많은 양의 데이터를 처리할 수 있고, 신뢰할 수 있는 데이터 전송 서비스를 구축할 수 있음
    &lt;ul&gt;
      &lt;li&gt;모든 이벤트들은 Kafka를 통해 수집되고, 저장 및 소비가 가능&lt;/li&gt;
      &lt;li&gt;데이터 저장, 온오프라인 데이터베이스로의 이동, 백업, 실시간 처리, 분석, 대시보드, AI 등&lt;/li&gt;
      &lt;li&gt;Email, 텍스트 메시지 등과 같이 Notification을 생성할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_1.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka의 주요한 Components
    &lt;ul&gt;
      &lt;li&gt;Brokers: The dedicated servers to receive, store, process, and distribute events&lt;/li&gt;
      &lt;li&gt;Topics: The containers or databases of events&lt;/li&gt;
      &lt;li&gt;Partitions: Divide topics into different brokers&lt;/li&gt;
      &lt;li&gt;Replications: Duplicate partitions into different brokers&lt;/li&gt;
      &lt;li&gt;Producers: Kafka client applications to publish events into topics&lt;/li&gt;
      &lt;li&gt;Consumers: Kafka client applications are subscribed to topics and read events from them&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://miro.medium.com/v2/resize:fit:1400/format:webp/1*DmzwWAyl437Iktk-60RWKQ.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://techblog.gccompany.co.kr/apache-kafka%EB%A5%BC-%EC%82%AC%EC%9A%A9%ED%95%98%EC%97%AC-eda-%EC%A0%81%EC%9A%A9%ED%95%98%EA%B8%B0-bf263c79efd0&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka는 Distributed Client-server Architecture의 형태
    &lt;ul&gt;
      &lt;li&gt;Sever 사이드에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Broker&lt;/code&gt; 라고 불리는 클러스터가 있고, 이벤트를 수신, 저장, 배포하는 역할 수행&lt;/li&gt;
      &lt;li&gt;이러한 Broker들은 효율적으로 작업되도록 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ZooKeeper&lt;/code&gt; 라고 불리는 분산 시스템으로부터 관리됨&lt;/li&gt;
      &lt;li&gt;Kafka는 TCP 기반의 네트워크 통신 프로토콜을 사용해 클라이언트와 서버 사이에서 데이터를 교환&lt;/li&gt;
      &lt;li&gt;Client 사이드에서는 서버와 통신하기 위해 Kafka CLI나 자바나 파이썬 같은 클라이언트를 제공&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_2.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka의 주요한 특징
    &lt;ul&gt;
      &lt;li&gt;많은 양의 데이터 처리가 가능하고, 동시에 처리할 수 있는 확장성이 뛰어난 Distribution System&lt;/li&gt;
      &lt;li&gt;Kafka 클러스터는 이벤트 스트리밍을 병렬로 처리할 수 있는 Event Brokers가 존재 (빠르고, 확장성 좋음)&lt;/li&gt;
      &lt;li&gt;이벤트 저장 공간을 여러 개의 파티션으로 나누거나, 복제하여 장애를 방지하고 안정성이 높음&lt;/li&gt;
      &lt;li&gt;이벤트를 영구적으로 저장할 수 있음&lt;/li&gt;
      &lt;li&gt;오픈소스이기 때문에 무료로 사용할 수 있고, 커스터마이징이 가능&lt;/li&gt;
      &lt;li&gt;오픈소스이고, 문서화도 잘 되어 있긴 하지만 Kafka 클러스터를 구축하기 위해서는 인프라 설계가 필요 (어려움)&lt;/li&gt;
      &lt;li&gt;이를 위해서 Confluent Cloud, IBM Event Streams, Amazon MSK 같은 서비스들도 존재&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-kafka를-통해-event-streaming-파이프라인-만들기&quot;&gt;4. Kafka를 통해 Event Streaming 파이프라인 만들기&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Kafka 클러스터는 하나 이상의 Broker를 포함하고 있고, 이를 이벤트 수신, 저장, 처리, 배포 전용 서버로 이해 가능&lt;/li&gt;
  &lt;li&gt;Broker는 ZooKeeper라는 전용 서버에 의해 동기화 되고, 관리됨&lt;/li&gt;
  &lt;li&gt;Broker는 Event를 Topic으로 저장 및 관리하고, Consumers에게 배포&lt;/li&gt;
  &lt;li&gt;Kafka는 다른 분산 시스템과 마찬가지로, 분할 (Partitioning) 및 복제 (Replicating) 개념을 구현&lt;/li&gt;
  &lt;li&gt;이를 통해 데이터 처리량을 향상시켜서 여러 Broker와 동시에 병렬적으로 작업이 가능&lt;/li&gt;
  &lt;li&gt;일부 Broker가 다운 되더라도, 여전히 다른 Broker에서 복제하여 작업을 수행할 수 있음&lt;/li&gt;
  &lt;li&gt;즉, 아래 그림과 같이 log topic과 user topic이 두 파티션으로 구분되고, 복제되어 서로 다른 Broker에 저장&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_3.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka CLI는 사용자가 이벤트 스트리밍 파이프라인을 구축할 수 있도록 도와줄 수 있음&lt;/li&gt;
  &lt;li&gt;Kafka-topics 스크립트는 간단하게 Kafka 클러스터의 항목을 관리하는데 자주 사용할 수 있는 스크립트
    &lt;ul&gt;
      &lt;li&gt;Create a topic, List topics, Get topics details, Delete a topics&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_4.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka Producer는 등록된 순서에 따라 Topic 파티션에 이벤트를 등록하는 클라이언트 응용 프로그램&lt;/li&gt;
  &lt;li&gt;Producer에서 Event를 게시할 때, 선택적으로 Event를 Key와 연결할 수 있음&lt;/li&gt;
  &lt;li&gt;동일한 Key에 연결된 Event는 동일한 Topic 파티션에 Publish 됨&lt;/li&gt;
  &lt;li&gt;Key와 연결되지 않은 Event는 로테이션으로 Topic 파티션에 Publish 됨&lt;/li&gt;
  &lt;li&gt;아래 그림에서 같이 Log를 만드는 Event Source 1과 사용자 활동을 추적하는 Event Source 2가 있다고 가정
    &lt;ul&gt;
      &lt;li&gt;Kafka Producer를 통해 log topics과 user topics을 각각의 파티션에 publish&lt;/li&gt;
      &lt;li&gt;이때, 프로그램의 이름이나 사용자 ID 같은 Key 값으로 이벤트를 연결할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_5.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Producer는 Topic에 대하여 Event를 Publish 하거나, Write 하는 것이 가장 중요&lt;/li&gt;
  &lt;li&gt;아래 그림에서처럼 Key 값을 포함하거나, 포함하지 않고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Producer&lt;/code&gt;를 시작시킬 수 있음
    &lt;ul&gt;
      &lt;li&gt;Key를 주는 경우에는, 사용자 1에 대한 모든 Event가 동일한 파티션에 저장되어 Consumer가 사용할 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_6.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Event가 Publish 되고, Topic 파티션에 저장되면, Event를 읽을 수 있는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Consumer&lt;/code&gt;를 만들 수 있음&lt;/li&gt;
  &lt;li&gt;Consumer는 저장된 Event를 읽고, Topic을 다룰 수 있는 클라이언트 어플리케이션
    &lt;ul&gt;
      &lt;li&gt;Consumer는 Topic 파티션의 데이터를 publish 된 순서에 따라서 읽음&lt;/li&gt;
      &lt;li&gt;각각의 파티션에 Offset (상대 위치)를 저장하고, 이를 이용하여 Event가 발생할 때, 그것을 읽을 수 있음&lt;/li&gt;
      &lt;li&gt;Offset을 0으로 재설정할 수 있고, 이를 통해 Consumer는 Topic 파티션의 모든 이벤트를 처음부터 읽을 수 있음&lt;/li&gt;
      &lt;li&gt;Kafka에서 Producer와 Consumer는 완전히 분리되어 있어서 (Fully Decoupled) Producers는 Consumers와 동기화 시킬 필요가 없고, Event가 Topic에 저장된 후에는 Consumer가 독립적으로 스케줄에 따라 작업&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;로그와 사용자 행동 이벤트를 Topic 파티션으로부터 publish 하기 위해서는 거기에 맞는 각각의 Consumer가 필요함&lt;/li&gt;
  &lt;li&gt;그리고 Kafka는 Consumer에게 Event를 push하고, Consumer는 Event Destination으로 보냄&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_7.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka Consumer Script를 통해 Consumer를 실행시키기 위해서는 다음과 같이 수행
    &lt;ul&gt;
      &lt;li&gt;log topic으로부터 Event를 읽은 후, Script를 실행하여 Kafka 클러스터와 Topic을 지정&lt;/li&gt;
      &lt;li&gt;Consumer는 마지막 파티션 Offset에서부터 시작해서 새로운 이벤트만 읽음&lt;/li&gt;
      &lt;li&gt;새로운 이벤트를 처리한 후에는 파티션 Offset도 업데이트 되고, Kafka에 반영&lt;/li&gt;
      &lt;li&gt;가끔씩 사용자가 처음부터 모든 Event를 읽고자 한다면, 옵션을 추가해주면 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_8.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-weather-pipeline-example&quot;&gt;5. Weather Pipeline Example&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;날씨와 트위터의 Event Stream을 수집하고, 사람들이 트위터에서 극단적 날씨에 대해 말하는 것을 분석하고자 함
    &lt;ul&gt;
      &lt;li&gt;이를 위해 JSON 포멧으로 날씨와 트위터 데이터를 실시간으로 받는 IBM Weather API, Twitter API 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;날씨와 트위터를 JSON 형태로 Kafka에서 받기 위해서는, Kafka 클러스터에서 weather topic과 twitter topic 필요
    &lt;ul&gt;
      &lt;li&gt;Weather Producer와 Twitter Producer를 만들고, 데이터는 바이트로 직렬화 되어 Kafka Topic에 저장&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;2개의 Topic에서 Event를 읽기 위해서 Producer와 마찬가지로, Weather Consumer와 Twitter Consumer 필요
    &lt;ul&gt;
      &lt;li&gt;Kafka Topic에 저장된 바이트 (Bytes)는 Event JSON 데이터로 역직렬화 (Deserialized)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터들을 관계형 데이터베이스로 전송하기 위해 DB Writer를 사용해서 JSON 파일 분석 및 DB 레코드를 만듦&lt;/li&gt;
  &lt;li&gt;그 후, SQL 쿼리를 통해서 이 레코드들을 데이터베이스에 작성하고, 저장&lt;/li&gt;
  &lt;li&gt;마지막으로는 앞서 만든 DB로부터 데이터를 활용해 대시보드를 만들고 시각화 및 분석을 진행할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_9.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-kafka-streaming-process&quot;&gt;6. Kafka Streaming Process&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Event Streaming에서 데이터 엔지니어는 데이터 전송, 필터링, 집계, 향상 등을 통해 데이터를 처리해야 함&lt;/li&gt;
  &lt;li&gt;Streams 처리를 위해 개발된 어플리케이션을 Stream Processing Applications이라고 함&lt;/li&gt;
  &lt;li&gt;Kafka 기반 Stream Processing Applications 구현을 위한 간단한 방법은 한 Topic에서 Event를 읽고, 처리한 후에 다른 Topic을 Publish 하기 위해 Ad hoc 데이터 프로세서를 구현하는 것
    &lt;ul&gt;
      &lt;li&gt;Weather API로 JSON 데이터를 받은 후, Weather Producer는 Weather Topic으로 Publish&lt;/li&gt;
      &lt;li&gt;Consumer는 Weather Topic으로부터 데이터를 읽음&lt;/li&gt;
      &lt;li&gt;Ad hoc 데이터 프로세서를 만들어서 극단적으로 높은 기온과 같이 이상기후 데이터만 필터링하도록 함
        &lt;ul&gt;
          &lt;li&gt;프로세서는 간단한 스크립트 파일이거나, Kafka에서 클라이언트와 함께 작동해 데이터를 다루는 프로그램&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;프로세서는 처리된 데이터를 다른 Produer로 전달하고, Publish 하여 Topic을 만듦&lt;/li&gt;
      &lt;li&gt;그 Topic은 Consumer로 전달 및 처리되고, 시각화를 위해 대시보드 등에 전달&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_10.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ad hoc 프로세서는 처리해야 할 많은 Topic이 있는 경우에 복잡해질 수 있는데, Kafka는 이를 해결할 수 있음&lt;/li&gt;
  &lt;li&gt;Kafka는 Stream Processing을 위해서 Streams API를 제공
    &lt;ul&gt;
      &lt;li&gt;Kafka Streams API는 이벤트 스트리밍 파이프라인에서 데이터 처리를 돕는 간단한 클라이언트 라이브러리&lt;/li&gt;
      &lt;li&gt;Kafka Topics에 저장된 데이터를 처리하고 분석하기 때문에 Streams API의 입출력이 모두 Kafka Topics&lt;/li&gt;
      &lt;li&gt;Kafka Streams API는 각각의 기록들이 한 번만 처리되도록 보장&lt;/li&gt;
      &lt;li&gt;Kafka Streams API는 한 번에 하나의 레코드만 처리&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_11.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka Streams API는 Stream-processing topology 라는 계산 그래프를 기반으로 함
    &lt;ul&gt;
      &lt;li&gt;이 Topology에서 각 노드는 Upstream 프로세서에서 Streams을 받고, 맵핑, 필터링, 포메팅 등과 같은 데이터 변환을 수행하며, 다운스트림 프로세서로 출력 Stream을 생성하는 Stream 프로세서&lt;/li&gt;
      &lt;li&gt;그렇기 때문에 그래프의 가장자리는 Input, Output Streams&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;프로세서에는 다음과 같은 두 가지 특별한 유형이 있음
    &lt;ul&gt;
      &lt;li&gt;Source 프로세서: Consumer처럼 Kafka Topic을 처리하고, 처리된 Streams을 Downstream 프로세서로 전달&lt;/li&gt;
      &lt;li&gt;Sink 프로세서: Producer처럼 받은 Streams을 Kafka Topic으로 Publish 하는 역할&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_12.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Kafka Streams API를 통해 날씨 스트림 처리 어플리케이션을 다시 설계한다면 다음과 같음&lt;/li&gt;
  &lt;li&gt;Ad hoc 프로세서를 개발하는 것 대신, Kafka Streams API를 사용하면 됨&lt;/li&gt;
  &lt;li&gt;Kafka Streams Topology에는 세 개의 Stream 프로세서가 있음
    &lt;ul&gt;
      &lt;li&gt;Source 프로세서: Raw Weather Topic으로부터 Weather Streams을 처리하고, Stream 프로세서로 전달&lt;/li&gt;
      &lt;li&gt;Stream 프로세서: 이상기후와 같은 조건을 필터링하고, Sink 프로세서로 전달&lt;/li&gt;
      &lt;li&gt;Sink 프로세서: 처리된 Weather Topic의 결과를 Publish&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이러한 방식은 처리해야 할 Topic의 수가 많은 경우에 Ad hoc 데이터 프로세서보다 훨씬 더 쉬운 방법&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/230128_13.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;수업 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 27 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/Coursera-Data-Engineering-4%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/Coursera-Data-Engineering-4%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>Data Engineering</category>
        
        <category>Airflow</category>
        
        <category>Kafka</category>
        
        <category>ETL</category>
        
        <category>ELT</category>
        
        
        <category>Education</category>
        
        <category>Coursera - Data Engineering</category>
        
      </item>
    
      <item>
        <title>ETL &amp; Data Pipelines with Shell, Airflow and Kafka 3주차</title>
        <description>&lt;p&gt;이번 글에서는 &lt;a href=&quot;https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka/home/info&quot;&gt;Coursera의 ETL and Data Pipelines with Shell, Airflow and Kafka (IBM)&lt;/a&gt; 3주차 강의를 정리합니다. &lt;br /&gt;
이 강좌는 ETL 및 ELT 데이터 파이프라인에 대해 학습하며, Airflow와 Kafka 등을 이용해 이를 배우게 됩니다. &lt;br /&gt;
3주차에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Airflow를 이용해 데이터 파이프라인을 구축하는 방법&lt;/code&gt;에 대해 공부합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-데이터-학습-목표&quot;&gt;1. 데이터 학습 목표&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Apache Airflow의 주요 기능 및 원칙 나열&lt;/li&gt;
  &lt;li&gt;작업 및 종속성의 DAG로 워크플로 설명&lt;/li&gt;
  &lt;li&gt;워크플로를 코드로 정의할 때의 장점 소개&lt;/li&gt;
  &lt;li&gt;특정 DAG를 시각화 하는 다양한 방법 요약&lt;/li&gt;
  &lt;li&gt;DAG 정의 파일의 주요 구성 요소 설명 및 연산자를 인스턴스화 하여 태스크 생성&lt;/li&gt;
  &lt;li&gt;로깅 기능을 사용해 작업 상태 모니터링 및 DAG 실행 문제 진단&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-apache-airflow&quot;&gt;2. Apache Airflow&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Apache Airflow는 훌륭한 오픈소스 워크플로 오케스트레이션 도구&lt;/li&gt;
  &lt;li&gt;Batch Data Pipeline과 같은 워크플로를 만들고 실행할 수 있는 플랫폼&lt;/li&gt;
  &lt;li&gt;Airflow에서는 워크플로가 DAG (Directed Acyclic Graph)로 표시&lt;/li&gt;
  &lt;li&gt;Airflow는 Kafka, Storm, Spark 같은 도구들과는 달리, 데이터 스트리밍 솔루션이 아닌, 워크플로 관리자&lt;/li&gt;
  &lt;li&gt;기본 구성 요소
    &lt;ul&gt;
      &lt;li&gt;Airflow는 예약된 워크플로의 트리거를 처리하는 내장형 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Scheduler&lt;/code&gt;가 존재&lt;/li&gt;
      &lt;li&gt;Scheduler는 예약된 각 워크플로에서 개별 작업을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Executor&lt;/code&gt;에게 전달&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Executor&lt;/code&gt;는 이러한 작업을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Worker&lt;/code&gt;에 할당하여 실행 및 처리. 그리고 다음 작업 실행&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Web Server&lt;/code&gt;는 Airflow의 대화식 사용자 인터페이스 제공 (여기에서 DAG를 검사하고, 트리거 및 디버그 가능)&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DAG Directory&lt;/code&gt;는 Scheduler, Executor, Workers에서 액세스 할 수 있는 모든 DAG 파일이 포함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DAG는 태스크 (수행할 작업들)들 사이의 종속성과 실행 순서를 지정&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://airflow.apache.org/docs/apache-airflow/stable/_images/arch-diag-basic.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/overview.html&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Airflow의 주요 기능과 장점
    &lt;ul&gt;
      &lt;li&gt;표준 Python을 사용하여 워크플로를 만들고, 이를 통해 데이터 파이프라인 구축 시, 유연성을 유지할 수 있음&lt;/li&gt;
      &lt;li&gt;UI가 유용하여 모니터링, 스케줄링, 워크플로 관리 등을 Web app을 통해 할 수 있음&lt;/li&gt;
      &lt;li&gt;IBM Cloudant와 같이 다른 많은 Plug-and-play 서비스들과 통합 가능&lt;/li&gt;
      &lt;li&gt;Python에 대한 지식이 있다면, 누구든지 워크플로를 배포할 수 있음&lt;/li&gt;
      &lt;li&gt;오픈소스이기 때문에 무엇인가를 공유하고 싶다면, PR을 만들 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Airflow의 네 가지 주요 원칙
    &lt;ul&gt;
      &lt;li&gt;확장성: 모듈식 아키텍쳐를 가지고 있으며, 메시지 큐를 사용하여 임의의 Workers의 수를 관리&lt;/li&gt;
      &lt;li&gt;동적: Airflow 파이프라인은 Python으로 정의되고, 동적 파이프라인 생성을 허용하여 동시 작업 가능&lt;/li&gt;
      &lt;li&gt;확장 가능성: 자신의 환경에 맞게 연산자를 쉽게 정의하고, 라이브러리를 확장할 수 있음&lt;/li&gt;
      &lt;li&gt;린 (Lean): Airflow는 매개변수화가 Jinja 템플릿 엔진으로 내장되어 린하고 명시적&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-dag-directed-acyclic-graph&quot;&gt;3. DAG (Directed Acyclic Graph)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;DAG는 방향성 비순환 그래프라고 하는 특별한 종류의 그래프
    &lt;ul&gt;
      &lt;li&gt;Graph: Nodes and Edges&lt;/li&gt;
      &lt;li&gt;Directed Graph: Each edge has a direction&lt;/li&gt;
      &lt;li&gt;Acyclic: No loops (cycles)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DAG는 Airflow에서 워크플로 또는 파이프라인을 나타내는데 사용 (Python 코드로 정의된 워크플로)
    &lt;ul&gt;
      &lt;li&gt;파이프라인에서 수행하는 각 작업은 DAG에서 노드로 표시&lt;/li&gt;
      &lt;li&gt;Edge는 두 작업이 실행되어야 하는 순서를 정의&lt;/li&gt;
      &lt;li&gt;따라서 DAG는 Airflow에서 실행해야 하는 작업을 정의하고, 어떤 순서로 실행해야 할지를 위해 사용&lt;/li&gt;
      &lt;li&gt;이 DAG의 구조는 Python 스크립트로 정의되므로, 작업과 해당 종속성 역시 코드로 정의 (스케줄링도 마찬가지)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Task는 Python으로 작성되고, Task는 Operators를 구현&lt;/li&gt;
  &lt;li&gt;Operators는 DAG의 각 작업이 수행하는 작업을 정의하는데 사용&lt;/li&gt;
  &lt;li&gt;DAG는 다음의 논리 블록으로 구성된 Python 스크립트 (아래 그림과 같음)
    &lt;ul&gt;
      &lt;li&gt;라이브러리 불러오기, DAG Arguments, DAG 정의, 태스크 정의, 태스크 파이프라인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://storage.googleapis.com/analyticsmayhem-blog-files/dbt-airflow/sample%20dag%20definition.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://analyticsmayhem.com/dbt/schedule-dbt-models-with-apache-airflow/&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Apache Airflow Scheduler를 사용해서 작업자 배열에 워크플로를 배포할 수 있음 (DAG에서 정한 기준에 따라 작동)
    &lt;ul&gt;
      &lt;li&gt;Airflow Scheduler 인스턴스를 시작하면, 코드에서 지정한 ‘Start Date’ 기준으로 실행&lt;/li&gt;
      &lt;li&gt;그 후에 Scheduler는 후속 DAG를 지정한 일정 간격에 따라서 실행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이와 같이 코드로 워크플로를 정의한다는 것은 유지보수, 버전관리, 협업, 테스트의 측면에서 장점을 갖음&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;task2 &amp;gt;&amp;gt; task3&lt;/code&gt; 코드는 task2가 실행된 후, task3가 실행된다는 의미&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-airflow의-ui&quot;&gt;4. Airflow의 UI&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Airflow 사용자 인터페이스의 랜딩 페이지는 아래와 같고, 기본값은 DAG에 대한 데이터가 포함된 테이블 형태
    &lt;ul&gt;
      &lt;li&gt;각 행에는 다음과 같은 환경의 DAG에 대한 대화형 정보가 표시 (DAG 이름, 스케줄, Owner, 최근 태스크 등)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://airflow.apache.org/docs/apache-airflow/1.10.6/_images/dags.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://airflow.apache.org/docs/apache-airflow/1.10.6/ui.html&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;DAG 이름을 클릭하게 되면, ‘Tree View’ 형태로 결과를 아래와 같이 확인할 수 있음
    &lt;ul&gt;
      &lt;li&gt;각 실행에 대한 작업의 상태를 타임라인 형태로 보여주고, 기본 날짜와 실행 횟수를 선택할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://airflow.apache.org/docs/apache-airflow/1.10.6/_images/tree.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://airflow.apache.org/docs/apache-airflow/1.10.6/ui.html&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;또한 ‘Graph View’ 형태로 결과를 아레와 같이 확인할 수 있음
    &lt;ul&gt;
      &lt;li&gt;DAG의 작업과 종속성을 확인할 수 있고, 각 작업은 연산자 유형에 따라 색으로 구분&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://airflow.apache.org/docs/apache-airflow/1.10.6/_images/graph.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://airflow.apache.org/docs/apache-airflow/1.10.6/ui.html&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-airflow-monitoring-and-logging&quot;&gt;5. Airflow Monitoring and Logging&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;개발자가 작업 상태를 모니터링 하고, 문제를 진단하며 디버깅하기 위해서는 로깅 기능이 필요
    &lt;ul&gt;
      &lt;li&gt;Airflow는 Default로 로컬 파일 시스템에 로그 파일들이 저장되어 빠르게 확인 가능&lt;/li&gt;
      &lt;li&gt;Airflow의 production deployment인 경우, 원격 접속을 위해 클라우드 상에 로그 파일을 보낼 수 있음&lt;/li&gt;
      &lt;li&gt;로그 파일을 검색 엔진과 대시보드로 보내서 검색 및 분석할 수 있고, 이때는 Elastic Search나 Splunk를 권장&lt;/li&gt;
      &lt;li&gt;로그 파일의 Default 위치: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;logs/dag_id/task_id/execution_date/try_number.log&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;Airflow의 Web Server를 통해 UI 형태로도 로그 결과를 확인할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;구성 요소의 상태를 확인하고, 모니터링 하기 위한 Metrics
    &lt;ul&gt;
      &lt;li&gt;Counters: 성공 또는 실패한 작업 수와 같이 항상 증가하는 Metrics&lt;/li&gt;
      &lt;li&gt;Gauges: 현재 실행 중인 태스크의 수와 같이 변동 (Fluctuate)할 수 있는 Metrics&lt;/li&gt;
      &lt;li&gt;Timers: 태스크가 성공 또는 실패까지 걸리는 시간과 같이 Time duration과 연관된 Metrics&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Production 환경에서 Airflow의 Metrics는 수집되고, 전송되며 분석됨
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;StatsD&lt;/code&gt;는 Airflow에서 데이터를 수집하여, Metrics 모니터링 시스템으로 전송할 수 있는 네트워크 데몬&lt;/li&gt;
      &lt;li&gt;그리고 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Prometheus&lt;/code&gt;에 전달되어 Metrics를 모니터링하고 분석되며, 대시보드에서 시각화를 할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 27 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/Coursera-Data-Engineering-3%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/Coursera-Data-Engineering-3%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>Data Engineering</category>
        
        <category>Airflow</category>
        
        <category>Kafka</category>
        
        <category>ETL</category>
        
        <category>ELT</category>
        
        
        <category>Education</category>
        
        <category>Coursera - Data Engineering</category>
        
      </item>
    
      <item>
        <title>ETL &amp; Data Pipelines with Shell, Airflow and Kafka 2주차</title>
        <description>&lt;p&gt;이번 글에서는 &lt;a href=&quot;https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka/home/info&quot;&gt;Coursera의 ETL and Data Pipelines with Shell, Airflow and Kafka (IBM)&lt;/a&gt; 2주차 강의를 정리합니다. &lt;br /&gt;
이 강좌는 ETL 및 ELT 데이터 파이프라인에 대해 학습하며, Airflow와 Kafka 등을 이용해 이를 배우게 됩니다. &lt;br /&gt;
2주차에는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;데이터 파이프라인에 사용되는 도구와 기술&lt;/code&gt;에 대해 공부하며, Bash 스크립트와 일괄 및 스트림 처리를 배웁니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-데이터-학습-목표&quot;&gt;1. 데이터 학습 목표&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ETL의 각 단계에서 일어나는 일을 요약&lt;/li&gt;
  &lt;li&gt;ETL 파이프라인의 중요성과 작동 방식 설명&lt;/li&gt;
  &lt;li&gt;Shell 스크립트를 통해 ETL 파이프라인 구현 방법 요약&lt;/li&gt;
  &lt;li&gt;주요한 데이터 파이프라인 프로세스 설명&lt;/li&gt;
  &lt;li&gt;배치 및 스트리밍 데이터 파이프라인에 대해 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-shell-스크립트&quot;&gt;2. Shell 스크립트&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Shell은 유닉스 계열 운영 체제를 위한 강력한 사용자 인터페이스&lt;/li&gt;
  &lt;li&gt;명령을 해석하고, 다른 프로그램을 실행시킬 수도 있음&lt;/li&gt;
  &lt;li&gt;파일, 유틸리티 및 응용 프로그램에 대한 액세스를 가능하게 하며, 대화식 스크립팅 언어&lt;/li&gt;
  &lt;li&gt;Shell을 통해 작업을 자동화할 수도 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://hiseon.me/wp-content/uploads/2019/07/bash-start.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://hiseon.me/linux/linux-shell-script-example/&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-데이터-파이프라인&quot;&gt;3. 데이터 파이프라인&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 파이프라인 개념은 광범위 하게 적용되는데, 순차적으로 연결된 일련의 프로세스라고 할 수 있음
    &lt;ul&gt;
      &lt;li&gt;한 프로세스의 출력은 다음 프로세스에 대한 입력으로 전달&lt;/li&gt;
      &lt;li&gt;데이터를 이동하거나 수정하는 파이프라인 (데이터를 추출하여 전달하는 시스템)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 파이프라인의 길이는 데이터 파이프라인에 걸리는 시간을 뜻함. 성능과 관련한 고려사항은 다음과 같음
    &lt;ul&gt;
      &lt;li&gt;대기 시간: 단일 작업에 걸리는 총 시간 (대기 시간은 개별 시간의 합으로 각 처리 단계에서 소비)&lt;/li&gt;
      &lt;li&gt;처리량: 단위 시간 당 파이프라인을 통해 얼마나 많은 데이터를 공급할 수 있는지를 뜻함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 파이프라인 프로세스의 공통 단계
    &lt;ul&gt;
      &lt;li&gt;하나 이상의 데이터 소스에서 데이터를 추출&lt;/li&gt;
      &lt;li&gt;추출된 데이터를 파이프라인으로 수집&lt;/li&gt;
      &lt;li&gt;파이프라인 내에서 선택적으로 데이터 변환 후 최종 로드&lt;/li&gt;
      &lt;li&gt;실행할 작업을 예약하거나, 트리거를 걸어주는 매커니즘&lt;/li&gt;
      &lt;li&gt;전체 워크플로 모니터링 및 원활하게 실행 되도록 필요한 유지 관리, 최적화
        &lt;ul&gt;
          &lt;li&gt;작업에 걸리는 시간, 시간에 따라 처리되는 데이터 양, 과부하 등으로 인한 오류 및 장애 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;이벤트 로깅 시스템을 통해 특정 이벤트 (오류)가 발생시 관리자에게 알려주도록 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Load Balanced Pipelines
    &lt;ul&gt;
      &lt;li&gt;한 단계가 데이터 패킷에 대한 프로세스를 완료했을 때, 대기열에 있는 다음 패킷을 사용&lt;/li&gt;
      &lt;li&gt;파이프라인이 작동하는 동안 스테이지를 유휴 상태로 두지 않음&lt;/li&gt;
      &lt;li&gt;즉, 모든 단계에서 패킷을 처리하는데 동일한 시간이 소요되어야 한다는 것 (병목 현상이 없음)&lt;/li&gt;
      &lt;li&gt;하지만 시간 및 비용 등을 고려하면, 파이프라인이 완벽하게 로드 밸런싱 되지는 못함
        &lt;ul&gt;
          &lt;li&gt;거의 항상 데이터 흐름에서 병목 현상이 있는 단계가 있다는 것&lt;/li&gt;
          &lt;li&gt;이 병목 현상이 있는 단계를 병렬화할 수 있다면, 속도를 더 높일 수 있을 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 파이프라인 도구 및 기술
    &lt;ul&gt;
      &lt;li&gt;Python의 Pandas 라이브러리
        &lt;ul&gt;
          &lt;li&gt;Excel 또는 CSV 스타일의 테이블 형식 데이터 처리 가능&lt;/li&gt;
          &lt;li&gt;하지만 빅데이터로 확장하는데는 제한적. 데이터 프레임 조작은 메모리 내에서만 가능하기 때문&lt;/li&gt;
          &lt;li&gt;유사한 라이브러리로는 Vaex, Dask, Spark 등이 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Apache Airflow
        &lt;ul&gt;
          &lt;li&gt;Python 프로그래밍 언어를 기반으로 하는 오픈 소스 데이터 파이프라인 플랫폼&lt;/li&gt;
          &lt;li&gt;데이터 파이프라인 워크플로를 프로그래밍 방식으로 작성, 예약, 모니터링 가능&lt;/li&gt;
          &lt;li&gt;AWS를 포함한 대부분의 클라우드 플랫폼과 통합&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Talend
        &lt;ul&gt;
          &lt;li&gt;또 다른 오픈소스 데이터 파이프라인 개발 및 배포 플랫폼&lt;/li&gt;
          &lt;li&gt;빅데이터 마이그레이션, 데이터 웨어하우징, 프로파일링 지원&lt;/li&gt;
          &lt;li&gt;협업, 모니터링, 스케줄링 기능 포함. 그리고 파이프라인을 생성할 수 있는 GUI 존재&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;AWS Glue
        &lt;ul&gt;
          &lt;li&gt;분석을 위해 데이터를 쉽게 준비하고 로드할 수 있음&lt;/li&gt;
          &lt;li&gt;데이터 소스를 크롤링하여 데이터 형식을 검색&lt;/li&gt;
          &lt;li&gt;데이터를 저장할 스키마를 제안하고, AWS 콘솔을 이용해 ETL 작업을 빠르게 생성 및 실행 가능&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Panoply
        &lt;ul&gt;
          &lt;li&gt;ETL 보다는 ELT에 초점을 맞추고, 코드 없이 데이터를 연결 및 통합 할 수 있음&lt;/li&gt;
          &lt;li&gt;SQL 베이스로 데이터를 볼 수 있고, 데이터 파이프라인을 최적화 하는 대신 분석에 집중하도록 함&lt;/li&gt;
          &lt;li&gt;Tableau나 Power BI 같은 대시보드 및 BI 도구와 통합 가능&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Streaming 데이터 파이프라인 도구로는 Storm, Spark, Kafka 등이 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-batch--streaming-micro-batch--hybrid-lambda-파이프라인&quot;&gt;4. Batch &amp;amp; Streaming, Micro-batch &amp;amp; Hybrid Lambda 파이프라인&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Batch Data Pipeline
    &lt;ul&gt;
      &lt;li&gt;데이터를 하나의 큰 단위로 추출하고 운영할 때 사용&lt;/li&gt;
      &lt;li&gt;주기적으로 작동 (Hours, Days, Weeks 등)&lt;/li&gt;
      &lt;li&gt;트리거를 기반으로 시작할 수도 있음 (예를 들면, 소스에 누적되는 데이터가 일정 크기에 도달했다면 실행)&lt;/li&gt;
      &lt;li&gt;최신 데이터에 의존하지 않는 경우, 정확성이 중요한 경우에 적합&lt;/li&gt;
      &lt;li&gt;주기적인 데이터 백업, 거래내역 로딩, 고객 주문 및 청구 처리, 중장기 매출 예측, 일기 예보 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Streaming Data Pipeline
    &lt;ul&gt;
      &lt;li&gt;데이터 패킷을 연속적으로 빠르게 수집 (예를 들면, 개별 신용 카드 거래, 소셜 미디어 활동 등)&lt;/li&gt;
      &lt;li&gt;레코드 또는 이벤트가 발생하는 즉시 처리 (거의 실시간으로 작동)&lt;/li&gt;
      &lt;li&gt;소셜 미디어 피드 및 감성 분석, 사기 탐지, 사용자 행동 분석, 타겟 광고, 주식 시장 거래, 실시간 추천 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Micro-batch Data Pipeline
    &lt;ul&gt;
      &lt;li&gt;배치 크기를 줄이고, 개별 배치 프로세스의 새로고침 빈도를 높여서 실시간으로 처리할 수 있음&lt;/li&gt;
      &lt;li&gt;로드 밸런싱에 도움이 되어서 전체 지연 시간을 줄일 수 있음&lt;/li&gt;
      &lt;li&gt;변환 과정에서 매우 짧은 데이터 기간만 필요한 경우에 유용&lt;/li&gt;
      &lt;li&gt;Batch와 Streaming 방법에서는 Accuracy와 Latency에 대한 Trade-off가 존재&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Lambda Architecture
    &lt;ul&gt;
      &lt;li&gt;빅데이터를 처리하도록 설계된 하이브리드 아키텍쳐로 Batch와 Streaming 방법을 결합한 것&lt;/li&gt;
      &lt;li&gt;히스토리 데이터는 Batch layer로, 실시간 데이터는 Speed layer로, 그 후 두 계층이 Serving layer로 통합&lt;/li&gt;
      &lt;li&gt;정확성과 속도를 목표로 할 때 이 아키텍쳐를 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-참고-내용&quot;&gt;5. 참고 내용&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Batch Processing (일괄 처리)
    &lt;ul&gt;
      &lt;li&gt;안정적이고 확장 가능한 데이터 인프라를 구축하는 데 중요한 단계 (일괄 처리 알고리즘 MapReduce)&lt;/li&gt;
      &lt;li&gt;설정된 시간 간격 동안 저장소에 데이터의 묶음(batch)을 로드하며, 일반적으로 사용량이 적은 업무 시간에 예약&lt;/li&gt;
      &lt;li&gt;대용량 데이터에 대한 작업으로 전체 시스템에 부담을 줄 수 있는 작업이 다른 워크로드에 미치는 영향을 최소화&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Streaming Processing (스트리밍 처리)
    &lt;ul&gt;
      &lt;li&gt;데이터를 지속적으로 업데이트 해야 할 때 활용&lt;/li&gt;
      &lt;li&gt;데이터가 생성되는 즉시 연속 스트림을 처리하는 것 (실시간 분석)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://velog.velcdn.com/images/roo333/post/b9be72e3-55ca-4d1e-955f-c908c4fabb7c/image.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://velog.io/@roo333/%EB%B0%B0%EC%B9%98-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8B%B1-VS-%EC%8A%A4%ED%8A%B8%EB%A6%BC-%ED%94%84%EB%A1%9C%EC%84%B8%EC%8B%B1&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Load Balancing (로드 밸런싱): 서버에 가해지는 부하 (로드)를 분산 (밸런싱) 해주는 장치 또는 기술
    &lt;ul&gt;
      &lt;li&gt;Round Robin 방식: 서버에 들어온 요청을 순서대로 돌아가며 배정하는 방식&lt;/li&gt;
      &lt;li&gt;Weighted Round Robin 방식: 각 서버마다 가중치를 매겨서, 가중치가 높은 서버에 요청을 우선적으로 배분&lt;/li&gt;
      &lt;li&gt;IP Hash 방식: 클라이언트의 IP 주소를 특정 서버로 매핑하여 요청을 처리하는 방식&lt;/li&gt;
      &lt;li&gt;Least Connection 방식: 요청이 들어온 시점에 가장 적은 연결 상태를 보이는 서버에 우선적으로 배분&lt;/li&gt;
      &lt;li&gt;Least Response Time 방식: 서버의 현재 연결 상태와 응답시간을 모두 고려하여 트래픽을 배분&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FllMx6%2FbtqKxQTfpKV%2FtQbWvFHMgSu4sLanPQFHUK%2Fimg.jpg&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://icarus8050.tistory.com/101&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Lambda Architecture (람다 아키텍쳐)
    &lt;ul&gt;
      &lt;li&gt;Batch Layer: 데이터 조회 요청에 걸리는 시간을 최소화 하기 위해 배치를 이용해 데이터를 미리 계산
        &lt;ul&gt;
          &lt;li&gt;Batch Layer의 저장소에서는 Raw 데이터를 보관&lt;/li&gt;
          &lt;li&gt;Batch 뷰의 데이터가 부정확 할 때 복구 할 수 있음&lt;/li&gt;
          &lt;li&gt;이 단계에서는 Apache Hadoop을 사용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Speed Layer: 배치가 도는 간격 사이에서는 데이터 조회가 불가능하므로, 배치 레이어에서 생기는 갭을 채움
        &lt;ul&gt;
          &lt;li&gt;이 단계에서는 Apache Storm, SQLstream, Apache Spark를 사용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Serving Layer: 배치 레이어와 스피드 레이어의 출력을 저장&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://ok-data.github.io/assets/Lambda_Arch/2020-07-16-17-17-13.png&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://ok-data.github.io/2020/06/20/Lambda_Arch/&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 27 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/Coursera-Data-Engineering-2%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/Coursera-Data-Engineering-2%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>Data Engineering</category>
        
        <category>Airflow</category>
        
        <category>Kafka</category>
        
        <category>ETL</category>
        
        <category>ELT</category>
        
        
        <category>Education</category>
        
        <category>Coursera - Data Engineering</category>
        
      </item>
    
      <item>
        <title>ETL &amp; Data Pipelines with Shell, Airflow and Kafka 1주차</title>
        <description>&lt;p&gt;이번 글에서는 &lt;a href=&quot;https://www.coursera.org/learn/etl-and-data-pipelines-shell-airflow-kafka/home/info&quot;&gt;Coursera의 ETL and Data Pipelines with Shell, Airflow and Kafka (IBM)&lt;/a&gt; 1주차 강의를 정리합니다. &lt;br /&gt;
이 강좌는 ETL 및 ELT 데이터 파이프라인에 대해 학습하며, Airflow와 Kafka 등을 이용해 이를 배우게 됩니다. &lt;br /&gt;
1주차에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;데이터 처리 기술&lt;/code&gt;에 대해 공부하며 ETL과 ELT 개념을 학습하고, 둘의 차이는 무엇인지에 대해 알아봅니다. &lt;br /&gt;
또한 ETL에서 ELT로 변화하게 되는 이유 등에 대해서도 학습할 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-데이터-학습-목표&quot;&gt;1. 데이터 학습 목표&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ETL 프로세스가 어떠한 것인지 학습&lt;/li&gt;
  &lt;li&gt;ELT 프로세스가 왜 최근 등장하고 있는 트렌드인지 학습&lt;/li&gt;
  &lt;li&gt;데이터를 Extraction 하고, Loading 하는 방법을 학습&lt;/li&gt;
  &lt;li&gt;Batch Loading과 Stream Loading에 대해서 학습&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-etl과-elt&quot;&gt;2. ETL과 ELT&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ETL (Extraction, Transformation, Loading)
    &lt;ul&gt;
      &lt;li&gt;ETL은 데이터를 수집하고, 준비하는 자동화 된 데이터 파이프라인 엔지니어링 방법론. 이를 통해 향후 데이터 웨어하우스나, 데이터 마트 같은 분석 환경에서 사용할 수 있도록 도움. 즉, 여러 소스의 데이터를 큐레이팅 해서 통합된 데이터에 맞추는 프로세스&lt;/li&gt;
      &lt;li&gt;Extration: 하나 이상의 소스에서 데이터를 읽거나 얻는 과정
        &lt;ul&gt;
          &lt;li&gt;데이터에 대한 액세스를 구성하고, 애플리케이션으로 읽는 것 (일반적으로 자동화 되어 있음 - 크롤링 등)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Transformation: 데이터를 의도된 용도에 맞게 변환하는 과정 (Data Wrangling)
        &lt;ul&gt;
          &lt;li&gt;Cleaning, Filtering, Joining, Feature Engineering, Formatting and data typing 등&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Loading: 변환된 데이터를 가져와 새 환경에 로드하는 과정 (데이터베이스, 데이터 웨어하우스, 데이터 마트)
        &lt;ul&gt;
          &lt;li&gt;이 과정의 핵심 목표는 수집된 데이터를 쉽게 사용할 수 있도록 하는 것 (대시보드, 리포트 등에 활용)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ELT (Extraction, Loading, Transformation)
    &lt;ul&gt;
      &lt;li&gt;ELT는 ETL과 유사하지만, 순서가 조금 다른 데이터 파이프라인 엔지니어링 방법론. ELT는 데이터가 수집된 그대로 대상 환경에 직접 로드하고, 데이터 레이크 환경 등에서 사용자가 원하는대로 변환해서 사용하게 됨.&lt;/li&gt;
      &lt;li&gt;Extraction: 데이터를 얻는 과정&lt;/li&gt;
      &lt;li&gt;Loading: Raw data를 있는 그대로 사용하여 로드하는 과정&lt;/li&gt;
      &lt;li&gt;Transformations: 사용자의 의도 및 요구대로 데이터를 변환시키는 과정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://cdn.buttercms.com/rdSqAjSiRqy6hk7PCCVU&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://www.integrate.io/ko/blog/etl-vs-elt-5-critical-differences-ko/&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;ELT 방법이 등장하게 된 이유&lt;/strong&gt;로는 빅데이터를 다루는 과정에서 빠르게 발전하고 있는 Cloud computing을 통해 ELT 방법론으로 데이터 이동과 처리를 면확히 구분할 수 있음. 또한 Raw 데이터를 적재한 후, 작업하기 때문에 정보 손실이 없다는 점도 장점 중 하나&lt;/li&gt;
  &lt;li&gt;ETL과 ELT 비교: 최근에는 ETL에서 ELT로 변화되고 있는 추세
    &lt;ul&gt;
      &lt;li&gt;ETL에서의 변환은 파이프라인 내에서 발생하는데, ELT에서는 파이프라인과 분리되어 동작. 따라서 ELT 방법에서는 Raw data 자체를 보호하며, 필요에 따라 데이터를 변환시켜 사용할 수 있음&lt;/li&gt;
      &lt;li&gt;ETL은 고정적인 프로세스이지만, ELT는 유연성을 가지고 있다는 특징이 있음&lt;/li&gt;
      &lt;li&gt;ETL에서는 전통적으로 구조화 된 관계형 데이터를 처리하기 때문에 확장성 등에서 문제가 발생할 수도 있는데, ELT에서는 모든 종류의 구조화 및 비구조화 데이터를 처리할 수 있음&lt;/li&gt;
      &lt;li&gt;ETL 파이프라인은 수정하는데 시간과 노력이 필요하지만, ELT에서는 더 민첩하게 대처할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-데이터-extraction-기술&quot;&gt;3. 데이터 Extraction 기술&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;OCR: 종이 문서 등에서 텍스트를 스캔&lt;/li&gt;
  &lt;li&gt;ADC Sampling: 아날로그 오디오 녹음 및 신호를 디지털화&lt;/li&gt;
  &lt;li&gt;CCD Sampling: 이미지를 캡쳐하고 디지털화&lt;/li&gt;
  &lt;li&gt;이메일, 휴대폰, 설문 등을 통해 얻은 통계 데이터 및 사용자 로그, 쿠키&lt;/li&gt;
  &lt;li&gt;웹 크롤링, API, 데이터베이스 쿼리&lt;/li&gt;
  &lt;li&gt;Edge computing, 의료기기를 통한 데이터 획득&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-데이터-transformation-기술&quot;&gt;4. 데이터 Transformation 기술&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;응용 프로그램에 맞게 데이터 형식을 지정하는 과정&lt;/li&gt;
  &lt;li&gt;Data typing, Data structuring (JSON, XML, CSV)&lt;/li&gt;
  &lt;li&gt;Anonymizing, Encrypting (암호화 및 익명화)&lt;/li&gt;
  &lt;li&gt;Cleaning, Normalizing, Filtering, Sorting, Aggregating, Binning, Joining&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Schema-on-write&lt;/code&gt;는 ETL에서 사용되는 일반적 접근으로, 로드하기 전에 정의된 스키마를 준수해야 하는 것. 안전성 있게 데이터를 일관되게 구조화할 수 있지만, 다양성을 제한할 수도 있음&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Schema-on-read&lt;/code&gt;는 ELT 접근 방식과 관련 있고, 여기서 스키마는 Raw data에 적용. 엄격한 절차가 없기 때문에 잠재적으로 더 많은 데이터에 접근할 수 있음&lt;/li&gt;
  &lt;li&gt;일반적으로 Raw data는 변환된 데이터보다 훨씬 큰데, ETL에서는 변환 과정에서 일부 데이터를 손실하게 될 수 있음. 하지만 ELT에서는 데이터를 그대로 복사하여 정보를 손실시키지 않음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-데이터-loading-기술&quot;&gt;5. 데이터 Loading 기술&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Full: 초기 기록 전체를 데이터베이스에 로드. 하나의 큰 배치로 데이터를 로드&lt;/li&gt;
  &lt;li&gt;Incremental: 새로운 데이터를 삽입하거나, 이미 로드된 데이터를 업데이트&lt;/li&gt;
  &lt;li&gt;Scheduled: 주기적으로 데이터 로드를 예약&lt;/li&gt;
  &lt;li&gt;On-demand: 요청 시, 필요에 따라 데이터 로드&lt;/li&gt;
  &lt;li&gt;Batch and Stream: 데이터를 일괄적으로 로드하거나, 스트리밍 (실시간 처리)&lt;/li&gt;
  &lt;li&gt;Push and Pull: 데이터가 서버로 푸시되거나, 서버에서 클라이언트로 푸시&lt;/li&gt;
  &lt;li&gt;Parallel and Serial: 병렬 로드, 직렬 로드&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-참고-내용&quot;&gt;6. 참고 내용&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Data Warehouse (DW): 의사결정에 도움을 주기 위해 분석 가능한 형태로 변환된 데이터가 저장된 중앙 저장소&lt;/li&gt;
  &lt;li&gt;Data Lake: 사전에 정의된 구조 없이 방대한 양의 원시 데이터가 그대로 저장 (모든 데이터에 대한 중앙 리포지토리)&lt;/li&gt;
  &lt;li&gt;Data Mart: 영업, 재무, 마케팅 등 단일 주제 등에 중점을 둔 단순한 형태의 데이터 웨어하우스&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://content.altexsoft.com/media/2021/08/data-mart-vs-data-warehouse-vs-data-lake-architect.png.webp&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;https://www.altexsoft.com/blog/what-is-data-mart/&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 파이프라인을 통해 순차적으로 공급되거나 파이프 되는 데이터의 패킷&lt;/li&gt;
  &lt;li&gt;이상적으로는 세 번째 패킷이 수집이 될 때까지 세 개의 ETL 프로세스가 모두 서로 다른 패킷에서 동시에 실행&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/MLfMnQrvTJ63zJ0K79yekA_04d039d54e0a49fe822d0690854099f1_M2_L1_ETL-Techniques-Fig-2.png?expiry=1674950400000&amp;amp;hmac=gt7nO6dGfagIg3nZzgACNBG7GuY2ETbAUOwHSetje-U&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;강의 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;워크플로의 세부 사항을 개별 작업 사이의 종속성으로 분류하게 되면, 복잡성을 제어할 수 있음&lt;/li&gt;
  &lt;li&gt;Apache Airflow 같은 워크플로 오케스트레이션 도구가 이러한 역할을 수행&lt;/li&gt;
  &lt;li&gt;Airflow는 DAG (Directed Acyclic Graph)로 워크플로를 나타냄 (그림 참고)&lt;/li&gt;
  &lt;li&gt;Airflow 작업은 연산자라고 하는 미리 정의된 템플릿을 사용하여 표현
    &lt;ul&gt;
      &lt;li&gt;이 연산자에는 Bash 실행을 위한 Bash 연산자와 Python 실행을 위한 Python 연산자가 포함&lt;/li&gt;
      &lt;li&gt;그렇기 때문에 ETL 파이프라인 및 기타 여러 종류의 워크플로를 프로덕션에 배포하는데 유용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;아래 그림에서 녹색 상자는 개별 작업을 나타내고, 화살표는 작업 간의 종속성을 나타냄&lt;/li&gt;
&lt;/ul&gt;

&lt;figure style=&quot;text-align: center;&quot;&gt;
    &lt;img src=&quot;https://d3c33hcgiwev3.cloudfront.net/imageAssetProxy.v1/VJyE2lxIQvuchNpcSOL7Dw_91ef5c601357470ab33b15395678bdf1_M2_L1_ETL-Techniques-Fig-4.png?expiry=1674950400000&amp;amp;hmac=qB0zqTq32xTDV-4SUwqIjFRKg0VwhuHyWVEGjjHDtyY&quot; width=&quot;700&quot; height=&quot;700&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;강의 자료&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 27 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/Coursera-Data-Engineering-1%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/Coursera-Data-Engineering-1%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>Data Engineering</category>
        
        <category>Airflow</category>
        
        <category>Kafka</category>
        
        <category>ETL</category>
        
        <category>ELT</category>
        
        
        <category>Education</category>
        
        <category>Coursera - Data Engineering</category>
        
      </item>
    
      <item>
        <title>LG Aimers 2기 시계열 분석 (고려대학교 강필성 교수님)</title>
        <description>&lt;p&gt;이번 글에서는 LG Aimers의 AI 전문가 과정에서 시계열 데이터의 순차적 특성을 고려한 모형과 그 학습 원리를 배웁니다. 모형의 어떠한 특성이 시계열 데이터의 특성을 학습에 반영하게 되는지 그리고 어떻게 모델의 성능을 향상시킬 수 있는지를 배울 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-순환신경망-기반의-시계열-데이터-회귀&quot;&gt;1. 순환신경망 기반의 시계열 데이터 회귀&lt;/h2&gt;

&lt;h3 id=&quot;non-sequential-vs-sequential-time-series-data&quot;&gt;Non-Sequential vs Sequential (Time-Series) Data&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Non-Sequential Data: 시간 정보를 포함하지 않고 생성되는 데이터
    &lt;ul&gt;
      &lt;li&gt;순차 데이터가 아닌 경우, 데이터는 N by D 행렬로 표현 (N: 관측치 수, D: 변수 수)&lt;/li&gt;
      &lt;li&gt;(예) 특정 고객의 금융 상품 이용 현황으로 대출 상품 추천 (X: 고객별 금융 상품 이용 현황, Y: 대출사용 유무)&lt;/li&gt;
      &lt;li&gt;순서가 없는 인공신경망 구조: x -&amp;gt; h -&amp;gt; y&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Sequential Data: 시간 정보를 포함하여 순차적으로 생성되는 데이터
    &lt;ul&gt;
      &lt;li&gt;순차 데이터의 경우, 데이터는 (N) by T by D Tensor 로 표현 (T는 측정 시점 수)&lt;/li&gt;
      &lt;li&gt;(예) 반도체 공정에서 주기적으로 측정되는 여러 센서 값을 이용한 Critical Dimension 예측&lt;/li&gt;
      &lt;li&gt;순서가 있는 인공신경망 구조: x -&amp;gt; h (과거 정보를 누적시켜서 다시 재학습, Recurrent 형태) -&amp;gt; y&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-basics-forward-path&quot;&gt;RNN Basics: Forward Path&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기본 RNN (Vanilla RNN) 구조에서 정보의 흐름
    &lt;ul&gt;
      &lt;li&gt;t 시점에서의 은닉 노드의 값은 두 부분에서 영향을 받게 됨&lt;/li&gt;
      &lt;li&gt;첫번째는, t-1 시점까지 은닉 노드에 저장된 정보&lt;/li&gt;
      &lt;li&gt;두번째는, t 시점에서 새롭게 제공되는 입력 정보&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-basics-gradient-vanishing-exploding-problem&quot;&gt;RNN Basics: Gradient Vanishing, Exploding Problem&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;RNN에서의 Backpropagation에서 활성화 함수는 Tanh로 가정
    &lt;ul&gt;
      &lt;li&gt;순전파를 통해서 모델이 얼마나 틀렸는지를 확인한 후, Cost 함수를 사용하여 역전파를 수행&lt;/li&gt;
      &lt;li&gt;그래서 가장 현재 입력과 출력 간의 쌍들을 잘 맞출 수 있는 가중치 행렬을 구하게 됨 (역전파 목적)&lt;/li&gt;
      &lt;li&gt;이를 일반화시켜서 표현하면, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(1 - tanh^2(z_t)) * x_t&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;그런데, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;1 - tanh^2(x)&lt;/code&gt;의 값은 값이 양 또는 음의 값으로 발산할 때, 값이 0으로 수렴&lt;/li&gt;
          &lt;li&gt;즉, 특정한 값을 벗어나게 되면, Gradient 값이 소실되는 문제가 발생한다는 것&lt;/li&gt;
          &lt;li&gt;이를 해결하기 위해, LSTM과 GRU 방식이 등장&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-hidden-unit-lstm-long-short-term-memory&quot;&gt;RNN Hidden Unit: LSTM (Long Short-Term Memory)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Gradient exploding, vanishing 문제를 해결하여 Long-term dependency 학습 가능&lt;/li&gt;
  &lt;li&gt;Vanilla RNN은 Input이 주어지고, 이전 시점에서의 은닉층 (Hidden State)의 정보가 주어지면 이 두 개를 한 번 연산하고, 다음 State로 내보내게 되고 (Concatenate), 이것이 다음 시점의 Output으로 만들어짐&lt;/li&gt;
  &lt;li&gt;LSTM은 Cell state라고 하여 각 시점마다 Cell state가 존재하고, 모델 상부를 관통하는 선의 모양&lt;/li&gt;
  &lt;li&gt;Vanilla RNN은 주기억장치 하나만 있는데, LSTM은 주기억장치와 보조기억장치가 함께 있어서 보조기억장치의 역할을 잘 개선시켜서 과거에 멀리 떨어져 있던 정보들을 잘 기억하고, 선택적으로 반영할 수 있게 해주는 것
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;lstm-process&quot;&gt;LSTM Process&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Step 1. 지금까지 Cell state에 저장된 정보 중 얼마만큼 망각 (Forget) 할 것인지 결정
    &lt;ul&gt;
      &lt;li&gt;Forget gate: 이전 단계의 Hidden state h_t-1과 현 단계의 입력 x_t로부터 0과 1 사이값 출력 (시그모이드)
        &lt;ul&gt;
          &lt;li&gt;1: 지금까지 Cell state에 저장된 모든 정보 보존&lt;/li&gt;
          &lt;li&gt;0: 지금까지 Cell state에 저장된 모든 정보 무시&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;이전 단계의 결과와 현 단계의 입력을 Concatenate 하고, 모델의 파라미터 (W_f)를 통해 지금까지 저장된 정보가 얼마나 유용한지 현재 시점에서 유용한지 판단 (1 또는 0으로 결과값을 반환)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Step 2. 새로운 정보를 얼마만큼 Cell state에 저장할 것인지를 결정
    &lt;ul&gt;
      &lt;li&gt;Input gate: 어떤 값을 업데이트 할 것인지를 결정
        &lt;ul&gt;
          &lt;li&gt;고정적으로 결정하는 것이 아닌, 그때 그때 시점 별로 Data에 따라 Adaptive 하게 결정되는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Tanh layer를 사용하여 새로운 Cell state의 후보를 생성
        &lt;ul&gt;
          &lt;li&gt;후보를 생성하는데 있어서 필요한 데이터는 이전 시점의 Hidden state와 현 시점의 입력 정보&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Step 3. 예전 Cell state를 새로운 Cell state로 업데이트
    &lt;ul&gt;
      &lt;li&gt;예전 Cell state를 얼마만큼 망각할 것인가를 계산한 Forget gate 결과값과 곱합&lt;/li&gt;
      &lt;li&gt;새로운 Cell state 후보와 얼마만큼 보존할 것인가를 계산한 Input gate 결과값을 곱합&lt;/li&gt;
      &lt;li&gt;두 값을 더하여 새로운 Cell state 값으로 결정&lt;/li&gt;
      &lt;li&gt;즉, 과거부터 지금까지 이만큼의 정보를 저장했고, 현재는 이 정도의 정보를 새로 받았는데, 그 과거 정보를 얼마만큼 보존하고, 현재 정보를 얼마만큼 반영해야 하는지를 데이터를 기반으로 학습으로 결정하는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Step 4. 출력 값을 결정
    &lt;ul&gt;
      &lt;li&gt;이전 Hidden state 값과 현재의 입력 값을 이용하여 Output gate 값을 산출&lt;/li&gt;
      &lt;li&gt;Output gate 값과 현재의 Cell state 값을 결합하여 현재의 Hidden state 값을 계산&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Vanilla RNN은 W_xh, W_hh, W_hy 라는 세 가지 가중치 행렬을 사용하여 학습&lt;/li&gt;
  &lt;li&gt;LSTM은 W_f, W_i, W_c, W_o 라는 네 가지 가중치 행렬을 학습시켜야만 작동&lt;/li&gt;
  &lt;li&gt;또한 LSTM의 입력 값은 이전 Hidden state 값과 현재의 입력 값 (h_t-1, x_t)을 Concatenate 한 것&lt;/li&gt;
  &lt;li&gt;Vanilla RNN이 먼 시점에서 정보를 기억하지 못해서 보완하게 된 LSTM 방법론&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-hidden-unit-gru-gated-recurrent-unit&quot;&gt;RNN Hidden Unit: GRU (Gated Recurrent Unit)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM을 단순화 한 구조. 실제 활용에서는 LSTM과 GRU의 성능 차이는 미비함
    &lt;ul&gt;
      &lt;li&gt;GRU가 LSTM에 비해 상대적으로 단순하기 때문에 GRU -&amp;gt; LSTM 순서로 적용하는게 일반적 순서&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;별도의 Cell state가 존재하지는 않음&lt;/li&gt;
  &lt;li&gt;LSTM의 Forget gate와 Input gate를 하나의 Update gate로 결합
    &lt;ul&gt;
      &lt;li&gt;Update gate는 현재 정보를 얼마만큼 많이 반영해 줄것인가를 결정하는 부분&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reset gate를 통해 망각과 새로운 정보 업데이트 정도를 결정
    &lt;ul&gt;
      &lt;li&gt;Reset gate는 과거의 정보를 얼마만큼 덜 반영하거나, 더 반영하는지를 연산해주는 부분&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-variations-bidirectional-rnn&quot;&gt;RNN Variations: Bidirectional RNN&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Bidirectional RNN (양방향 순환신경망): 정보의 입력을 시간의 순방향과 역방향 관점에서 함께 처리 (번역기 예시)&lt;/li&gt;
  &lt;li&gt;Deep-Bidirectional RNN: RNN의 Hidden layer가 한 층일 필요가 없이, 더 쌓아보는 접근법 (CNN처럼)
    &lt;ul&gt;
      &lt;li&gt;단, CNN과는 다르게, RNN은 층을 여러 개 깊게 쌓는다고 해서 성능의 향상을 보장하지는 않음&lt;/li&gt;
      &lt;li&gt;경험적으로도 쌓지 않거나, 아니면 최대 두 개에서 네 개 층 정도만 쌓는 것이 Maximum 층이 될 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rnn-attention&quot;&gt;RNN: Attention&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;어느 시점 정보가 RNN의 최종 출력 값에 영향을 미치는지를 알려줄 수 있는 매커니즘&lt;/li&gt;
  &lt;li&gt;Bahadanau attention (2015): Attention 매커니즘을 구현하기 위해 별도로 다시 모델을 학습함&lt;/li&gt;
  &lt;li&gt;Luong attention (2015): 별도로 학습하지 않아도, Attention score를 산출할 수 있어서 이 방법론을 더 선호함&lt;/li&gt;
  &lt;li&gt;일반적인 Vanilla RNN에서는 각 시점 별로 Hidden state의 정보가 전달되고, 가장 마지막 층에서 최종적인 예측값을 도출하는데, Attention 방법에서는 가장 마지막의 Hidden state 정보와 첫번째 Hidden state와의 알파값이라는 유사도 (기여도)를 산출함. 그리고 각 Hidden state 마다 유사도 (기여도) 값을 산출한 후, 과거에 한 번 사용된 Hidden state 값을 버리는 것이 아닌 그 값과 알파값과의 선형 결합을 통해 C라고 하는 Context vector를 만듦. 최종적으로 이 값과 가장 마지막 시점에서의 Hidden state vector를 결합하여 h 틸다라는 새로운 Hidden state vector를 만들고, 이 값을 통해서 최종적인 예측을 수행&lt;/li&gt;
  &lt;li&gt;이를 통해, Attention 구조를 차용하지 않았을 때에 비해 성능이 훨씬 향상되는 것이 일반적. 또한 알파값이 결국 0부터 1 사이이면서 모든 알파들의 값을 더했을 때는 1이 나오기 때문에, 이 예측 과정에서 몇 번째 시점이 가장 중요한 역할을 수행했는지를 추론할 수 있음&lt;/li&gt;
  &lt;li&gt;알파_i 값은 i번째 Hidden state vector가 Context vector 생성에 기여하는 비중
    &lt;ul&gt;
      &lt;li&gt;두 Hidden state vector 사이의 Score 산출 방식 중 가장 간단한 방식은 두 벡터의 내적을 사용하는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;알파값이라고 하는 유사도 (기여도)가 산출이 되면, Context vector는 단순히 지금까지 계산을 해왔던 각 시점들의 Hidden state vector와 알파와의 선형 결합
    &lt;ul&gt;
      &lt;li&gt;Vanilla RNN, LSTM, GRU 모두는 중간 시점에 해당하는 Hidden state h들의 정보는 버려지고, 마지막만 사용&lt;/li&gt;
      &lt;li&gt;Attention을 활용하게 되어 중간 시점에 해당하는 Hidden state vector도 다시 한번 의사결정에 활용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이렇게 Context vector를 만든 후, h 틸다라고 하는 새로운 Hidden state vector는 가장 마지막 시점의 Hidden state와 Context vector를 Concatenate 하고, 학습 대상이 되는 가중치를 곱한 후, 비선형 활성화를 통해 만들어지게 되고, 최종 예측을 수행&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;순환신경망은 순차 데이터를 처리하는데 특화된 인공신경망 구조&lt;/li&gt;
  &lt;li&gt;현 시점에서 Hidden state (H_t)는 이전 시점의 Hidden state (H_t-1)와 현 시점의 입력 (x_t)를 이용해 계산&lt;/li&gt;
  &lt;li&gt;Vanilla RNN은 Long-term relationship을 파악하는데 어려움을 겪어서 LSTM, GRU가 등장
    &lt;ul&gt;
      &lt;li&gt;LSTM은 Input, Forget, Output gate와 Cell state를 활용하여 정보를 선택적으로 활용&lt;/li&gt;
      &lt;li&gt;GRU는 Reset, Update gate 만을 사용하여 LSTM 보다 단순화 된 구조&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Attention은 시계열 데이터의 어느 시점이 최종 예측에 영향을 미쳤는지 판별할 수 있는 기법. 일반적으로 Attention 구조가 그렇지 않은 구조보다 일반적으로 예측 성능이 우수한 경향이 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-합성곱-기반의-시계열-데이터-회귀&quot;&gt;2. 합성곱 기반의 시계열 데이터 회귀&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;합성곱 즉, CNN 구조는 이미지 데이터를 처리하기 위해 고안된 구조&lt;/li&gt;
  &lt;li&gt;이 모델을 시계열 데이터에 적용했을 때도 효과적이었음&lt;/li&gt;
  &lt;li&gt;합성곱 신경망: 합성곱 연산을 통해 이미지로부터 필요한 특질 (Feature)을 스스로 학습할 수 있는 능력을 갖춘 신경망&lt;/li&gt;
  &lt;li&gt;이미지 인식 종류: Classification, Classification + Localization, Object Detection, Instance Segmentation
    &lt;ul&gt;
      &lt;li&gt;Classification: 주어진 이미지에서 어떤 객체가 있는지 분류하는 것&lt;/li&gt;
      &lt;li&gt;Classification + Localization: 어떤 객체가 어디에 있는지 Bounding box로 표현까지 해주는 것&lt;/li&gt;
      &lt;li&gt;Object Detection: 여러 객체에 대하여 Classification + Localization을 수행&lt;/li&gt;
      &lt;li&gt;Instance Segmentation: Object Detection을 pixel 단위로 수행하는 것 (Polygon 생성)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-basics-image-representation&quot;&gt;CNN Basics: Image Representation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;이미지를 어떻게 컴퓨터에게 숫자로 인식시킬 것인가?&lt;/li&gt;
  &lt;li&gt;컬러 이미지는 3차원의 Tensor로 표현됨 (Width * Height * 3 (RGB))&lt;/li&gt;
  &lt;li&gt;문제점: 모든 픽셀 하나의 입력 노드로 간주하고, 서로 다른 가중치로 연결하면 Input layer와 First hidden layer에 너무 많은 학습을 시켜야 하는 weights (가중치)가 생김
    &lt;ul&gt;
      &lt;li&gt;이 단계에서 필요한 가중치 수: 1685 * 2247 * 3 * 첫번째 Hidden layer의 노드 수&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-basics-convolution&quot;&gt;CNN Basics: Convolution&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Image Convolution (Filter, Kernel): 특정 속성을 탐지하는데 사용하는 Matrix (예를 들면, Edge detection)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;convolutional-neural-network-cnn&quot;&gt;Convolutional Neural Network (CNN)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;합성곱 신경망: 합성곱 연산을 통해 &lt;strong&gt;이미지로부터 필요한 특질을 스스로 학습할 수 있는 능력&lt;/strong&gt;을 갖춘 심층 신경망&lt;/li&gt;
  &lt;li&gt;이미지 데이터가 갖는 특징: 인접 픽셀 간 높은 상관관계 (Spatially-local correlation), 이미지의 부분적 특성은 고정된 위치에 등장하지 않음 (Feature invariance)
    &lt;ul&gt;
      &lt;li&gt;Spatially-local correlation을 고려하기 위해 Sparse connection 구성
        &lt;ul&gt;
          &lt;li&gt;Sparse connection: 전체 픽셀이 아닌 일부 픽셀만 가중치로 연결하여 연산 수행&lt;/li&gt;
          &lt;li&gt;인접한 변수만을 이용하여 새로운 Feature 생성&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Invariant feature를 추출하기 위해 Shared weight 개념 이용
        &lt;ul&gt;
          &lt;li&gt;Shared weight: 동일한 필터는 대상 영역에 상관없이 같은 가중치를 사용하여 연산을 수행&lt;/li&gt;
          &lt;li&gt;특정 특질을 추출하기 위한 도구로서 같은 대상 크기에는 위치가 다르더라도 동일한 Weight 적용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNN의 작동 과정
    &lt;ul&gt;
      &lt;li&gt;일반적인 CNN은 Convolution 연산, Activation 연산 (대부분 ReLU), Pooling 연산의 반복으로 구성&lt;/li&gt;
      &lt;li&gt;일정 횟수 이상의 Feature Learning 과정 이후에는, Flatten 과정을 통해 이미지가 1차원 벡터로 변환&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;이미지는 3차원 Tensor (RGB) 이므로, 필터 역시도 3차원&lt;/li&gt;
  &lt;li&gt;Filter가 한번에 한 칸씩 이동하면, 시간이 오래 걸리지 않을까? &lt;strong&gt;Stride&lt;/strong&gt;를 통해 한번에 여러 칸 이동하도록 함&lt;/li&gt;
  &lt;li&gt;가장자리에 있는 픽셀은 중앙의 픽셀보다 합성곱 연산이 적게 수행되지 않을까? &lt;strong&gt;Padding&lt;/strong&gt;을 추가하여 이를 보완&lt;/li&gt;
  &lt;li&gt;최종적인 Output size는 Stride와 Padding 값에 따라 달라질 수 있음
    &lt;ul&gt;
      &lt;li&gt;Output size = ((H + 2P - F / S) + 1) * ((W + 2P - F / S) + 1)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-basics-activation&quot;&gt;CNN Basics: Activation&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;합성곱을 통해 학습된 값들의 비선형 변환을 수행 (대부분 ReLU를 사용)&lt;/li&gt;
  &lt;li&gt;선형 변환만 하게 되면, 아무리 복잡한 구조로 조합해도 선형 조합의 결과는 선형 결과가 도출&lt;/li&gt;
  &lt;li&gt;대부분 Rectified Linear Unit (ReLU)을 사용
    &lt;ul&gt;
      &lt;li&gt;0보다 작은 값들은 0으로 변환하고, 0보다 큰 값들은 그 값을 반환&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-basics-pooling&quot;&gt;CNN Basics: Pooling&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;문제점: 고차원의 Tensor를 보다 Compact 하게 축약해야 하지 않을까?&lt;/li&gt;
  &lt;li&gt;Pooling: 일정 영역의 정보를 축약하는 역할 (Max pooling, Average pooling)&lt;/li&gt;
  &lt;li&gt;Strided convolution: Average pooling은 Strided convolution의 특수한 케이스 (모든 weight가 1/n)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-basics-flatten&quot;&gt;CNN Basics: Flatten&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;평탄화 (Flatten): 2차원, 3차원의 Matrix, Tensor 구조를 1차원의 Vector로 변환하는 과정&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-하이퍼파라미터&quot;&gt;CNN: 하이퍼파라미터&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Convolution filter의 크기: 이미지 데이터에는 주로 2-d Convolution을 사용 (가로와 세로의 크기가 같음)&lt;/li&gt;
  &lt;li&gt;Convolution filter의 수: 이 수가 많을수록 동질 영역에서 다양한 형태의 특질을 추출할 수 있음&lt;/li&gt;
  &lt;li&gt;Stride의 크기: Conv filter가 건너뛰는 픽셀의 수. Stride가 작을수록 촘촘하게 특질을 추출하지만 시간 오래 걸림&lt;/li&gt;
  &lt;li&gt;Zero Padding 크기: Conv 연산의 편의성을 위해 사용. Conv 연산 전후의 크기를 일치시키기 위한 목적
    &lt;ul&gt;
      &lt;li&gt;Filter와 Stride의 크기에 따라서 필요한 Padding의 크기가 결정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-architecture-1-alexnet&quot;&gt;CNN Architecture 1: AlexNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;원본 이미지가 너무 크게 되면, 연산량이 부족했기 때문에 이미지를 Resize (224 * 224 차원 * 3 (RGB))&lt;/li&gt;
  &lt;li&gt;Filter의 크기는 11 * 11 차원, Stride는 4, Filter의 개수는 96개 (Output 사이즈는 55 * 55)
    &lt;ul&gt;
      &lt;li&gt;초반에는 넓게 보되 듬성듬성 보고, 후반부에는 좁게 보되 세밀하게 보자&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;그 다음 Conv 연산에서는 Max pooling 후, Filter의 크기는 5 * 5 차원으로, 개수는 256개로 늘림&lt;/li&gt;
  &lt;li&gt;그리고 Max pooling 후, 3 * 3 Filter 384개로 적용
    &lt;ul&gt;
      &lt;li&gt;촘촘하게 보되, 특질을 찾아낼 수 있도록 Filter의 개수를 증가시킴&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;총 5번의 Conv 연산 후, 13 * 13 * 256 차원의 Tensor를 Flatten 하여 4096차원의 Dense layer로 변환&lt;/li&gt;
  &lt;li&gt;최종적으로 정리하면 다음과 같음
    &lt;ul&gt;
      &lt;li&gt;224 * 224 크기의 이미지를 Input으로 사용&lt;/li&gt;
      &lt;li&gt;초기 단계에서는 큰 필터 사이즈와 Stride를 사용&lt;/li&gt;
      &lt;li&gt;상위 Layer로 갈수록 작은 필터 사이즈와 Stride를 사용&lt;/li&gt;
      &lt;li&gt;2개의 Fully connected layer 존재&lt;/li&gt;
      &lt;li&gt;파라미터의 총 개수: 60 Million&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-architecture-2-vggnet&quot;&gt;CNN Architecture 2: VGGNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;원본 이미지는 24 * 24 * 3 차원의 Tensor인데, 64 차원의 Filter로 Conv 연산 두 번 수행&lt;/li&gt;
  &lt;li&gt;그리고 Max pooling 후, 128 차원의 Filter로 Conv 연산 두 번 수행&lt;/li&gt;
  &lt;li&gt;그 후, Max pooling 후, 사이즈를 줄여서 256 차원의 Filter로 Conv 연산 세 번 수행&lt;/li&gt;
  &lt;li&gt;최종적으로 정리하면 다음과 같음
    &lt;ul&gt;
      &lt;li&gt;AlexNet에 비해서 단순하지만 깊은 구조&lt;/li&gt;
      &lt;li&gt;3 by 3 Convolution with stride 1을 기본 연산으로 하며, 중간 중간에 2 by 2 max pooling을 수행&lt;/li&gt;
      &lt;li&gt;파라미터의 총 개수: 138 Million&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;cnn-for-time-series-data&quot;&gt;CNN for Time-Series Data&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Time-Series Data의 가정은 모든 변수는 동일한 주기 (초, 분, 시간 등)로 수집되고 있음&lt;/li&gt;
  &lt;li&gt;Task 1 (분류): 특정 기간 (제품 가공 기간) 데이터를 입력으로 하고, 특정 범주를 출력으로 하는 모델 (양/불 판정 등)&lt;/li&gt;
  &lt;li&gt;Task 2 (회귀): 특정 기간 데이터를 입력으로 하고, 특정 수치 (품질 지표)를 출력으로 하는 모델
    &lt;ul&gt;
      &lt;li&gt;Task 1, 2는 Input인 X 데이터는 동일하지만, Output인 Y 데이터의 형태가 다름&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Task 3 (회귀): 일부 기간 데이터를 입력으로 하고, 이후 기간 데이터를 예측 (태양광 발전량 예측, 전력 소모 예측 등)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Task 4 (이상탐지): 일부 기간 데이터를 입력으로 하고, 해당 상황의 정상 및 비정상 여부를 탐지&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;1-D Convolution vs 2-D Convolution
    &lt;ul&gt;
      &lt;li&gt;시계열 데이터는 이미지 데이터와 달리, 변수들 사이에 Spatial Correlation이 존재하지 않음&lt;/li&gt;
      &lt;li&gt;시간 축으로 움직이는 Convolution은 의미가 있지만, 변수 축으로 움직이는 Convolution은 의미가 없음&lt;/li&gt;
      &lt;li&gt;2-D Convolution은 시간과 변수 두 축을 모두 Convolution 연산을 통해서 탐색하는 방식&lt;/li&gt;
      &lt;li&gt;1-D Convolution은 모든 변수를 한번에 고려하여 시간 축에 대해서만 Convolution 연산을 수행&lt;/li&gt;
      &lt;li&gt;따라서 Filter의 크기가 정사각형이 아닌, 직사각형 형태의 Filter를 사용. 즉, 이것은 모든 변수를 한꺼번에 고려하여, 시점에 대해서만 이동을 시키는 것
        &lt;ul&gt;
          &lt;li&gt;Filter의 세로 크기는 변수의 개수만큼 고정이 될 것 (이미지 CNN과의 차이)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;1-D Convolution에서 1개의 Filter를 사용하면 1개의 Vector가 결과물로 산출&lt;/li&gt;
      &lt;li&gt;이와 같은 Conv 연산을 반복하면, 각 Vector 상의 같은 인덱스는 같은 Time-series에 대한 데이터&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dilated-convolution&quot;&gt;Dilated Convolution&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;의문: 보다 긴 길이의 시계열 데이터를 효율적으로 처리할 수는 없을까?
    &lt;ul&gt;
      &lt;li&gt;Standard Convolution은 항상 인접한 연속된 시점의 데이터에 대한 합성곱 연산을 수행&lt;/li&gt;
      &lt;li&gt;합성곱 연산을 조금 더 띄엄띄엄 해본다면?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-1&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;합성곱 연산은 Feature Invariance와 Spatial Correlation을 반영하기 위해 사용되는 연산&lt;/li&gt;
  &lt;li&gt;이미지를 처리할 때는 정방형의 필터를 사용하여 가로, 세로 두 방향으로 움직이는 2-D 합성곱 연산을 사용&lt;/li&gt;
  &lt;li&gt;반면, 시계열 데이터에서는 변수 축으로 필터가 이동하는 것은 의미가 없어 필터의 한쪽 크기는 변수의 개수와 같음&lt;/li&gt;
  &lt;li&gt;그리고 시간 축으로만 움직이는 1-D 합성곱 연산을 수행하도록 함&lt;/li&gt;
  &lt;li&gt;합성곱 연산은 한 레이어에서만 적용하는 것이 아닌, 반복적으로 쌓아 올려서 사용할 수도 있음&lt;/li&gt;
  &lt;li&gt;보다 긴 길이의 시계열 데이터를 한번에 처리하기 위해서는, Standard Convolution 보다는 중간 지점을 생략하는 Dilated Convolution을 사용하면 보다 효율적인 정보의 처리가 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-트랜스포머-기반의-시계열-데이터-회귀&quot;&gt;3. 트랜스포머 기반의 시계열 데이터 회귀&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;앞서 배운 CNN은 원래 이미지 데이터를 처리하기 위해 제안되었으나, 시계열 데이터에 적용한 것&lt;/li&gt;
  &lt;li&gt;RNN은 원래부터 시계열 데이터나, 순차 데이터를 처리하기 위해 만들어진 방법론&lt;/li&gt;
  &lt;li&gt;Transformer는 원래 자연어 데이터를 처리하기 위해 만들어진 모델이지만, 이 모델이 이미지와 시계열으로도 확장
    &lt;ul&gt;
      &lt;li&gt;Transformer는 언어 모델의 한 종류&lt;/li&gt;
      &lt;li&gt;언어 모델은 특정 문장 (단어의 나열)이 등장할 확률을 계산해주는 모델&lt;/li&gt;
      &lt;li&gt;즉, 특정한 언어에서 특정한 문장 즉, 단어의 나열이 얼마나 그럴듯하고 자연스러운지 확률을 계산해주는 모델&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformer는 Attention의 병렬적 사용을 통해 효율적인 학습이 가능한 구조의 언어 모델
    &lt;ul&gt;
      &lt;li&gt;개괄적 구조: 내부에 인코더 파트와 디코더 파트가 존재하며, 이 둘 사이를 이어주는 연결고리가 존재함&lt;/li&gt;
      &lt;li&gt;Input 데이터를 처리하는 Encoder 모듈과 처리가 완료된 다음 단어를 하나씩 반환해주는 Decoder 모듈이 있음&lt;/li&gt;
      &lt;li&gt;Encoder는 여섯 개의 블록으로 구성되어 있고, Decoder도 역시 여섯 개의 블록으로 구성됨&lt;/li&gt;
      &lt;li&gt;입력 데이터가 들어왔을 때, Encoder는 층층이 이전 단계에서 다음 단계로 정보가 전달&lt;/li&gt;
      &lt;li&gt;Decoding 과정에서는 가장 마지막 단계에서의 Encoder가 모든 단계의 Decoder에 정보를 전달하고, 하위 Decoder의 정보가 상위 Decoder로 정보를 전달해주는 구조&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transformer의-작동-원리&quot;&gt;Transformer의 작동 원리&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;개별 단어에 대한 임베딩 벡터 (Word2Vec, GloVe 등)를 최초 입력으로 사용함
    &lt;ul&gt;
      &lt;li&gt;Word2Vec, GloVe 등의 방법론은 실제 단어에서 두 단어가 의미적으로 유사하면, 공간상에서도 유사하도록 학습&lt;/li&gt;
      &lt;li&gt;이 단어 임베딩은 가장 아래에 위치한 인코더에서만 입력으로 1회 사용&lt;/li&gt;
      &lt;li&gt;나머지 인코더들을 하위 인코더에서 출력된 결과물을 입력으로 사용&lt;/li&gt;
      &lt;li&gt;최초 논문에서는 512 차원의 임베딩을 사용&lt;/li&gt;
      &lt;li&gt;입력으로 사용되는 리스트는 사용자가 지정하는 하이퍼파라미터 - 컴퓨팅 자원이 충분하다면 큰 값 지정 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Positional Encoding
    &lt;ul&gt;
      &lt;li&gt;Transformer는 단어를 순차적이지 않고, 한번에 받아서 입력 시퀀스에서 단어들 간의 위치 관계를 표현해야 함
        &lt;ul&gt;
          &lt;li&gt;RNN 구조에서는 단어를 순차적으로 받았던 것과 약간 차이가 있음&lt;/li&gt;
          &lt;li&gt;따라서 Transformer에서는 입력 시퀀스에서 어떤 단어가 먼저 들어왔고, 나중에 들어왔는지 확인 필요&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;이 역할을 수행하도록 Positional Encoding을 만들고, 모든 단어 임베딩에 이것을 더해 입력 벡터를 구성함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multi-Head Attention
    &lt;ul&gt;
      &lt;li&gt;어떤 토큰들을 받았을 때, 그 문서 안에서 또는 문장 안에서 단어들이 서로 어떤 연관관계를 가지는지 해석하는 것&lt;/li&gt;
      &lt;li&gt;Positional Encoding이 더해진 단어 임베딩은 첫 번째 인코더 블록에서 Self-Attention과 FFNN을 거침&lt;/li&gt;
      &lt;li&gt;특정 위치의 단어는 해당 위치를 유지하면서 연산이 수행
        &lt;ul&gt;
          &lt;li&gt;Self-Attention 과정에서는 이 경로간 의존성이 존재&lt;/li&gt;
          &lt;li&gt;FFNN (Feed Forward Neural Network) 과정에서는 의존성이 존재하지 않아서 병렬화가 가능&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Encoding Procedure
    &lt;ul&gt;
      &lt;li&gt;인코더는 일련의 벡터들을 입력으로 받음&lt;/li&gt;
      &lt;li&gt;입력된 벡터들은 Self-Attention과 FFNN을 거쳐서 상위 인코더의 입력으로 투입&lt;/li&gt;
      &lt;li&gt;즉, Encoder 1의 Output은 Encoder 2의 Input으로도 활용된다는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Self-Attention에 대한 이해
    &lt;ul&gt;
      &lt;li&gt;The animal did’t cross the street because it was too tired.&lt;/li&gt;
      &lt;li&gt;이와 같은 문장에서 it이 해당하는 의미를 사람은 알기 쉽지만, 알고리즘은 그렇지 못함&lt;/li&gt;
      &lt;li&gt;이는 입력 시퀀스의 다른 위치에 있는 단어를 둘러보면서, 특정 위치의 단어를 잘 설명 및 표현 할 수 있게 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Self-Attention의 절차
    &lt;ul&gt;
      &lt;li&gt;Step 1. 입력 벡터에 대해서 세 가지의 벡터를 생성
        &lt;ul&gt;
          &lt;li&gt;실제로는 Query, Key, Value에 대응하는 행렬을 곱해서 생성&lt;/li&gt;
          &lt;li&gt;Query: 다른 단어들을 고려하여 표현하고자 하는 대상이 되는 현재 단어에 대한 임베딩 벡터
            &lt;ul&gt;
              &lt;li&gt;어떤 단어가 다른 단어들과 무슨 관계가 있는지를 알고싶은 질의 대상이 되는 단어&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Key: Query가 들어왔을 때, 다른 단어들과 매칭을 하기 위해 사용되는 레이블로 사용되는 임베딩 벡터&lt;/li&gt;
          &lt;li&gt;Value: Key와 연결된 실제 단어를 나타내는 임베딩 벡터&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Step 2. 지금 표현하고자 하는 단어 (Q)에 대하여 어떤 단어를 고려해야 하는지 (K)를 알려주는 스코어 산출
        &lt;ul&gt;
          &lt;li&gt;Q와 K를 곱한 후, 소프트맥스 함수를 취하여 이 스코어를 계산해줌&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Step 3. Step 2에서 계산한 스코어를 차원의 루트 수로 나눠줌
        &lt;ul&gt;
          &lt;li&gt;이 과정을 통해 Gradient 전파가 보다 안정적으로 수행됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Step 4. Step 3의 결과를 이용하여 소프트맥스 함수를 적용해 해당 단어에 대한 집중도를 산출&lt;/li&gt;
      &lt;li&gt;Step 5. Step 4에서 산출된 확률값과 해당 단언의 Key 값을 곱함&lt;/li&gt;
      &lt;li&gt;Step 6. Step 5에서 산출된 모든 값들을 더해서 출력으로 반환&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Masked Multi-head Attention
    &lt;ul&gt;
      &lt;li&gt;디코딩 단계에서 셀프 어텐션은 Query 토큰보다 뒤에 위치한 토큰들에 대한 정보는 가용하지 않다고 가정하고, 해당 부분을 전부 Masking 처리&lt;/li&gt;
      &lt;li&gt;순차적으로 수행할 필요 없이 한번에 수행 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The Final Linear and Softmax Layer
    &lt;ul&gt;
      &lt;li&gt;Linear layer: 단순 FFNN 형태로서 마지막 디코더의 출력 결과물을 이용해 모든 단어의 출력 확률을 산출하기 위해서 차원을 늘려주는 역할을 수행&lt;/li&gt;
      &lt;li&gt;Softmax layer: 개별 단어들의 출력 확률값을 반환&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;transformer의-시계열-데이터-적용&quot;&gt;Transformer의 시계열 데이터 적용&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Transformer를 다변량 시계열 데이터에 최초로 적용한 논문
    &lt;ul&gt;
      &lt;li&gt;Transformer의 Encoder 구조만을 사용&lt;/li&gt;
      &lt;li&gt;Pre-training 과업을 위하여 연속적 길이의 Input masking 사용&lt;/li&gt;
      &lt;li&gt;Layer Normalization 대신 Batch Normalization 사용&lt;/li&gt;
      &lt;li&gt;Fine-tuning 단계에서 구조를 어떻게 설계하느냐에 따라 분류, 회귀, 예측 등 다양한 테스크에 적용 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Time-Series Transformer (TST) 작동 원리
    &lt;ul&gt;
      &lt;li&gt;TST는 크게 Pre-training과 Fine-tuning의 두 부분으로 구분
        &lt;ul&gt;
          &lt;li&gt;Pre-training은 데이터를 통해 이 Transformer의 구조를 미리 학습하는 것&lt;/li&gt;
          &lt;li&gt;Fine-tuning은 원하는 문제를 잘 풀 수 있도록 고도화하는 단계&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;입력 데이터 변환
        &lt;ul&gt;
          &lt;li&gt;원본 입력 데이터 X는 m개의 변수와 w개의 Time window length를 가짐&lt;/li&gt;
          &lt;li&gt;Pre-training 과정에서는 이 중 일부를 Masking 한 뒤, Transformer의 Input이 되는 d차원으로 변환&lt;/li&gt;
          &lt;li&gt;Fine-tuning 과정에서는 Masking 없이 d차원으로 변환&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;입력 데이터 Masking
        &lt;ul&gt;
          &lt;li&gt;Transformer의 사전 학습 목적은 Masking 된 부분을 정확하게 예측하는 것으로 설계
            &lt;ul&gt;
              &lt;li&gt;각 Cell에 대한 Masking 여부를 독립적으로 결정하게 되면, Trivial Solution으로도 문제를 잘 맞춤&lt;/li&gt;
              &lt;li&gt;Trivial Solution은 Masking 된 Cell 이전 시점 혹은 이후 시점 값을 그대로 사용하거나, 평균값 사용&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Masking 길이가 평균만큼이 되는 기하 분포를 따르도록 Markov Chain을 적용하여 Masking 여부 결정
            &lt;ul&gt;
              &lt;li&gt;모든 변수에 대해 동일한 시점을 Masking 하는 것보다 변수 별로 독립적으로 Masking segment를 결정하는 것이 실험적으로 더 우수한 성능을 보였음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Positional Encoding
        &lt;ul&gt;
          &lt;li&gt;NLP의 Transformer처럼 TST에서도 입력 데이터에 이것을 더해 Transformer 인코더의 입력값으로 사용&lt;/li&gt;
          &lt;li&gt;고정된 Positional Encoding을 사용할 수도 있고, 학습 가능한 Positional Encoding을 사용할 수도 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Self-Attention in Transformer Encoder Block
        &lt;ul&gt;
          &lt;li&gt;총 3개의 Encoder 블록을 사용 (자연어에서는 6개를 사용했음)&lt;/li&gt;
          &lt;li&gt;NLP에서의 Transformer와 다르게 Layer Normalization 대신 Batch Normalization 사용
            &lt;ul&gt;
              &lt;li&gt;시계열 데이터는 NLP Word Embedding에는 없는 이상치가 존재&lt;/li&gt;
              &lt;li&gt;시계열 데이터는 각 관측치의 길이 변화가 NLP의 문장 길이의 변화보다 작음&lt;/li&gt;
              &lt;li&gt;이러한 상황 하에서는 실험적으로 Batch Normalization이 우수한 성능을 보이는 것으로 입증&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Pre-training
        &lt;ul&gt;
          &lt;li&gt;각 관측치 및 Epoch 마다 독립적으로 생성한 Mask를 Input과 Element-wisse Multiplication 하여 Masked Input을 도출&lt;/li&gt;
          &lt;li&gt;이것을 Input Encoding과 Transformer Encoder에 순차적으로 넣어서 Latent Representation을 도출하고, 이를 Linear Output Layer에 넣어서 모든 값이 채워져 있는 각 시점의 데이터를 예측&lt;/li&gt;
          &lt;li&gt;Masking 된 부분의 실제값과 예측값의 Mean Squared Error를 기반으로 모델을 학습&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Fine-tuning
        &lt;ul&gt;
          &lt;li&gt;1단계: Masking을 적용하지 않은 Input time window를 Input encoding과 Transformer Encoder에 순차적으로 넣어서 Representation을 도출&lt;/li&gt;
          &lt;li&gt;2단계: 도출된 모든 시점의 Representation을 Concatenate 한 것을 Output Linear Layer에 Input으로 넣어서 회귀 또는 분류의 정답을 예측&lt;/li&gt;
          &lt;li&gt;3단계: Task의 실제 정답과 TST가 예측한 값의 차이를 통해 Output Linear Layer를 Fine-tuning&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tst의-성능&quot;&gt;TST의 성능&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Supervised Learning에 사용되는 Label의 비율 증가에 따른 TST (Pretrained)와 TST (Sup only)의 성능 변화로 다음 결과 도출
    &lt;ul&gt;
      &lt;li&gt;두 모델 모두 사용 가능한 Label의 비율이 증가할수록 성능이 향상&lt;/li&gt;
      &lt;li&gt;Pretrained TST가 Sup only TST 보다 모든 비율에서 높은 성능을 도출한 것을 통해 동일한 Training set을 중복 사용하여 모델을 학습한 것이 효과가 있다는 것을 알 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Fine-tuning 데이터의 개수가 고정되었을 때, Pre-training에 사용되는 데이터의 비율 증가에 따른 Pretrained TST의 성능 변화를 통해 다음 결과 도출
    &lt;ul&gt;
      &lt;li&gt;Pre-training에서 많은 데이터를 학습할수록 Pretrained TST의 성능이 향상&lt;/li&gt;
      &lt;li&gt;Pretrained TST의 Fine-tuning 데이터의 개수가 적을수록 Pre-training에 사용되는 데이터의 비율 증가에 따른 성능 향상 폭이 큼&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;summary-2&quot;&gt;Summary&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;TST는 Transformer의 Encoder 구조만을 사용&lt;/li&gt;
  &lt;li&gt;Pre-training 과업을 위하여 연속적 길이의 Input masking을 사용&lt;/li&gt;
  &lt;li&gt;Layer Normalization 대신 Batch Normalization 사용&lt;/li&gt;
  &lt;li&gt;Fine-tuning 단계에서 구조를 어떻게 설계하느냐에 따라 회귀, 분류, 예측 등 다양한 테스크로 적용 가능&lt;/li&gt;
  &lt;li&gt;Pre-training을 통해 기존의 Supervised Learning만 수행했을 경우보다, 우수한 예측 성능을 보임&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 25 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/LG-Aimers-%EC%8B%9C%EA%B3%84%EC%97%B4-%EB%B6%84%EC%84%9D/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/LG-Aimers-%EC%8B%9C%EA%B3%84%EC%97%B4-%EB%B6%84%EC%84%9D/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Education</category>
        
        <category>LG Aimers 2기</category>
        
      </item>
    
      <item>
        <title>LG Aimers 2기 인과추론 (서울대학교 이상학 교수님)</title>
        <description>&lt;p&gt;이번 글에서는 LG Aimers의 AI 전문가 과정에서 인과성에 대해 추론하고, 경험적 데이터를 사용해 인과 관계를 결정하는 방법을 익히게 됩니다. 이를 통해 데이터를 생성한 프로세스에 대해 만들어야 하는 필수 가정과 이러한 가정이 합리적인지 평가하는 방법, 마지막으로 추정되는 양을 해석하는 방법을 학습합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-causality&quot;&gt;1. Causality&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;인과성에 대한 소개와 인과적 추론을 하기 위한 기본 개념&lt;/li&gt;
  &lt;li&gt;인과성 (Causality)은 하나의 어떤 무엇인가가 다른 무엇을 생성함에 있어서 영향을 미치는 것
    &lt;ul&gt;
      &lt;li&gt;원인과 결과 사이의 관계는 필요조건이나 충분조건일 필요는 없음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;과학은 일반적 사실이나 법칙을 포함하는 지식 체계
    &lt;ul&gt;
      &lt;li&gt;법칙은 현상의 본질적 구조를 명확하게 한 것&lt;/li&gt;
      &lt;li&gt;즉, 원인과 결과의 매커니즘을 기술한 것 (Cause and Effect)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AI는 어떤 에이전트가 목표를 성취하기 위해서 합리적인 액션을 취하는 것
    &lt;ul&gt;
      &lt;li&gt;강화학습에서는 주어진 상황에서 어떤 행동을 취할지를 학습함&lt;/li&gt;
      &lt;li&gt;환경에 변화를 주어 원하는 상태로 변화시키는 인과관계로 해석이 가능함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 사이언스는 데이터를 수집, 처리, 분석하여 결과를 대중들과 상호작용할 것인가 하는 많은 부분에서 상관성과 인과성 모두 고려해야 함 (기계학습은 데이터의 상관성을 학습하는 것)&lt;/li&gt;
  &lt;li&gt;인과추론은 알 수 없는 실험 결과를 관측 데이터와 연결하는 것 (모델에 대한 형식적, 수학적 이해가 필요함)&lt;/li&gt;
  &lt;li&gt;Structural Causal Model (SCM)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;pearls-causal-hierarchy&quot;&gt;Pearl’s Causal Hierarchy&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Associateional or Observational (기본적인 관측 계층)&lt;/li&gt;
  &lt;li&gt;Interventional or Experimental (실험 계층)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Counterfactual (관측과 실험에 의한 값을 동시에 고려하는 반사실적 계층)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Correlation (level 1) vs Causation (level 2)
    &lt;ul&gt;
      &lt;li&gt;초콜릿의 소비량과 노벨상 수상의 상관관계&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;simpsons-paradox&quot;&gt;Simpson’s Paradox&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;신장 결석 환자가 병원에 왔음. 그리고 환자를 진찰 및 처방하고, 결과가 리포트 되고, 헬스케어 데이터가 생성&lt;/li&gt;
  &lt;li&gt;데이터를 확인해보니, 신장 결석 사이즈가 작을 때는 처방 A의 효과가 더 좋았음&lt;/li&gt;
  &lt;li&gt;신장 결석 사이즈가 클 때도 처방 A의 효과가 더 좋았음&lt;/li&gt;
  &lt;li&gt;그러나 모든 데이터를 합쳐서 보면, 처방 B의 효과가 더 좋았던 것을 볼 수 있음&lt;/li&gt;
  &lt;li&gt;즉, 이 프로세스는 결석의 상태와 처방에 따라 환자의 나중 건강 상태가 결정되고 있음&lt;/li&gt;
  &lt;li&gt;인과적 분석을 하기 위해서는 주어진 데이터 뿐 아니라, 각 변수들이 가지는 인과적 관계를 이해하는 것이 필요함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;데이터-분석-시-고려할-것&quot;&gt;데이터 분석 시, 고려할 것&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;주어진 데이터가 상관성을 지니고 있는지, 인과성을 지니고 있는지 확인&lt;/li&gt;
  &lt;li&gt;알고자 하는 질문이 조건부 확률 같은 상관성에 대한 것인지, 인과성에 관한 것인지를 알아야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-causal-effect-identification&quot;&gt;2. Causal Effect Identification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;인과추론을 수행하기 위한 기본적인 방법론 제시&lt;/li&gt;
  &lt;li&gt;Query, Causal Diagram, Data -&amp;gt; Causal Inference Engine -&amp;gt; Solution&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-modern-identification&quot;&gt;3. Modern Identification&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;인과추론의 다양한 연구 방향 제시&lt;/li&gt;
  &lt;li&gt;General Identification: 여러 데이터가 한 도메인에 주어졌을 때, 그것을 활용하여 원하는 인과 효과를 계산하는 것&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 24 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/LG-Aimers-%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/LG-Aimers-%EC%9D%B8%EA%B3%BC%EC%B6%94%EB%A1%A0/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Education</category>
        
        <category>LG Aimers 2기</category>
        
      </item>
    
      <item>
        <title>LG Aimers 2기 Explainable AI (서울대학교 문태섭 교수님)</title>
        <description>&lt;p&gt;이번 글에서는 LG Aimers의 AI 전문가 과정에서 설명가능한 AI에 대하여 학습합니다. 머신러닝은 크고 복잡한 데이터를 이해하고, 이들 간의 관계성을 살펴보는 기술이지만 본질적으로 해석 가능성을 제한하는 블랙박스인 경우가 많습니다. 현실에서 이것을 사용할 때는 그에 따른 해석이 요구되는데, 따라서 그 한계를 보완하기 위해 Explainable AI 즉, 설명가능한 AI 기술에 대해 학습하여 머신러닝 모델과 그 의미에 대하여 학습할 것입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-explainable-ai-1&quot;&gt;1. Explainable AI 1&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;최근 딥러닝은 그 성능이 매우 빠르게 발전하고 있음&lt;/li&gt;
  &lt;li&gt;하지만 대용량 학습 데이터로부터 학습하는 모델 구조는 점점 더 복잡해지고, 이해하기 어려워진다는 한계점이 있음&lt;/li&gt;
  &lt;li&gt;즉, 입력을 주게 되면, 그에 따른 결과가 나오는 하나의 블랙박스 형태의 결과가 보여진다는 것
    &lt;ul&gt;
      &lt;li&gt;이러한 예측 결과가 사람에게 직접 영향을 미치게 되는 경우에는 이 한계점은 매우 심각하게 될 것&lt;/li&gt;
      &lt;li&gt;자율주행, 의학적 진단, 대출 승인 등을 비롯한 AI 편향성 문제&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Reliability &amp;amp; Robustness (Pascal VOC 2007 Classification)
    &lt;ul&gt;
      &lt;li&gt;말 이미지가 주어졌을 때, 말의 어느 부분을 보고 해당 사진이 말이라는 것을 알게 됐는지 특정 부분을 하이라이트&lt;/li&gt;
      &lt;li&gt;XAI 기법은 이미지에서 말에 해당하는 부분이 아닌, 아랫쪽에 주로 하이라이트가 되어 있었음&lt;/li&gt;
      &lt;li&gt;데이터를 확인해보니, 말 사진에는 텍스트 워터마크가 있었고, 이것으로 말이라고 예측하고 있었다는 것&lt;/li&gt;
      &lt;li&gt;이처럼 XAI 기법을 통해서 모델이나 데이터셋의 오류를 색출하고, 편향성을 확인 할 수 있을 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결국, 왜 알고리즘이 그런 예측 결과를 냈는지 설명하여 신뢰 여부를 결정할 수 있어야 함&lt;/li&gt;
  &lt;li&gt;XAI에서 설명가능하다는 것은 모델을 사용할 때, 그 동작을 이해하고 신뢰할 수 있게 해주는 기계학습 기술으로 정의&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;xai의-대비되는-종류&quot;&gt;XAI의 대비되는 종류&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Local vs Global
    &lt;ul&gt;
      &lt;li&gt;Local: Describes an individual prediction (개별적인 예측 결과를 설명)&lt;/li&gt;
      &lt;li&gt;Global: Describes entire model behavior (전반적인 행동을 설명)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;White-box vs Black-box
    &lt;ul&gt;
      &lt;li&gt;White-box: Explainer can access the inside of model (모델 내부 구조를 알고 설명)&lt;/li&gt;
      &lt;li&gt;Black-box: Explainer can access only the output (모델 구조를 모르고, 출력만으로 설명)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Intrinsic vs Post-hoc
    &lt;ul&gt;
      &lt;li&gt;Intrinsic: Restricts the model complexity before training&lt;/li&gt;
      &lt;li&gt;Post-hoc: Applies after the ML model is trained&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model-specific vs Model-agnostic
    &lt;ul&gt;
      &lt;li&gt;Model-specific: Some methods restricted to specific model classes&lt;/li&gt;
      &lt;li&gt;Model-agnostic: Some methods can be used for any model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;-&amp;gt; Linear model, Simple Decision Tree: Global, White-box, Intrinsic, Model-specific &lt;br /&gt;
-&amp;gt; Grad-CAM: Local, White-box, Post-hoc, Model-agnostic&lt;/p&gt;

&lt;h3 id=&quot;simple-gradient-method&quot;&gt;Simple Gradient method&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Simply use the gradient as the explanation (딥러닝의 Back-propagation)&lt;/li&gt;
  &lt;li&gt;Strength: Easy to compute (via Back-propagation)&lt;/li&gt;
  &lt;li&gt;Weakness: Becomes noisy (due to shattering gradient problem)
    &lt;ul&gt;
      &lt;li&gt;즉, 똑같은 예측 결과를 갖는 조금씩 변하는 이미지들에 대해 각 이미지에 대한 설명이 많이 다를 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;smoothgrad&quot;&gt;SmoothGrad&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Simple method to address the noisy gradients&lt;/li&gt;
  &lt;li&gt;이미지가 주어졌을 때, 작은 노이즈인 엡실론을 섞어준 뒤, 노이즈가 섞인 입력 이미지에 Gradient를 구하는 과정을 여러 번 수행하여 그 Gradient의 평균으로 설명하는 것 (대략 50번 정도)&lt;/li&gt;
  &lt;li&gt;Strength: Clearer interpretation via simple averaging, Applicable to most sensitive maps&lt;/li&gt;
  &lt;li&gt;Weakness: Computationally expensive&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-explainable-ai-2&quot;&gt;2. Explainable AI 2&lt;/h2&gt;

&lt;h3 id=&quot;class-activaiton-map-cam&quot;&gt;Class Activaiton Map (CAM)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;어떤 이미지가 CNN 모델의 입력으로 주어졌을 때, 이미지에 해당하는 Activation들이 최종 이미지에 최종 Activation으로 나타나게 됨. 이러한 각 Activation map은 Global Average Pooling 별로 학습된 w_1부터 w_n을 활용하여 Activation map을 결합하게 됨. 최종 결합된 이미지는 하이라이트 되어 표현됨&lt;/li&gt;
  &lt;li&gt;어떤 Activation map에 Activation이 크게 된다는 것은 그 Map이 주어진 입력과 관련이 많다는 뜻이고, 그것을 결합하는 w가 크다는 것도 최종 분류에 큰 영향을 주는 Activation이라는 것&lt;/li&gt;
  &lt;li&gt;CAM은 Object detection 이나, Semantic segmentation 등 더 복잡한 응용 분야에도 적용 가능&lt;/li&gt;
  &lt;li&gt;Strength: It clearly shows what objects the model is looking at&lt;/li&gt;
  &lt;li&gt;Weakness
    &lt;ul&gt;
      &lt;li&gt;Model-specific: It can be applied only to models with limited architecture&lt;/li&gt;
      &lt;li&gt;It can only be obtained at the last convolutional layer and this makes the interpretation resolution coarse (마지막 Convolutional layer의 Activation에서 얻을 수 있으므로 해상도가 좋지 않음)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;grad-cam&quot;&gt;Grad-CAM&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CAM 방식을 보완한 방식으로, Global Average Pooling layer가 없는 모델도 적용할 수 있음&lt;/li&gt;
  &lt;li&gt;특정 모델 구조를 가지고 학습된 w를 사용하는 것이 아닌, Activation map의 Gradient를 구한 다음에 그것의 Global Average Pooling 값으로 w를 적용한다는 것&lt;/li&gt;
  &lt;li&gt;Strength: Model-agnostic 하여 It can be applied to various output models&lt;/li&gt;
  &lt;li&gt;Weakness: Average gradient sometimes is not accurate&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;perturbation-based&quot;&gt;Perturbation-based&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;입력 데이터를 조금씩 바꾸면서 그에 대한 출력을 보고, 그 변화에 기반하여 설명하는 접근법&lt;/li&gt;
  &lt;li&gt;Local Interpretable Model-agnostic Explanations (LIME)
    &lt;ul&gt;
      &lt;li&gt;어떤 분류기가 딥러닝 모델처럼 복잡한 비선형적 특징을 가지더라도, 주어진 데이터 포인트들에 대해서는 아주 Local 하게는 다 선형적인 모델로 근사화가 가능하다는 관찰에서 출발&lt;/li&gt;
      &lt;li&gt;그래서 주어진 데이터를 조금씩 교란해 가면서, 교란된 입력 데이터를 모델에 여러 번 통과시켜 나오는 출력을 보고, 나오는 입출력 Pair들을 간단한 선형 모델로 근사하여 설명을 얻어내는 방식&lt;/li&gt;
      &lt;li&gt;Strength: Black-box interpretation (딥러닝 모델 뿐 아니라, 입력과 출력을 얻을 수 있다면 모두 적용 가능)&lt;/li&gt;
      &lt;li&gt;Weakness: Computationally expensive, Hard to apply to certain kind of models, When the underlying model is still locally non-linear&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Randomized Input Sampling for Explanation (RISE)
    &lt;ul&gt;
      &lt;li&gt;LIME 방식과 비슷하게, 여러 번 입력을 Perturb 해서 설명을 구하는 방식&lt;/li&gt;
      &lt;li&gt;랜덤한 Mask를 만들어서 그 Mask를 씌운 입력이 모델을 통과 했을 때, 해당 클래스에 대한 예측 확률이 얼마나 떨어지는 확인하여 설명력을 예측하게 됨. 즉, 여러 개의 랜덤 마스킹이 되어 있는 입력에 대해 출력 스코어를 구하고, 그 확률을 통해 이 마스크들을 가중치를 두어 평균을 냈을 때 나오는 것이 설명 Map&lt;/li&gt;
      &lt;li&gt;Strength: Much clear saliency-map&lt;/li&gt;
      &lt;li&gt;Weakness: High computational complexity, Noisy due to sampling&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Different approach for XAI
    &lt;ul&gt;
      &lt;li&gt;Identify most influential training data point for the given prediction&lt;/li&gt;
      &lt;li&gt;Influence function
        &lt;ul&gt;
          &lt;li&gt;Measure the effect of removing a training sample on the test loss value&lt;/li&gt;
          &lt;li&gt;특정 Training 이미지 없이 모델을 훈련시켰을 때, 해당 모델의 성능이 얼마만큼 변할지 근사화 하는 함수&lt;/li&gt;
          &lt;li&gt;이 함수를 가지고, 각 Training 이미지마다 영향력을 계산하고, 그 값이 가장 큰 이미지를 설명으로 제공&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-explainable-ai-3&quot;&gt;3. Explainable AI 3&lt;/h2&gt;

&lt;h3 id=&quot;xai-방법을-비교-평가하는-방법&quot;&gt;XAI 방법을 비교 평가하는 방법&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;사람들이 직접 XAI 방법들이 만들어낸 설명을 보고 비교 및 평가하는 것
    &lt;ul&gt;
      &lt;li&gt;AMT (Amazon Mechanical Turk) Test&lt;/li&gt;
      &lt;li&gt;Guided Backprop, Guided Grad-CAM 등 확인 가능&lt;/li&gt;
      &lt;li&gt;Weakness: Obtaining human assessment is very expensive&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Human annotation을 활용하는 것
    &lt;ul&gt;
      &lt;li&gt;Some metrics employ human annotations (localization and semantic segmentation) as a ground truth, and compare them with interpretation&lt;/li&gt;
      &lt;li&gt;Pointing game: Bounding box를 활용하여 평가하는 방법&lt;/li&gt;
      &lt;li&gt;Weakly supervised semantic segmentation: 어떤 이미지에 대해 Label만 주어졌을 때, 그것을 활용하여 픽셀 별로 객체의 Label을 예측하는 방법 (Weakly supervised인 이유는, 픽셀 별로 정답 Label이 없기 떄문)&lt;/li&gt;
      &lt;li&gt;IoU (Intersection over Union): 정답 Map과 이렇게 만들어낸 Segmentation map이 얼마나 겹치는지 평가&lt;/li&gt;
      &lt;li&gt;Weakness: Hard to make the human annotations, Such localization and segmentation labels are not a ground truth of interpretation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Pixel Perturbation 즉, 픽셀을 교란하여 모델의 출력값의 변화를 테스트
    &lt;ul&gt;
      &lt;li&gt;성이 있는지 분류하는 모델이 있을 때, 성 부분을 가려서 모델을 통과시키면 그 값이 크게 떨어질 것임. 만약 성이 아닌 다른 부분을 가리게 된다면, 분류 스코어 값은 크게 변동이 없을 것임&lt;/li&gt;
      &lt;li&gt;AOPC (Area Over the MoRF Perturbation Curve): 주어진 이미지에 대해 각 XAI 기법이 설명을 제공하면, 그 설명의 중요도 순서대로 각 픽셀을 정렬할 수 있고, 그 순서대로 픽셀을 교란했을 때, 원래 예측한 분류 스코어 값이 얼마나 빨리 바뀌는지를 측정하는 것&lt;/li&gt;
      &lt;li&gt;Insertion: 중요한 순서대로, 백지 상태의 이미지에서 중요한 픽셀의 순서대로 하나씩 추가해가면서 분류기의 출력 스코어가 어떻게 변화하는지 확인하는 것. 따라서 이 값이 클수록 좋은 결과&lt;/li&gt;
      &lt;li&gt;Deletion: AOPC 방법과 같이 XAI 기법이 제공한 중요도 순서대로 픽셀을 하나씩 지워가며, 분류 확률 값이 떨어지는 확인하는 것으로, AOPC와는 반대로 커브의 아래 쪽의 면적을 구함. 따라서 이 값이 낮을수록 좋은 결과&lt;/li&gt;
      &lt;li&gt;Weakness: 데이터를 지우거나, 추가하는 과정이 머신러닝의 주요한 가정에 위반하는 경우가 있음. 즉, 어떤 픽셀을 지우고, 모델의 입력으로 넣었을 때, 해당 이미지는 모델을 학습시킨 Training 이미지들의 분포와 다르기 때문에 정확하지 않다는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;ROAR (RemOve And Retrain)
    &lt;ul&gt;
      &lt;li&gt;XAI 기법이 발견한 중요한 픽셀을 지우고 나서, 지운 데이터를 통해 모델을 재학습하고, 정확도가 얼마나 떨어지는지 평가하는 방법 (재학습한 후에 나오는 모델의 성능이 떨어지는 경우에는 좋은 설명 방법이라는 것)&lt;/li&gt;
      &lt;li&gt;앞선 방법들에 비해 조금 더 객관적이고, 정확한 평가를 할 수 있지만, 계산 복잡도가 크다는 단점이 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;xai-방법의-신뢰성에-관한-연구&quot;&gt;XAI 방법의 신뢰성에 관한 연구&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Sanity Checks
    &lt;ul&gt;
      &lt;li&gt;Model Randomization
        &lt;ul&gt;
          &lt;li&gt;Model Randomization Test&lt;/li&gt;
          &lt;li&gt;분류 모델에 위쪽 Layer부터 모델의 계수들을 순차적으로 Randomized 한 후에 얻어지는 설명들을 구함&lt;/li&gt;
          &lt;li&gt;계속 Randomized 되었으므로, 시간이 지날수록 성능이 떨어지지 않은 모델들은 학습이 잘못됨을 추론&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Adversarial Attack
        &lt;ul&gt;
          &lt;li&gt;어떤 입력 이미지에 대한 픽셀을 아주 약간 바꿨을 떄, 분류기의 예측 결과를 완전히 다르게 만든다는 것&lt;/li&gt;
          &lt;li&gt;많은 설명 방법들이 Gradient와 연관되는 값을 사용하는데, Decision boundary가 불연속적으로 나오게 된다면, Gradient의 방향이 급격하게 변할 수 있기 때문에 조금만 입력이 바뀌어도 Gradient가 아주 많이 바뀔 수가 있다는 것&lt;/li&gt;
          &lt;li&gt;이에 조금 더 강건하게 반응하기 위해 ReLU 대신, Softplus를 사용하라&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Adversarial Model Manipulation
        &lt;ul&gt;
          &lt;li&gt;모델이 편향되었다는 것을 알았을 때, 해당 모델을 다시 고쳐서 재학습 시키는 것이 아니라, 모델 계수를 조금씩 조작하여 모델의 정확도는 차이가 없지만 XAI의 결과가 공정한 것처럼 보일 수도 있다는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Thu, 19 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/LG-Aimers-%EC%84%A4%EB%AA%85%EA%B0%80%EB%8A%A5%ED%95%9C-AI/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/LG-Aimers-%EC%84%A4%EB%AA%85%EA%B0%80%EB%8A%A5%ED%95%9C-AI/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Education</category>
        
        <category>LG Aimers 2기</category>
        
      </item>
    
      <item>
        <title>LG Aimers 2기 지도학습 - 딥러닝 (KAIST 주재걸 교수님)</title>
        <description>&lt;p&gt;이번 글에서는 LG Aimers의 AI 전문가 과정에서 딥러닝에 대한 기본 개념과 대표적인 모형의 학습 원리를 학습합니다. 특히, 이미지와 언어 모델 학습을 위한 딥러닝 모델과 학습 원리를 배우게 될 것입니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-introduction-to-dnn&quot;&gt;1. Introduction to DNN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Artificial Intelligence &amp;gt; Machine Learning &amp;gt; Deep learning&lt;/li&gt;
  &lt;li&gt;하나하나의 뉴런들이 모여서, 하나의 신경망을 구성 (여러 Layers를 가지고 있음) = DNN&lt;/li&gt;
  &lt;li&gt;DNN 적용을 위해서는 빅데이터, 컴퓨팅 성능, 진보된 알고리즘 모델이 필요함
    &lt;ul&gt;
      &lt;li&gt;영상 인식, 이미지 합성, 기계 번역, 챗봇, 자연어 처리, 주식 가격 예측 등에 적용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;퍼셉트론 (Perceptron): y = f(w0 + w1x1 + w2x2)
    &lt;ul&gt;
      &lt;li&gt;AND, OR, XOR Gate&lt;/li&gt;
      &lt;li&gt;XOR Gate는 Single layer 퍼셉트론으로는 결과값을 낼 수 없음&lt;/li&gt;
      &lt;li&gt;Input layer - Hidden layer - Output layer (2-layer neural network)&lt;/li&gt;
      &lt;li&gt;Input layer - Hidden layer1 - Hidden layer2 - Output layer (3-layer neural network)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Forward Propagation, Activation Function (Sigmoid, Softmax), Loss Function (MSE, Cross entropy loss)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-training-nn&quot;&gt;2. Training NN&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Gradient Descent: Loss Function을 최소화 시킬 수 있는 최적의 파라미터 값을 업데이트 하는 과정
    &lt;ul&gt;
      &lt;li&gt;Loss function이 복잡한 경우에는, 수렴 속도가 늦어지는 경우도 존재&lt;/li&gt;
      &lt;li&gt;따라서 Original Gradient Descent 알고리즘을 다양한 형태로 변화시킴 (Momentum, Adagrad, Adam 등)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;NN의 학습 과정
    &lt;ol&gt;
      &lt;li&gt;가장 처음으로는, 학습 데이터에 대해 Random Initialize 된 값으로 Forward Propagation을 수행&lt;/li&gt;
      &lt;li&gt;그 상태에서 Loss Function의 값을 최소화 시킬 수 있는 파라미터를 찾아나감 (Backward Propagation)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Gradient Vanishing을 해결하기 위해 Tanh, ReLU 활성화 함수 등을 사용하게 됨&lt;/li&gt;
  &lt;li&gt;Batch Normalization (배치 정규화)
    &lt;ul&gt;
      &lt;li&gt;각 배치 단위 별로 데이터가 다양한 분포를 가지더라도, 배치 별로 평균과 분산을 이용해 정규화 해주는 것&lt;/li&gt;
      &lt;li&gt;활성화 함수의 출력값을 정규화 하여 그 분포를 고르게 해주는 효과&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-cnn-and-이미지-분류&quot;&gt;3. CNN and 이미지 분류&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Fully Connected NN, Convolution NN (CNN - 이미지 처리에 사용), Recurrent NN (RNN - 시계열 데이터에 사용)&lt;/li&gt;
  &lt;li&gt;CNN은 특정 클래스에 존재할 수 있는 작은 특정 패턴들을 정의하고, 패턴들이 주어진 이미지에 있는지를 판단&lt;/li&gt;
  &lt;li&gt;매칭의 정도를 나타내는 결과값을 활성화 지도 (Activation map)이라고 하고, 이는 특정 Convolution Filter를 주어진 입력 이미지에 가능한 모든 위치에 오버랩을 시켜서 매칭되는 정도를 나타낸 것&lt;/li&gt;
  &lt;li&gt;Channel, Filter, Pooling Layer (Max, Average)
    &lt;ul&gt;
      &lt;li&gt;일반적인 구조: Conv - ReLU - Conv - ReLU - Pooling - Conv - ReLU - Pooling - FC&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hyperparameters
    &lt;ul&gt;
      &lt;li&gt;Convolution: Number of filters, Size of filters&lt;/li&gt;
      &lt;li&gt;Pooling: Window size, Window stride&lt;/li&gt;
      &lt;li&gt;Fully Connected: Number of layers, Number of neurons&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNN 모델은 어느 종류의 layer를 어느 위치에, 어느 순서로 배치하는가가 중요한 문제. 이에 대하여 많은 연구자들이 CNN 모델에 대한 architecture를 사전에 정의한 것들이 있음 = AlexNet, VGGNet, GoogleNet, ResNet
    &lt;ul&gt;
      &lt;li&gt;VGGNet: 각각의 Conv layer에서 사용하는 필터의 가로, 세로 size를 3 by 3으로 고정하고, layer를 깊게 쌓아서 문제를 해결한 알고리즘&lt;/li&gt;
      &lt;li&gt;ResNet (Residual Network): Conv layer를 통해 나온 output에 layer를 또 추가하는 것. layer를 추가적으로 쌓아서 생기는 비효율성에 대하여 일부 layer는 skip 할 수 있도록 하는 skip connection을 추가한 알고리즘&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNN 알고리즘의 꾸준한 성능 개선과 함께, 모델의 layer 개수도 꾸준히 증가 됨 (AlexNet은 8개, ResNet은 152개 층)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-seq2seq-with-attention&quot;&gt;4. Seq2Seq with Attention&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Recurrent Neural Network (RNN)
    &lt;ul&gt;
      &lt;li&gt;one to one, one to many, many to one, many to many&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RNN 모델의 기울기 소실 및 폭발의 문제를 해결하기 위한 LSTM, GRU 모델 등장&lt;/li&gt;
  &lt;li&gt;Seq2Seq, Encoder &amp;amp; Decoder
    &lt;ul&gt;
      &lt;li&gt;Original Seq2Seq 모델은 매 time step 마다 생성되는 RNN의 각 time step의 Hidden state vector는 같은 dimension으로 이뤄져야 한다는 제약 조건이 있음. Output vector가 다시 Input vector로 사용되기 위해. 이러한 경우에 축적해야 되는 정보가 시간이 길어짐에 따라서 점점 많아지지만, 정보는 항상 똑같은 개수의 dimension에 저장하게 되어 정보를 유실하게 된다는 한계점이 있음&lt;/li&gt;
      &lt;li&gt;이러한 Bottleneck 문제를 해결하고자 attention 이라는 추가적인 모델이 Seq2Seq에 도입&lt;/li&gt;
      &lt;li&gt;입력 sequence에 주어지는 데이터를 encoder에서 인코딩 한 후, decoder에서는 encoder의 마지막 time step의 Hidden state vector 만을 입력으로 받지 않고, 그 입력과 더불어 decoder의 각 time step에서 encoder에서 나온 여러 인코딩 데이터 중 필요로 하는 것을 가지고 가서 예측에 사용&lt;/li&gt;
      &lt;li&gt;기본적 구조는 encoder와 decoder가 존재하고, encoder의 마지막 time step의 Hidden state vector가 decoder의 가장 최초의 Hidden state vector인 h0로 사용됨. 그런데 여기서 어떤 decoder의 각 time step에서 추가적으로 encoder에 있는 여러 Hidden state vector로부터 필요한 정보를 취사선택해서 가장 유관하다고 생각하는 정보를 추가적인 입력으로 사용함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-transformer&quot;&gt;5. Transformer&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Transformer 모델은 Seq2Seq with attention 모델의 개선된 버전
    &lt;ul&gt;
      &lt;li&gt;Solving long-term dependency problem&lt;/li&gt;
      &lt;li&gt;Seq2Seq with attention 모델에서 Encoder와 Decoder가 RNN 기반의 모델로 구성됨&lt;/li&gt;
      &lt;li&gt;Transformer 모델은 Encoder와 Decoder에서도 attention 기반으로 동작하는 모델
        &lt;ul&gt;
          &lt;li&gt;RNN 기반의 모델들은 Long-term Dependency 문제가 있음 (오랜 시계열을 거쳐 정보가 소실 될 수 있음)&lt;/li&gt;
          &lt;li&gt;Transformer에서는 가까이 있거나 멀리 있는 정보를 접근해서 필요한 정보를 사용할 수 있게 됨&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Self-attention, Multi-head Attention&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Layer Normalization은 각 단어에서 발견된 특정한 dimension으로 이루어진 벡터의 각 원소에 평균과 분산을 계산
    &lt;ul&gt;
      &lt;li&gt;그리고 각 단어 내에서 발생된 벡터 각각의 원소 값의 평균과 분산이 0과 1이 되도록 정규화&lt;/li&gt;
      &lt;li&gt;dimension 혹은 layer 내의 각 노드 별로 학습된 trainable parameters로 affine transformation 수행&lt;/li&gt;
      &lt;li&gt;이는 Batch Normalization과 비슷하게 학습을 조금 더 안정화시키고, 성능을 개선시킬 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Positional Encoding은 각 단어의 입력 벡터에 몇 번째 순서에서 나타났다는 알려 줄 수 있는 정보
    &lt;ul&gt;
      &lt;li&gt;즉, 각 단어의 순서나 위치를 구분할 수 있게 되는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transformer 모델은 기존에 RNN 및 Convolution 기반의 Sequence를 인코딩하는 모델보다 좋은 성능을 보임
    &lt;ul&gt;
      &lt;li&gt;이렇게 가능하게 된 이유는 Long term dependency를 근본적으로 해결했기 때문&lt;/li&gt;
      &lt;li&gt;자연어처리 외에도 다양한 도메인에 적용되고 있음&lt;/li&gt;
      &lt;li&gt;또한 Transformer에서 제한된 Layer 수를 더 많이 늘리되, Block 자체의 설계나 디자인은 그대로 계승하고, Model의 사이즈를 점차 늘리면서 Self-supervised 방법론을 추가로 사용하여 대규모 데이터를 학습한 사전 학습 모델을 활용할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-self-supervised-learning&quot;&gt;6. Self-supervised Learning&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Self-supervised Learning (자가지도학습)은 데이터의 Labeling 과정 없이도 Raw data 만으로 모델을 학습
    &lt;ul&gt;
      &lt;li&gt;즉, Raw 데이터나, 별도의 추가적인 Label 없이 입력 데이터만으로 입력 데이터 중에 일부를 가려놓고, 가려진 입력 데이터를 주었을 때 가려진 부분을 잘 복원 혹은 예측하도록 하여, 주어진 입력 데이터의 일부를 예측하도록 모델을 학습 (Computer Vision 분야의 Inpainting Task, Zigsaw Puzzle Task 등)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Transfer Learning의 기본 아이디어는 자가지도학습을 통해 만들어진 (Inpainting) 모델의 앞쪽 Layer는 물체를 잘 인식하기 위해 필요로 하는 유의미한 패턴을 추출할 수 있도록 학습됨. 그리고 뒤쪽의 Layer는 실제 풀고자 하는 문제를 해결할 수 있도록 특화되어 학습이 될 것. 따라서 대규모 데이터로 학습된 모델에 풀고자 하는 문제를 해결할 수 있는 Layer를 뒤에 덧붙여서 모델을 재학습&lt;/li&gt;
  &lt;li&gt;BERT (Pre-training of Deep Bidirectional Transformers for Language Understanding)
    &lt;ul&gt;
      &lt;li&gt;BERT 모델은 Transformer의 Encoder 역할을 수행&lt;/li&gt;
      &lt;li&gt;입력 문장을 BERT 모델의 입력 Sequence로 제공하되, 입력 데이터의 일부를 가리고 그것을 예측하도록 함&lt;/li&gt;
      &lt;li&gt;두 개의 문장을 주고, 연속되게 등장하여 두 문장 사이에 어떤 의미 관계가 있는지 판단 (Next Sentence 예측)&lt;/li&gt;
      &lt;li&gt;CLS, MASK, SEP 토큰 추가&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Masked Language Model (MLM)
    &lt;ul&gt;
      &lt;li&gt;주어진 입력 문장에 대해, 특정 확률에 따라 각 단어를 Masked token으로 대체할지 전처리 수행&lt;/li&gt;
      &lt;li&gt;약 15% 비율의 단어를 Mask 단어로 대체하되, 거기서 80%는 Mask 단어, 10% 랜덤한 단어, 10%는 그대로 유지&lt;/li&gt;
      &lt;li&gt;Too little masking: Too expensive to train&lt;/li&gt;
      &lt;li&gt;Too much masking: Not enough to capture the given context&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Next Sentence Prediction (NSP)
    &lt;ul&gt;
      &lt;li&gt;두 개의 문장을 SEP token으로 구분하여 제공하고, CLS token으로부터 인코딩 된 Hidden state vector의 Binary classification 결과를 예측&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Further Details of BERT
    &lt;ul&gt;
      &lt;li&gt;Model Architecture
        &lt;ul&gt;
          &lt;li&gt;BERT BASE: L = 12, H = 768, A = 12&lt;/li&gt;
          &lt;li&gt;BERT LARGE: L = 24, H = 1024, A = 16&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Input Representation
        &lt;ul&gt;
          &lt;li&gt;WordPiece Embedding (30,000 WordPiece)&lt;/li&gt;
          &lt;li&gt;Learned positional embedding&lt;/li&gt;
          &lt;li&gt;CLS (Classification embedding)&lt;/li&gt;
          &lt;li&gt;Packed sentence embedding (SEP)&lt;/li&gt;
          &lt;li&gt;Segment Embedding&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Pre-training Tasks
        &lt;ul&gt;
          &lt;li&gt;Masked LM&lt;/li&gt;
          &lt;li&gt;Next sentence prediction&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GPT (Generative Pre-Trained Transformer)
    &lt;ul&gt;
      &lt;li&gt;GPT 모델은 Transformer의 Decoder 역할을 수행&lt;/li&gt;
      &lt;li&gt;주어진 텍스트 데이터들에 대해 문장을 가져와서 특정 문장의 일부만 주어졌을 때, 다음에 나타날 단어를 예측. 그리고 그 다음 단어가 주어졌을 때, 그 다음 단어를 예측하는 Word level의 Language modeling task를 학습&lt;/li&gt;
      &lt;li&gt;Zero-shot Summarization으로도 활용 가능
        &lt;ul&gt;
          &lt;li&gt;일반적으로 Summarization을 수행하기 위해서는 사전 학습된 모델을 가져와서 target task인 Summarization을 목적으로 입력 지문과 정답 요약 문장인 Labeled 학습 데이터를 가지고 주어진 모델을 Fine-tuning 하는데, GPT는 그런 과정 없이 Summarization을 수행할 수 있어서 Zero-shot이라고 함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;GPT3는 BERT와 GPT2와 다르게, 훨씬 더 많은 Layer 수를 가지고, 학습에 필요로 하는 1,750억 개의 파라미터
        &lt;ul&gt;
          &lt;li&gt;Zero-shot, One-shot, Few-shot Learning&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 18 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/LG-Aimers-%EB%94%A5%EB%9F%AC%EB%8B%9D/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/LG-Aimers-%EB%94%A5%EB%9F%AC%EB%8B%9D/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Education</category>
        
        <category>LG Aimers 2기</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 16주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 16주차 강의에 대한 정리입니다. &lt;br /&gt;
이 강의에서는 추천 시스템에 대하여 다루게 됩니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-recommendation-system-이란&quot;&gt;1. Recommendation System 이란?&lt;/h2&gt;

&lt;h3 id=&quot;추천-엔진의-정의&quot;&gt;추천 엔진의 정의&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;사용자: 서비스를 사용하는 사람&lt;/li&gt;
  &lt;li&gt;아이템: 서비스에서 판매하는 물품 (다른 사용자가 물품이 될 수도 있음 - 링크드인 등)&lt;/li&gt;
  &lt;li&gt;일반적으로 서비스가 성장하면, 사용자나 아이템의 수도 같이 성장하게 됨
    &lt;ul&gt;
      &lt;li&gt;특히, 사용자의 성장도가 훨씬 커짐&lt;/li&gt;
      &lt;li&gt;하지만 아이템의 수가 커지면서 아이템의 선택에 대한 이슈가 생김&lt;/li&gt;
      &lt;li&gt;모든 사용자가 능동적으로 검색하지 않고, 사람들이 추천에 대한 니즈가 생김&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Twitter의 알파 제인의 정의: 사용자가 관심 있어 할 만한 아이템을 제공해주는 자동화 된 시스템 (관심, 자동화)&lt;/li&gt;
  &lt;li&gt;Yahoo의 디팍 아그라왈의 정의: 비즈니스 장기적 목표를 개선하기 위해 사용자에게 알맞은 아이템을 자동으로 보여주는 시스템 (장기적 목표, 매출액, 자동화)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;추천-엔진이-필요한-이유&quot;&gt;추천 엔진이 필요한 이유&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;조금의 노력으로 사용자가 관심 있어 할 만한 아이템을 찾아주는 방법
    &lt;ul&gt;
      &lt;li&gt;아이템의 수가 굉장히 큰 경우 더 의미가 있음&lt;/li&gt;
      &lt;li&gt;사람의 노다가로 해결할 수 없어 자동화가 필요함&lt;/li&gt;
      &lt;li&gt;개인화 (Personalization)로 연결 될 수 있음&lt;/li&gt;
      &lt;li&gt;또한 가끔씩 전혀 관심 없을 듯한 아이템도 추천 가능 (Serendipity)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;회사 관점에서는 추천 엔진을 기반으로 다양한 기능을 추가할 수 있음
    &lt;ul&gt;
      &lt;li&gt;마케팅 시, 추천 엔진 사용 (이메일 마케팅)&lt;/li&gt;
      &lt;li&gt;관련 상품 추천으로 쉽게 확장 할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;아이템의 수가 많아서 원하는 것을 찾기 어려운 경우 (검색의 수고를 덜어줌)&lt;/li&gt;
  &lt;li&gt;추천을 통해 신상품 등의 마케팅이 가능해짐 (추천을 통해 신상품 노출이 가능)&lt;/li&gt;
  &lt;li&gt;인기 아이템 뿐 아니라, 롱 테일의 다양한 아이템을 노출 할 수 있음 (개인화가 알고리즘화 될 수 있음)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;추천은-결국-매칭-문제&quot;&gt;추천은 결국 매칭 문제&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;사용자에게는 맞는 아이템을 매칭해주기 (아이템은 서비스에 따라 달라지고, 다른 사용자가 될 수도 있음)&lt;/li&gt;
  &lt;li&gt;어떤 아이템을 추천할 것인가?
    &lt;ul&gt;
      &lt;li&gt;지금 뜨는 아이템 추천 (개인화 되어 있지 않은 추천)&lt;/li&gt;
      &lt;li&gt;사용자가 마지막에 클릭했던 아이템 추천&lt;/li&gt;
      &lt;li&gt;사용자가 구매했던 아이템을 구매한 다른 사용자가 구매한 아이템 추천 (협업 필터링)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;추천 UI도 굉장히 중요 (보통 추천 유닛이 존재하고, 이를 어떤 순서로 노출시킬지?)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Cold Start 문제&lt;/strong&gt;: 사용자의 데이터가 없는 상태에서 새로운 추천을 어떻게 해야 할까?
    &lt;ul&gt;
      &lt;li&gt;따라서 사용자와 아이템 등에 대한 부가 정보들이 필요해짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;아이템 부가 정보: 먼저 분류 체계를 만들어야 함, 태그 형태로 부가 정보를 유지하는 것도 좋음
    &lt;ul&gt;
      &lt;li&gt;계층 구조의 분류 체계 (대분류, 소분류, 태그 및 키워드 등)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;사용자 프로파일 정보: 개인정보 (성별, 연령), 아이템 정보 (관심 및 서브 카테고리, 태그, 클릭, 구매 아이템)&lt;/li&gt;
  &lt;li&gt;무엇을 기준으로 추천 할 것인가? 클릭? 매출? 소비? 평점?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;추천-엔진-예제&quot;&gt;추천 엔진 예제&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;아마존 관련 상품 (Related Product) 추천 - 사용자: 멤버, 아이템: 상품
    &lt;ul&gt;
      &lt;li&gt;과거 구매 이력, 인기 있는 상품들, 주기적으로 구매 할 수 있는 상품들 등이 추천되고 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;넷플릭스 영화 및 드라마 추천 - 사용자: 멤버, 아이템: 영화, 드라마
    &lt;ul&gt;
      &lt;li&gt;격자 형태의 추천 유닛 (최근 뜨고 있는 것 기반, 과거 시청 이력 기반, 인기 Top 10 기반 추천)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;구글 자동 검색어 완성 - 사용자: 검색자, 아이템: 검색어
    &lt;ul&gt;
      &lt;li&gt;타이핑을 전체 완성하지 않아도 됨, 알지 못했던 새로운 키워드로 접근 가능하다는 등의 효과&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;링크드인 혹은 페이스북 친구 추천 - 사용자: 멤버, 아이템: 멤버
    &lt;ul&gt;
      &lt;li&gt;Industry, 지역, 학교 등과 관련 있는 사람들 추천&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;스포티파이 혹은 판도라 노래, 플레이리스트 추천 - 사용자: 멤버, 아이템: 노래, 플레이리스트&lt;/li&gt;
  &lt;li&gt;헬스케어 도메인의 위험 점수 계산 - 사용자: 의사, 간호사, 아이템: 환자
    &lt;ul&gt;
      &lt;li&gt;어느 환자가 더 위험한지 예측하여 치료시 우선순위를 주기 위함&lt;/li&gt;
      &lt;li&gt;환자 별로 발병 확률과 발병시 임팩트를 계산하여 곱하는 형태 (발병 확률을 모델링)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;유데미 강좌 추천 - 사용자: 멤버, 아이템: 강좌&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;앞서-살펴본-추천-엔진들의-공통점&quot;&gt;앞서 살펴본 추천 엔진들의 공통점&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;격자 형태의 UI 사용 (넷플릭스가 선구자)&lt;/li&gt;
  &lt;li&gt;다양한 종류의 추천 유닛들이 존재함
    &lt;ul&gt;
      &lt;li&gt;일부 유닛은 개인화 (사람에 따라 다른 아이템을 추천해줌)&lt;/li&gt;
      &lt;li&gt;일부 유닛은 인기도 등의 비개인화 정보 기반 (모든 사람에게 동일한 아이템 추천)&lt;/li&gt;
      &lt;li&gt;추천 유닛의 랭킹이 중요해짐
        &lt;ul&gt;
          &lt;li&gt;이 부분도 모델링 하여 개인화 하는 추세&lt;/li&gt;
          &lt;li&gt;클릭을 최적화 하고, 이 데이터 수집을 위한 실험을 수행 (데이터 수집을 위한 온라인 테스트)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;추천-엔진의-종류&quot;&gt;추천 엔진의 종류&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;컨텐츠 기반 (아이템 기반)
    &lt;ul&gt;
      &lt;li&gt;개인화 된 추천은 아니고, 비슷한 아이템을 기반으로 추천이 이뤄짐
        &lt;ul&gt;
          &lt;li&gt;책이라면, 타이틀, 저자, 책 요약, 장르 등의 정보를 사용&lt;/li&gt;
          &lt;li&gt;많은 경우, NLP 테크닉을 사용하여 텍스트 정보를 벡터 정보로 변환 (단어 카운트, TF-IDF, 임베딩)&lt;/li&gt;
          &lt;li&gt;구현이 상대적으로 간단 (보통 아이템의 수가 사용자의 수 보다 작음)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;협업 필터링 (Collaborative Filtering): 평점 기준
    &lt;ul&gt;
      &lt;li&gt;기본적으로 다른 사용자들의 정보를 이용하여 내 취향을 예측하는 방식
        &lt;ul&gt;
          &lt;li&gt;사용자 기반, 아이템 기반 두 종류가 존재&lt;/li&gt;
          &lt;li&gt;결국은 행렬 계산으로 이뤄짐 (Sparse 행렬 형태)&lt;/li&gt;
          &lt;li&gt;유사도 계산 (코사인 유사도 등)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;사용자 기반 (User): 나와 비슷한 평점 패턴을 보이는 사람을 찾아서 그 사람들의 평이 좋았던 것을 추천
        &lt;ul&gt;
          &lt;li&gt;나와 비슷한 사용자를 어떻게 찾을지가 중요 (사용자 프로파일 정보 구축, 프로파일간 유사도 계산 - KNN)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;아이템 기반 (Item): 평점의 패턴이 비슷한 아이템들을 찾아서 그것을 추천하는 방식
        &lt;ul&gt;
          &lt;li&gt;2001년에 아마존에서 논문으로 발표. 아이템들 간 유사도를 비교하는 것으로 시작&lt;/li&gt;
          &lt;li&gt;사용자 기반 협업 필터링과 비교하여 더 안정적이며, 좋은 성능을 보임&lt;/li&gt;
          &lt;li&gt;아이템의 수가 보통 작기 때문에 사용자에 비해 평점의 수가 평균적으로 많고, 계산량이 적음&lt;/li&gt;
          &lt;li&gt;즉, 사용자 기반 추천에 비해, 데이터에 대한 고객들의 평점 등에 대한 데이터가 많아서 성능이 좋음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;사용자 행동 기반
    &lt;ul&gt;
      &lt;li&gt;아이템 클릭, 구매, 소비 등의 정보를 기반으로 하는 추천
        &lt;ul&gt;
          &lt;li&gt;사용자와 아이템에 대한 부가 정보가 반드시 필요함&lt;/li&gt;
          &lt;li&gt;여기에 속하는 추천은 구현이 간단하긴 하지만, 아주 유용함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;모델링을 통해 사용자와 아이템 페어에 대한 클릭 확률 등의 점수 계산이 가능
        &lt;ul&gt;
          &lt;li&gt;의사 결정 트리나 딥러닝 등이 사용 가능 (유데미에서 채택한 방법)&lt;/li&gt;
          &lt;li&gt;Batch 기반 추천, 실시간 추천인지 방식 등도 결정해야 함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;위 알고리즘들을 하이브리드 형태로 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;유사도-측정-방법&quot;&gt;유사도 측정 방법&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;두 개의 비교 대상을 N차원 좌표로 표현. 사용자와 사용자 혹은 아이템과 아이템&lt;/li&gt;
  &lt;li&gt;보통 코사인 유사도나 피어슨 상관계수 유사도를 사용하게 됨
    &lt;ol&gt;
      &lt;li&gt;두 벡터의 방향성이 비슷할수록 1에 가까운 값이 계산되는 코사인 유사도 (동일할 경우 1이 됨)&lt;/li&gt;
      &lt;li&gt;두 벡터가 반대 방향을 향하는 경우에는 -1이 계산&lt;/li&gt;
      &lt;li&gt;피어슨 유사도는 코사인 유사도의 개선 버전으로 각 벡터를 중앙 (중심)으로 재조정&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;협업-필터링에-대한-문제&quot;&gt;협업 필터링에 대한 문제&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Cold start 문제
    &lt;ul&gt;
      &lt;li&gt;사용자: 아직 평점을 준 아이템이 없는 경우&lt;/li&gt;
      &lt;li&gt;아이템: 아직 평점을 준 사용자가 없는 경우&lt;/li&gt;
      &lt;li&gt;보통 컨텐츠 기반 혹은 사용자 행동 기반 추천과 병행하여 이 문제를 해결해나감&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;리뷰 정보의 부족 (Sparsity) - 리뷰를 했다는 자체도 사실은 관심으로 볼 수 있음&lt;/li&gt;
  &lt;li&gt;업데이트 시점 - 사용자나 아이템이 추가 될때마다 다시 계산해야 함&lt;/li&gt;
  &lt;li&gt;확장성 이슈 - 사용자와 아이템의 수가 늘어나면서 행렬 계산에 시간이 오래 걸림 (Spark 같은 것이 필요해지는 이유)&lt;/li&gt;
  &lt;li&gt;협업 필터링의 많은 문제들이 추천의 일반적인 문제이기도 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;협업-필터링-구현-방법&quot;&gt;협업 필터링 구현 방법&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;메모리 기반
    &lt;ul&gt;
      &lt;li&gt;앞서 설명한 방식 (사용자 기반, 아이템 기반)&lt;/li&gt;
      &lt;li&gt;사용자간 혹은 아이템간 유사도를 미리 계산&lt;/li&gt;
      &lt;li&gt;추천 요청이 오면, 유사한 사용자 혹은 아이템을 K개 뽑아서 이를 바탕으로 아이템 추천&lt;/li&gt;
      &lt;li&gt;구현과 이해가 상대적으로 쉽지만, 스케일하지 않음 (평점 데이터의 부족)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;모델 기반
    &lt;ul&gt;
      &lt;li&gt;넷플릭스 프라이즈 컨테스트 때 고안된 추천 방식 (아이템 행렬에서 비어 있는 평점을 SGD를 사용해서 채움)&lt;/li&gt;
      &lt;li&gt;이는 보통 SVD (Singular Vector Decompostion)을 사용해서 구현 (딥러닝의 오토인코더를 사용하기도 함)&lt;/li&gt;
      &lt;li&gt;평점을 포함한 다른 사용자 행동을 예측하는 방식으로 진화하고 있음
        &lt;ul&gt;
          &lt;li&gt;주로 암시적 정보 (클릭, 구매, 소비)를 기반으로 행동을 예측하게 됨&lt;/li&gt;
          &lt;li&gt;아이템 노출 - 아이템 클릭 - 아이템 구매 - 아이템 소비 (인프라 단에서 해당 데이터들 수집이 필요함)&lt;/li&gt;
          &lt;li&gt;사용자 행동 기반 간단한 추천 유닛 구성
            &lt;ul&gt;
              &lt;li&gt;사용자가 관심을 보인 특정 카테고리의 새로운 아이템, 인기 아이템 등&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;사용자 행동을 예측하는 추천 (클릭 혹은 구매)
            &lt;ul&gt;
              &lt;li&gt;지도 학습 문제로 접근 가능. 무엇을 학습하고, 예측하는 모델인지 먼저 생각해야 함
                &lt;ul&gt;
                  &lt;li&gt;어떤 기준으로 추천을 할까? = 머신러닝의 레이블 정보&lt;/li&gt;
                  &lt;li&gt;명시적 힌트: 리뷰 점수 (Rating), 좋아요 (Like)&lt;/li&gt;
                  &lt;li&gt;암시적 힌트: 클릭, 구매, 소비 등&lt;/li&gt;
                &lt;/ul&gt;
              &lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-recommendation-system-구현-1&quot;&gt;2. Recommendation System 구현 1&lt;/h2&gt;

&lt;h3 id=&quot;넷플릭스-프라이즈-개요&quot;&gt;넷플릭스 프라이즈 개요&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;2006년부터 3년간 운영된 넷플릭스의 기념비적인 추천 엔진 경진대회&lt;/li&gt;
  &lt;li&gt;넷플릭스 추천 시스템 품질을 10% 개선하는 팀에서 $1M 수여 약속 (RMSE가 평가 기준으로 사용)&lt;/li&gt;
  &lt;li&gt;프라이버시 이슈도 제기 되었긴 했지만, 넷플릭스 브랜드 인지도도 올라감&lt;/li&gt;
  &lt;li&gt;이를 기폭제로 캐글과 같은 머신러닝 경진대회 플랫폼이 등장&lt;/li&gt;
  &lt;li&gt;이 대회를 통해서 협업 필터링이 한 단계 발전하게 되었음
    &lt;ul&gt;
      &lt;li&gt;SVD를 활용한 SVD++는 이후 굉장히 많은 분야에서 활용됨&lt;/li&gt;
      &lt;li&gt;앙상블 방식의 모델들이 가장 좋은 성능을 보이게 됨 (하지만 실행시간이 너무 길어서, 실제로는 사용 불가)
        &lt;ul&gt;
          &lt;li&gt;앙상블과 랜덤포레스트: 다수의 분류기를 사용해서 예측하는 방식&lt;/li&gt;
          &lt;li&gt;성능이 좋긴 하지만, 훈련과 예측 시간이 오래 걸린다는 단점이 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;다양한 알고리즘들이 논문으로 학회에서 발표됨 (SVD++ 포함)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;추천-엔진의-발전-역사&quot;&gt;추천 엔진의 발전 역사&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;2001년 아마존이 아이템 기반 협업 필터링 논문 발표&lt;/li&gt;
  &lt;li&gt;2006년 ~ 2009년 넷플릭스 프라이즈
    &lt;ul&gt;
      &lt;li&gt;SVD를 이용한 사용자의 아이템 평점 예측 알고리즘 탄생&lt;/li&gt;
      &lt;li&gt;앙상블 알고리즘의 보편화&lt;/li&gt;
      &lt;li&gt;딥러닝의 일종이라고 할 수 있는 RBM (Restricted Boltzman Machine)이 단일 모델로 최고 성능을 보임&lt;/li&gt;
      &lt;li&gt;딥러닝이 추천의 분야에서 사용 가능성을 보이게 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;2010년 딥러닝이 컨텐츠 기반 음악 추천에 사용되기 시작&lt;/li&gt;
  &lt;li&gt;2016년 딥러닝을 기반으로 한 추천이 활기를 띠기 시작
    &lt;ul&gt;
      &lt;li&gt;오토인코더 기반으로 복잡한 행렬 계산을 단순화 하는 방식이 하나&lt;/li&gt;
      &lt;li&gt;아이템 관련 사용자 정보를 시간 순으로 인코드 하는 RNN을 사용하는 방식이 다른 방식&lt;/li&gt;
      &lt;li&gt;아마존에서 DSSNTE 라는 알고리즘을 오픈소스화 했다가 나중에 SageMaker 라는 제품으로 통합&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;유데미-추천-살펴보기&quot;&gt;유데미 추천 살펴보기&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;문제 정의: 학생들에게 관심 있을만한 강의를 먼저 보여주는 것&lt;/li&gt;
  &lt;li&gt;추천 UI - 격자 기반 UI, 다양한 추천 유닛들이 존재 (유닛 선택과 랭킹이 필요함)&lt;/li&gt;
  &lt;li&gt;온라인 강의 메타 데이터 - 분류 체계, 태그, 클릭 키워드 분석 등&lt;/li&gt;
  &lt;li&gt;다양한 행동 기반 추천 - 클릭, 구매, 소비 등&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;인기도-기반-추천-유닛-개발&quot;&gt;인기도 기반 추천 유닛 개발&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;인기도 기반 추천은 Cold start 이슈가 존재하지 않음&lt;/li&gt;
  &lt;li&gt;그렇다면, 인기도의 기준은? 평점? 매출? 최대 판매?&lt;/li&gt;
  &lt;li&gt;사용자 정보에 따라 확장 가능 (서울 지역 인기 아이템 추천 등)&lt;/li&gt;
  &lt;li&gt;단, 개인화는 되어 있지 않음 (어느 정도는 가능함)&lt;/li&gt;
  &lt;li&gt;아이템의 분류 체계 정보 존재 여부에 따라 쉽게 확장 가능 (특정 카테고리에서의 인기 아이템 추천)&lt;/li&gt;
  &lt;li&gt;인기도를 다른 기준으로 바꿔서 다양한 추천 유닛 생성 가능 (Top course, Newest course 등)&lt;/li&gt;
  &lt;li&gt;Cold start 이슈가 없는 추천 유닛 (현재 사용자들이 구매한 아이템, 사용자들이 보고 있는 아이템 등)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;유사도-측정-코사인-및-피어슨-유사도&quot;&gt;유사도 측정 (코사인 및 피어슨 유사도)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;벡터들 사이에 유사도를 판단&lt;/li&gt;
  &lt;li&gt;코사인 유사도: N차원 공간에 있는 두 개의 벡터 간의 각도 (원점에서)를 보고서 유사도를 판단하는 기준&lt;/li&gt;
  &lt;li&gt;평점처럼 방향 뿐 아니라, 벡터 크기의 정규화도 중요하면, 피어슨 유사도를 사용하게 됨 (코사인 유사도의 개선판)
    &lt;ul&gt;
      &lt;li&gt;먼저 벡터 A와 B의 값들을 보정&lt;/li&gt;
      &lt;li&gt;각 벡터 내 셀들의 평균값들을 구한 뒤, 평균값을 각 셀에서 빼줌&lt;/li&gt;
      &lt;li&gt;예를 들면, A = {3, 4, 5} 라면, 평균값은 4이고, 보정 후에는 {-1, 0, 1}이 됨&lt;/li&gt;
      &lt;li&gt;그 이후 계산은 코사인 유사도와 동일함. 이를 중앙 코사인 유사도 혹은 보정된 코사인 유사도라고 부름&lt;/li&gt;
      &lt;li&gt;이를 통해 모든 벡터가 원점을 중심으로 이동하고, 벡터 간 비교가 더 쉬워짐 (정규화 효과)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;tf-idf-소개와-실습&quot;&gt;TF-IDF 소개와 실습&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;텍스트를 행렬 (벡터)로 표현하는 방법&lt;/li&gt;
  &lt;li&gt;텍스트 문서를 행렬로 표현하는 방법은 여러 가지가 존재함
    &lt;ul&gt;
      &lt;li&gt;기본적으로 일단 단어를 행렬의 차원으로 표현해야 함&lt;/li&gt;
      &lt;li&gt;Bag of Words 방식은 문서들에 나타나는 단어 수가 N개이면, N차원으로 문서를 표현
        &lt;ul&gt;
          &lt;li&gt;딥러닝의 워드임베딩 사용시, 차원 수도 축소되고, 공간 상에서 비슷한 단어끼리 가깝게 위치&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;One Hot Encoding + Bag of Words (카운트) - 단어의 수를 카운트해서 표현함
    &lt;ul&gt;
      &lt;li&gt;가장 먼저는 Stopwords를 제거함 (the, is, in, we, can, see 등)&lt;/li&gt;
      &lt;li&gt;그 뒤 단어의 수를 계산함 (sky, blue, sun, bright, shining 5개)&lt;/li&gt;
      &lt;li&gt;단어 별로 차원을 배정 (sky = 1, blue = 2, sun = 3, bright = 4, shining = 5)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;One Hot Encoding + Bag of Words (TF-IDF) - 단어의 값을 TF-IDF 알고리즘으로 계산된 값으로 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;CountVectorizer 소개
    &lt;ul&gt;
      &lt;li&gt;앞서 Bag of Words 카운팅 방식을 구현한 모듈&lt;/li&gt;
      &lt;li&gt;벡터로 표현이 되면, 문서들 간의 유사도 측정이 가능함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;TF-IDF 소개
    &lt;ul&gt;
      &lt;li&gt;앞서 카운트 방식은 자주 나오는 단어가 높은 가중치를 가지게 됨&lt;/li&gt;
      &lt;li&gt;하지만 TF-IDF는 한 문서에서 중요한 단어를 카운트가 아닌 문서군 전체를 보고 판단함&lt;/li&gt;
      &lt;li&gt;어떤 단어가 한 문서에서 자주 나오면 중요하지만, 이 단어가 다른 문서들에서 자주 나오지 않으면 더 중요할 것&lt;/li&gt;
      &lt;li&gt;TF-IDF = TF(t, d) * IDF(t)
        &lt;ul&gt;
          &lt;li&gt;TF(t, d): 단어 t가 문서 d에서 몇 번 나왔는가?&lt;/li&gt;
          &lt;li&gt;DF(t): 단어 t가 전체 문서군에서 몇 번 나왔는가?&lt;/li&gt;
          &lt;li&gt;IDF(t): 앞서 DF(t)의 역비율
            &lt;ul&gt;
              &lt;li&gt;단어 t가 전체 문서들 중에서 몇 개의 문서에서 나왔는지? 이 비율을 역으로 계산한 것이 IDF&lt;/li&gt;
              &lt;li&gt;In(N/DF): N은 총 문서 수를 나타내고, DF는 단어가 나온 문서를 뜻함&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;### CountVectorizer 코드화
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CountVectorizer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'The sky is blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                            &lt;span class=&quot;c1&quot;&gt;# sky, blue
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;'The sun is bright'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                          &lt;span class=&quot;c1&quot;&gt;# sun, bright
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;'The sun in the sky is bright'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;# sun, sky, bright
&lt;/span&gt;    &lt;span class=&quot;s&quot;&gt;'We can see the shining run, the bright sun'&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# shining, sun, bright
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;countvectorizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CountVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'word'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'english'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;count_wm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;countvectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;sky&lt;/td&gt;
      &lt;td&gt;blue&lt;/td&gt;
      &lt;td&gt;sun&lt;/td&gt;
      &lt;td&gt;bright&lt;/td&gt;
      &lt;td&gt;shining&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc3&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;### TFidfVectorizer 코드화
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.feature_extraction.text&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TfidfVectorizer&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'The sky is blue'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                            
    &lt;span class=&quot;s&quot;&gt;'The sun is bright'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;                          
    &lt;span class=&quot;s&quot;&gt;'The sun in the sky is bright'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;               
    &lt;span class=&quot;s&quot;&gt;'We can see the shining run, the bright sun'&lt;/span&gt;  
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tfidfvectorizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TfidfVectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'word'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stop_words&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'english'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'l2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tfidf_wm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tfidfvectorizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit_transform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Cosine 유사도 계산
&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cosine_similarities&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tfidf_wm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt;sky&lt;/td&gt;
      &lt;td&gt;blue&lt;/td&gt;
      &lt;td&gt;sun&lt;/td&gt;
      &lt;td&gt;bright&lt;/td&gt;
      &lt;td&gt;shining&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc1&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/2) = 0.6191&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/1) = 0.7852&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/3) = 0.7071&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/3) = 0.7071&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc3&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/2) = 0.6578&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/3) = 0.5325&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/3) = 0.5325&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doc4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2, 2*log(4/3) = 0.7325&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/3) = 0.3662&lt;/td&gt;
      &lt;td&gt;1, 1*log(4/1) = 0.5738&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h3 id=&quot;tf-idf-문제점&quot;&gt;TF-IDF 문제점&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;정확히 동일한 단어가 나와야 유사도 계산이 이뤄짐 (동의어 처리가 안됨)&lt;/li&gt;
  &lt;li&gt;단어의 수가 늘어나고, 아이템의 수가 늘어나면 계산이 오래 걸림&lt;/li&gt;
  &lt;li&gt;결국 워드 임베딩을 사용하는 것이 더 좋음 (아니면, LSA 같은 차원 축소 방식을 사용해야 함)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-recommendation-system-구현-2&quot;&gt;3. Recommendation System 구현 2&lt;/h2&gt;

&lt;h2 id=&quot;4-딥러닝-기반-recommendation-system-구현-1&quot;&gt;4. 딥러닝 기반 Recommendation System 구현 1&lt;/h2&gt;

&lt;h2 id=&quot;5-딥러닝-기반-recommendation-system-구현-2&quot;&gt;5. 딥러닝 기반 Recommendation System 구현 2&lt;/h2&gt;
</description>
        <pubDate>Mon, 16 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-16%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-16%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>LG Aimers 2기 지도학습 - 분류, 회귀 (이화여대 강제원 교수님)</title>
        <description>&lt;p&gt;이번 글에서는 LG Aimers의 AI 전문가 과정에서 머신러닝의 한 부류인 지도학습에 대한 기본 개념과 분류 및 회귀의 목적과 차이점에 대해서 학습합니다. 또한 다양한 모델과 방법론을 통해서 언제 어떤 모델을 사용해야 하는지, 왜 사용 해야 하는지, 모델 성능을 향상 시키는 방법 등에 대해 학습합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-sl-foundation&quot;&gt;1. SL Foundation&lt;/h2&gt;

&lt;h3 id=&quot;machine-learning-problems&quot;&gt;Machine Learning Problems&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;지도학습 (Labeled data): Regression, Classification
    &lt;ul&gt;
      &lt;li&gt;데이터 X를 이용하여 정답인 Y로 가는 함수 h를 학습하는 것&lt;/li&gt;
      &lt;li&gt;새로운 데이터가 들어올 때도 동작 할 수 있도록 하는 함수를 찾아내는 것&lt;/li&gt;
      &lt;li&gt;지도학습은 모델의 Output과 실제 값의 Error를 줄여가면서 학습이 진행 (Training)&lt;/li&gt;
      &lt;li&gt;학습 단계에서 보지 못했던 새로운 데이터를 통해서 모델의 성능을 확인 (Testing)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;비지도학습 (Unlabeled data): Clustering, Dimensional Reduction&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;learning-model&quot;&gt;Learning Model&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;Goal: Target function (f: x -&amp;gt; y)&lt;/li&gt;
  &lt;li&gt;Training data&lt;/li&gt;
  &lt;li&gt;Learning model (Feature selection, Model selection, Optimization)&lt;/li&gt;
  &lt;li&gt;Hypothesis, Evaluation&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;model-generalization&quot;&gt;Model Generalization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;학습 과정에서 데이터가 제한 됨에 따라, 성능 역시 제한 될 수 밖에 없음&lt;/li&gt;
  &lt;li&gt;따라서 모델의 궁극적 목적은 어느 데이터가 들어와도 동작할 수 있는 &lt;strong&gt;일반화&lt;/strong&gt;가 필요함
    &lt;ul&gt;
      &lt;li&gt;Generalization Error (Training Error, Validation Error, Test Error)&lt;/li&gt;
      &lt;li&gt;Overall Error (Loss Function, Cost Function)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Test Error를 Training Error로 가까이 가도록 한다면?
    &lt;ul&gt;
      &lt;li&gt;실패하는 경우에는 Overfitting (High Variance) -&amp;gt; 정규화 등을 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Training Error가 0에 가까이 가도록 한다면?
    &lt;ul&gt;
      &lt;li&gt;실패하는 경우에는 Underfitting (High Bias) -&amp;gt; 조금 더 복잡한 모델 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Variance와 Bias의 Trade-off (Overfitting vs Underfitting)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;avoid-overfitting&quot;&gt;Avoid Overfitting&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data Augmentation, Ensemble&lt;/li&gt;
  &lt;li&gt;Regularization to penalize complex models (Lasso 등)&lt;/li&gt;
  &lt;li&gt;Cross-validation (CV, K-fold) - Train, Valid, Test Dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-linear-regression&quot;&gt;2. Linear Regression&lt;/h2&gt;

&lt;h3 id=&quot;linear-models&quot;&gt;Linear Models&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Hypothesis set H&lt;/li&gt;
  &lt;li&gt;많은 장점이 있음 (단순함, 해석 가능성, 일반화)&lt;/li&gt;
  &lt;li&gt;주어진 입력에 대해 출력과 선형적 관계를 추론 (단변량, 다변량 문제)&lt;/li&gt;
  &lt;li&gt;선형 모델은 파라미터 (x절편 및 y절편 등)가 달라짐에 따라 데이터 fitting 과정에서 오차가 발생&lt;/li&gt;
  &lt;li&gt;손실 함수가 가장 작게 되도록 하는 모델 파라미터를 찾는 것이 목표 (파라미터 최적화)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터의 양이 증가할수록 벡터의 차원의 수가 증가함에 따라 복잡도가 늘어나게 됨&lt;/li&gt;
  &lt;li&gt;Needs Iterative Algorithm (경사하강법)
    &lt;ul&gt;
      &lt;li&gt;Iterative 하게 최적의 파라미터를 찾아가는 과정&lt;/li&gt;
      &lt;li&gt;Gradient는 함수를 미분하여 얻는 값으로 해당 함수의 변화하는 정도를 확인할 수 있음&lt;/li&gt;
      &lt;li&gt;경사하강법에서는 Gradient가 최솟값이 되도록 반복적으로 파라미터를 변화 시킴&lt;/li&gt;
      &lt;li&gt;적절한 Learning Rate가 필요함 (하이퍼파라미터: 직접 사람이 지정해야 하는 값)&lt;/li&gt;
      &lt;li&gt;Global Optima, Local Optima - 최적화 된 포인트 찾아가기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-gradient-descent&quot;&gt;3. Gradient Descent&lt;/h2&gt;

&lt;h3 id=&quot;learning-rate&quot;&gt;Learning Rate&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Learning Rate에 따른 수렴 속도의 변화&lt;/li&gt;
  &lt;li&gt;Batch Gradient Descent 알고리즘은 전체 샘플 m개를 모두 고려해야 하는 단점이 존재
    &lt;ul&gt;
      &lt;li&gt;데이터가 증가하면 증가할수록 복잡도가 커짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Stochastic Gradient Descent 알고리즘은 Batch 경사하강법의 문제점을 해결한 방법
    &lt;ul&gt;
      &lt;li&gt;샘플의 개수를 1개로 줄여서 파라미터를 지속적으로 개선시키는 알고리즘&lt;/li&gt;
      &lt;li&gt;Batch 경사하강법에 비해 빠르게 Iteration을 돌 수 있다는 장점&lt;/li&gt;
      &lt;li&gt;하지만 각 샘플 하나씩 계산하기 때문에 노이즈의 영향이 크다는 단점&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Local Minimum을 피할 수 있는 방법
    &lt;ul&gt;
      &lt;li&gt;Momentum: 과거 Gradient가 업데이트 되었던 방향 및 속도를 반영하여 현재 포인트에서 Gradient가 0이 되더라도 계속해서 학습을 진행할 수 있는 동력을 제공하게 되는 것&lt;/li&gt;
      &lt;li&gt;Nesterov Momentum: Look ahead gradient step&lt;/li&gt;
      &lt;li&gt;AdaGrad: Adapts an individual learning rate of each direction&lt;/li&gt;
      &lt;li&gt;RMSProp: AdaGrad의 단점 (Learning rate가 작아지는 부분에서 학습 X)을 보완한 알고리즘&lt;/li&gt;
      &lt;li&gt;Adam (Adaptive Moment Estimation): RMSProp + Momentum 방식&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;모델의-과적합-문제&quot;&gt;모델의 과적합 문제&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모델이 너무 복잡한 경우, 학습 데이터에서만 좋은 결과를 보이는 Overfitting 된 모델이 나오게 됨&lt;/li&gt;
  &lt;li&gt;Overfitting을 피할 수 있는 방법: Features의 개수를 줄이기, Regularization (정규화)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-linear-classification&quot;&gt;4. Linear Classification&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Binary Classification: decision-boundary에 따라 class를 구분하기 (Logistic 모델)
    &lt;ul&gt;
      &lt;li&gt;Zero-One Loss function&lt;/li&gt;
      &lt;li&gt;Hinge Loss function&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Cross-entropy Loss function&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiclass Classification: One-VS-All&lt;/li&gt;
  &lt;li&gt;Linear 모델의 장점: 간단함, 해석 가능성이 높음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-advanced-classification&quot;&gt;5. Advanced Classification&lt;/h2&gt;

&lt;h3 id=&quot;svm-support-vector-machine-모델&quot;&gt;SVM (Support Vector Machine) 모델&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모델의 boundary 사이에서 존재하는 가장 가까운 margin을 갖는 Support Vector를 설정
    &lt;ul&gt;
      &lt;li&gt;Support Vector는 실제 테스트 시, 성능을 결정할 수 있는 중요한 (민감한) 데이터&lt;/li&gt;
      &lt;li&gt;따라서 Support Vector의 margin을 최대화 할 수 있는 모델을 구축하는 것이 목표&lt;/li&gt;
      &lt;li&gt;그렇기 때문에 Outliers에 대해서도 robust 하게 동작이 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimization 방법: Hard margin SVM, Soft margin SVM, Nonlinear transform&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;ann-artificial-neural-network-모델&quot;&gt;ANN (Artificial Neural Network) 모델&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Non-linear classification model&lt;/li&gt;
  &lt;li&gt;Activation functions: Sigmoid, ReLU, Leaky ReLU 등&lt;/li&gt;
  &lt;li&gt;ANN 모델의 Layer를 더 많이 쌓게 되면, Deep Neural Network (DNN)이 됨
    &lt;ul&gt;
      &lt;li&gt;DNN을 통해서 더 비선형적인 모델의 결과를 도출 할 수 있게 됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Gradient Vanishing, Backpropagation, CNN&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-ensemble&quot;&gt;6. Ensemble&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;앙상블 방식: 머신러닝에서 알고리즘의 종류에 상관 없이 서로 다르거나, 같은 매커니즘으로 동작하는 다양한 머신러닝 모델을 묶어서 함께 사용하는 방식 (예측 모델의 집합을 합쳐서 새로운 모델 만들기)&lt;/li&gt;
  &lt;li&gt;앙상블 모델의 장점
    &lt;ul&gt;
      &lt;li&gt;예측 성능을 안정적으로 향상 시킬 수 있다는 것&lt;/li&gt;
      &lt;li&gt;비교적 간단하게 사용 할 수 있다는 것&lt;/li&gt;
      &lt;li&gt;모델의 파라미터 튜닝이 많이 필요하지 않다는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Bagging (Bootstrapping + Aggregating): 학습 과정에서 Training samples을 랜덤하게 나눠서 선택하여 학습
    &lt;ul&gt;
      &lt;li&gt;Bootstrapping: 다수의 Sample data set을 생성하여 학습하는 방식&lt;/li&gt;
      &lt;li&gt;Aggregating: Committee prediction&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Boosting: Sequential 하게 동작하는 앙상블 모델 (Weak classifier의 Cascading)
    &lt;ul&gt;
      &lt;li&gt;Weak classifier: Bias가 높은 Classifier (비교적 성능이 낮은 모델)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Random Forest, Adaboost, GBM (Gradient Boosting Machine)&lt;/li&gt;
  &lt;li&gt;Performance Evaluation: Accuracy, Confusion Matrix (TN, FN, FP, TP), Precision, Recall, ROC Curve&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 13 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/LG-Aimers-%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/LG-Aimers-%EC%A7%80%EB%8F%84%ED%95%99%EC%8A%B5/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Education</category>
        
        <category>LG Aimers 2기</category>
        
      </item>
    
      <item>
        <title>LG Aimers 2기 품질성과 신뢰성 (한양대학교 배석주 교수님)</title>
        <description>&lt;p&gt;이번 글에서는 LG Aimers의 AI 전문가 과정에서 품질과 신뢰성에 대한 강의를 기록합니다. 한양대학교 배석주 교수님께서 강의해주시고, 이 강의를 통해 데이터 이해를 위한 기본적 소양을 기를 수 있습니다. 또한 품질의 의의와 각종 통계적 방법과 품질경영정보시스템의 방법론을 이해하며, 최적화를 위한 수학적 모델 및 분석 방법을 학습 할 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-품질-및-품질-비용&quot;&gt;1. 품질 및 품질 비용&lt;/h2&gt;
&lt;h3 id=&quot;품질-quality이란&quot;&gt;품질 (Quality)이란?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;전통적 품질 관리에서의 품질: 규격에 부합하는 것&lt;/li&gt;
  &lt;li&gt;품질을 보는 5개 관점
    &lt;ul&gt;
      &lt;li&gt;선험적 관점: 품질을 정의 할 수는 없더라도, 무엇인지 고객이 인지&lt;/li&gt;
      &lt;li&gt;제품 관점: 바람직한 성분이나 속성의 함량 차이가 곧 품질의 차이&lt;/li&gt;
      &lt;li&gt;사용자 관점: 용도 적합성&lt;/li&gt;
      &lt;li&gt;제조 관점: 요구사항에 부합되는 정도&lt;/li&gt;
      &lt;li&gt;가치 관점: 품질은 실제 용도와 판매가격의 최적 상태&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;품질의 구성 요소
    &lt;ul&gt;
      &lt;li&gt;제품특징: 시장 점유율의 확대나 보다 높은 가격을 통해 주로 &lt;strong&gt;판매 수익의 증대에 기여&lt;/strong&gt;하는 요소&lt;/li&gt;
      &lt;li&gt;무결함: 재작업, 폐기 처분, 고객 불만 등의 감소를 통한 &lt;strong&gt;원가 절감에 기여&lt;/strong&gt;하는 요소&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;품질의 유형
    &lt;ul&gt;
      &lt;li&gt;요구 품질 (Requirement of Quality): 제품 또는 서비스를 사용하는 사람의 입장에서 요구하는 품질&lt;/li&gt;
      &lt;li&gt;설계 품질 (Quality of Design): 기업의 제조 역량을 고려해 추상적 요구 품질을 명문화 (구체화)한 품질&lt;/li&gt;
      &lt;li&gt;제조 품질, 적합 품질 (Quality of Manufacturing or Confomance): 원자재 품질, 설비 능력, 공정 능력 등 제조 시스템의 다양한 원천에서 발생하는 변동성과 불확실성에 의해 제조 품질이 결정&lt;/li&gt;
      &lt;li&gt;사용 품질, 시장 품질 (Quality of Use of Market): 고객이 제품 또는 서비스를 실제 사용한 후, 그 제품으로부터 기본적 욕구의 충족, 애프터 서비스, 보전, 신뢰성 등에 대한 만족감 또는 불만을 인식함으로써 결정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;품질의 8가지 차원: 성능, 특징, 신뢰성, 적합성, 내구성, 서비스성, 심미성, 인지품질&lt;/li&gt;
  &lt;li&gt;종합적 품질 (Total Quality): 제품과 서비스가 아무리 훌륭해도, 고객에게 수용되지 않으면 의미 없음. 즉, 고객 지향적인 품질의 정의가 필요하고 중요함. 이를 위해 제조 시스템의 가치 사슬을 고려해 볼 필요가 있음&lt;/li&gt;
  &lt;li&gt;저품질 비용 (COPQ, Cost of Poor Quality): 기업 내에서 불필요하게 발생하는 이익손실비용을 측정하는 재무적 지표
    &lt;ul&gt;
      &lt;li&gt;저품질로 인해 드러난 실패 비용은 빙산의 일각&lt;/li&gt;
      &lt;li&gt;회계상 파악 가능한 손실 - 매출의 4~6% (전통적 실패 비용은 정의가 쉬움)&lt;/li&gt;
      &lt;li&gt;회계상 파악 불가능한 손실 - 매출의 25~30% (숨어 있는 실패 비용은 측정이 어렵고 정의가 곤란)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-통계적-공정-관리-spc&quot;&gt;2. 통계적 공정 관리 (SPC)&lt;/h2&gt;
&lt;h3 id=&quot;spc-필요성과-개념&quot;&gt;SPC 필요성과 개념&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Feed-Back 공정관리 시스템 (ERP, MEC)&lt;/li&gt;
  &lt;li&gt;SPC는 공정에서 요구되는 품질, 생산성 목표를 달성하도록 통계적 방법으로 공정을 효율적으로 운영하는 관리 방법
    &lt;ul&gt;
      &lt;li&gt;Statistical: 통계적 자료와 분석 기법을 이용하여&lt;/li&gt;
      &lt;li&gt;Process: 공정의 품질 변동을 주는 원인과 공정의 능력 상태를 파악하여&lt;/li&gt;
      &lt;li&gt;Control: 주어진 품질 목표가 달성되도록 끊임 없이 품질 개선이 이뤄지도록 관리하는 활동&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SPC의 장점과 단점
    &lt;ul&gt;
      &lt;li&gt;장점: 결함 방지에 효과적, 불필요한 공정 조정 방지, 계량치 및 계수치 데이터 모두에 사용 가능&lt;/li&gt;
      &lt;li&gt;단점: 데이터의 정확한 수집 및 관리가 필요, 관리도에 대한 올바른 분석과 패턴에 대한 조치 필요&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SPC의 적: 품질 변동
    &lt;ul&gt;
      &lt;li&gt;우연 원인 (Chance Cause): 엄격히 관리된 상태 하에서도 어느 정보 품질 변동을 발생시키는 원인&lt;/li&gt;
      &lt;li&gt;이상 원인 (Assignable Cause): 산발적으로 발생하여 품질 변동을 발생시키는 원인&lt;/li&gt;
      &lt;li&gt;SPC에서 사용되는 통계적 기법: 평균, 분산 및 확률분포, 관리도 및 공정능력 지수, QC 7가지 기본 도구
        &lt;ul&gt;
          &lt;li&gt;QC 7가지 도구란, 적은 데이터로부터 가능한 한 신뢰성이 높은 객관적 정보를 얻는데 가장 유효한 수단
            &lt;ul&gt;
              &lt;li&gt;특성 요인도, 파레트도, 체크 시트, 산점도, 히스토그램, 층별, 관리도 (그래프)&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-스마트-품질-경영&quot;&gt;3. 스마트 품질 경영&lt;/h2&gt;
&lt;h3 id=&quot;품질-40과-스마트-품질-경영&quot;&gt;품질 4.0과 스마트 품질 경영&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;전사적 품질 관리 (Total Quality Management)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;품질 4.0&lt;/strong&gt;은 기존 품질 관리 기법과 IoT, 빅데이터 등이 결합된 신개념 품질관리 및 경영 개념
    &lt;ul&gt;
      &lt;li&gt;품질 경영 시스템의 Digital Transformation&lt;/li&gt;
      &lt;li&gt;설명적 애널리틱스: 데이터로 과거에 무엇이 발생했는지를 분석하기 위한 기법 (상관관계 모니터링)&lt;/li&gt;
      &lt;li&gt;진단적 애널리틱스: 과거에 축적된 데이터로 인과관계를 찾아내어 특정 품질 관련 이벤트가 발생했는지 분석&lt;/li&gt;
      &lt;li&gt;예측적 애널리틱스: 통계학적 모델을 통해 미래에 어떤 사건이 발생할 확률로 예측하는 기법&lt;/li&gt;
      &lt;li&gt;처방적, 규범적 애널리틱스: 예측되는 이벤트를 위해 무엇을 하면 좋을지 처방하는 의사결정 관련 기법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;스마트 공장은 환경을 고려하고, 안전성을 확보하며 역동적 시장 변화에 대응하는 지능형 디지털 시스템&lt;/li&gt;
  &lt;li&gt;품질 관리 문화는 기업 내 여러 부서 간의 협업과 대화를 통해 합의를 이뤄내는 것이 중요함&lt;/li&gt;
  &lt;li&gt;빅데이터를 활용한 스마트 품질 경영
    &lt;ul&gt;
      &lt;li&gt;실시간 커뮤니티 피드백을 제공하는 방안&lt;/li&gt;
      &lt;li&gt;원격 진단 및 유지 보수&lt;/li&gt;
      &lt;li&gt;고도화 된 공급망 품질 관리&lt;/li&gt;
      &lt;li&gt;예시
        &lt;ul&gt;
          &lt;li&gt;공정 모니터링 시스템의 품질 예측 및 불량 요인 분석 알고리즘 개발&lt;/li&gt;
          &lt;li&gt;공정 변수와 품질 계측치의 상관관계를 파악할 수 있는 지표 도출 (군집분석)&lt;/li&gt;
          &lt;li&gt;공정 변수를 통해 품질 계측치를 예측할 수 있는 가상 계측 시스템 구축 (회귀분석)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-신뢰성-개념과-중요성&quot;&gt;4. 신뢰성 개념과 중요성&lt;/h2&gt;
&lt;h3 id=&quot;신뢰성의-중요성&quot;&gt;신뢰성의 중요성&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;제품 라이프 사이클 관점의 Total Cost 관리가 필요 (품질비용은 잠재적 Risk)
    &lt;ul&gt;
      &lt;li&gt;개발 단계에서 시장 품질은 예측 가능하고, Control 될 수 있어야 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신뢰성: 주어진 작동 환경에서 주어진 시간동안 시스템이 고유의 기능을 수행할 확률
    &lt;ul&gt;
      &lt;li&gt;품질: 정적 (현시점에서 제품의 특성), 전사적 추진 및 주로 생산 단계, 불량률&lt;/li&gt;
      &lt;li&gt;신뢰성: 동적 (미래의 성능과 고장), 전문분야 기술자로 구성된 팀에서 추진 및 설계 및 개발 단계, 고장률&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신뢰성 분석의 필요성 (취약한 설계, 과부하, 강도와 부하의 산포, 마모, 시간 매커니즘, 잠재된 오작동, 오류)&lt;/li&gt;
  &lt;li&gt;신뢰성 척도
    &lt;ul&gt;
      &lt;li&gt;신뢰도: 부품, 제품, 시스템 등이 주어진 사용 조건에서 일정 기간 요구되는 기능을 고장 없이 수행할 확률&lt;/li&gt;
      &lt;li&gt;순간 고장률: 어떤 시점까지 동작하고 있는 시스템이 계속되는 단위 시간 동안 고장을 일으킬 비율&lt;/li&gt;
      &lt;li&gt;평균 고장률: 총 동작 시간 동안의 고장 개수&lt;/li&gt;
      &lt;li&gt;평균 고장 시간: 수리불가시스템에서 고장이 발생하기까지의 평균 시간&lt;/li&gt;
      &lt;li&gt;평균 고장 간격: 수리가능시스템에서 고장 간격 간의 평균 동작 시간&lt;/li&gt;
      &lt;li&gt;보전도: 고장난 시스템이 주어진 조건 하에서 규정된 시간 내에 수리 (보전)을 완료할 확률&lt;/li&gt;
      &lt;li&gt;가용도: 수리 가능한 시스템이 어떤 특정 시점에 기능을 유지하고 있을 확률&lt;/li&gt;
      &lt;li&gt;고장률, 욕조곡선, 평균고장시간, 평균고장간격시간, 평균잔여수명, 백분위 수명&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신뢰성 데이터
    &lt;ul&gt;
      &lt;li&gt;수명 데이터: 의도된 기능을 제대로 수행하고 있거나, 고장인지의 여부로 판정 (Binary Data)&lt;/li&gt;
      &lt;li&gt;선능 데이터: 시간 경과에 따른 제품의 성능을 측정 (Continous Data)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-신뢰성-분포와-신뢰성-척도&quot;&gt;5. 신뢰성 분포와 신뢰성 척도&lt;/h2&gt;
&lt;h3 id=&quot;연속형-수명-분포와-신뢰성-척도&quot;&gt;연속형 수명 분포와 신뢰성 척도&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;지수분포: 확률밀도함수 및 누적분포함수, 고장률 함수, 평균 수명, 메디안 수명&lt;/li&gt;
  &lt;li&gt;지수분포 무기억성: 이산형에서 기하분포가 무기억성을 갖는 것처럼 연속형 분포에서는 지수분포가 무기억성을 가짐&lt;/li&gt;
  &lt;li&gt;지수분포의 일반화 형태는 감마분포
    &lt;ul&gt;
      &lt;li&gt;감마분포: 포아송 프로세스를 따르는 사건이 k건 발생하는데 걸린 시간의 확률 분포&lt;/li&gt;
      &lt;li&gt;포아송분포: 시간 Lambda 동안 포아송 프로세스를 따르는 사건의 개수에 대한 확률 분포&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;정규분포, 표준정규분포, 대수정규분포&lt;/li&gt;
  &lt;li&gt;베르누이와 이항분포, 포아송분포&lt;/li&gt;
  &lt;li&gt;신뢰성 데이터 표현 (수명분포), 고장 개수 표현 (이항분포, 포아송분포)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-ict-기반-예지-보전&quot;&gt;6. ICT 기반 예지 보전&lt;/h2&gt;
&lt;h3 id=&quot;시스템-열화&quot;&gt;시스템 열화&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;시스템은 사용시간, 빈도의 증가에 따라 열화&lt;/li&gt;
  &lt;li&gt;특정 운용 조건에서 요구된 기능을 수행하지 못하는 경우, 시스템 고장 발생&lt;/li&gt;
  &lt;li&gt;일반적으로 고장 후 교체 비용이 예방 보전 비용보다 훨씬 높음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;보전&quot;&gt;보전&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;보전의 목적: 안전하고 경제적으로 운전 될 수 있는 조건으로 장비 유지
    &lt;ul&gt;
      &lt;li&gt;비용 검토와 관련된 문제 인식&lt;/li&gt;
      &lt;li&gt;Safety Risk&lt;/li&gt;
      &lt;li&gt;휴지기간비용 및 수리비용 고려&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;보전도 (Maintainablilty): 예방보전 (상태기반, 시간기준), 사후보전 (보전연기, 즉시보전)
    &lt;ul&gt;
      &lt;li&gt;사후보전: 점검 및 정기 교환을 전혀 하지 않고, 장비 고장 후 수리
        &lt;ul&gt;
          &lt;li&gt;장점: 장비 수명이 다할 때까지 사용하므로, 2차 고장이 없다면 보전비 및 수리비가 모두 저렴&lt;/li&gt;
          &lt;li&gt;단점: 고장이 늘어나고, 생산공정에 미치는 영향이 크면 수율 및 생산 능력이 저하&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;시간기준보전: 장비의 열화에 가장 비례하는 파라미터로서 수리 주기를 정하고, 주기까지 사용시 무조건 수리
        &lt;ul&gt;
          &lt;li&gt;장점: 점검 등의 보전 공수가 적고, 고장도 적음&lt;/li&gt;
          &lt;li&gt;단점: Over Maintenance가 되어 수리비가 많이 들 수 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;상태기반보전: 장비 열화 상태를 온오프라인으로 파악하며, 열화를 나타내는 값이 미리 정한 기준에 달하면 수리
        &lt;ul&gt;
          &lt;li&gt;장점: TBM의 단점인 과잉 유지 관리를 방지&lt;/li&gt;
          &lt;li&gt;단점: 감시 체계 설치에 대한 비융이 들며, TBM에 비해 보전 인력이 더 필요할 수도 있음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;주요 보전 활동 개념 비교: BM - CM - PM - PdM&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 10 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/LG-Aimers-%ED%92%88%EC%A7%88%EC%84%B1%EA%B3%BC-%EC%8B%A0%EB%A2%B0%EC%84%B1/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/LG-Aimers-%ED%92%88%EC%A7%88%EC%84%B1%EA%B3%BC-%EC%8B%A0%EB%A2%B0%EC%84%B1/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Education</category>
        
        <category>LG Aimers 2기</category>
        
      </item>
    
      <item>
        <title>LG Aimers 2기 AI 윤리 (KAIST 차미영 교수님)</title>
        <description>&lt;p&gt;이번 글에서는 LG Aimers의 AI 전문가 과정에서 AI 윤리에 대한 강의를 기록합니다. KAIST 차미영 교수님께서 강의해주시고, 데이터 과학자로서 기본 소양과 어떤 자세를 가져야 하는지를 고민할 수 있습니다. 더불어 인공지능 기술로 어떻게 문제를 해결할 수 있을지에 대해 학습합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-데이터-분석과-ai-학습에서-유의할-점&quot;&gt;1. 데이터 분석과 AI 학습에서 유의할 점&lt;/h2&gt;

&lt;h3 id=&quot;데이터-처리-및-수집에서-윤리-이슈&quot;&gt;데이터 처리 및 수집에서 윤리 이슈&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터를 잘 해석하고 있는가?
    &lt;ul&gt;
      &lt;li&gt;초콜렛을 많이 먹으면 노벨상을 탄다? -&amp;gt; &lt;a href=&quot;https://www.nejm.org/doi/full/10.1056/nejmon1211064&quot;&gt;논문 링크&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;상관관계와 인과관계는 다름
        &lt;ul&gt;
          &lt;li&gt;상관관계 (Correlation): 일정한 수치로 계산되어 두 대상이 서로 관련성이 있다고 추측되는 관계&lt;/li&gt;
          &lt;li&gt;인과관계 (Causality): 일반적으로 어떤 사실과 다른 사실 사이의 원인과 결과 관계&lt;/li&gt;
          &lt;li&gt;키와 체중 사이에는 일정한 정도의 상관관계가 존재. 키가 큰 사람이 어느 정도 체중이 많이 나가는 경향이 있기 때문. 하지만 키가 크다고 반드시 체중이 많이 나가거나, 체중이 많다고 꼭 키가 크지 않기 때문에 둘 사이에 인과관계가 있다고 이야기 하기는 어려움&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 전처리와 분석 방법은 적절한가?
    &lt;ul&gt;
      &lt;li&gt;Error bar 추가하기 (데이터의 편차를 표시해주기)&lt;/li&gt;
      &lt;li&gt;적합한 통계 테스트 찾기&lt;/li&gt;
      &lt;li&gt;아웃라이어 제거하기&lt;/li&gt;
      &lt;li&gt;데이터 표준화하기&lt;/li&gt;
      &lt;li&gt;EDA (Exploratory Data Analysis) 충분한 시간 할애하기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;학습에 쓰는 데이터가 충분한가?
    &lt;ul&gt;
      &lt;li&gt;Under-fitting, Over-fitting 피하기&lt;/li&gt;
      &lt;li&gt;Appropirate-fitting으로 데이터 학습 결과가 적절한지 이해할 수 있어야 함&lt;/li&gt;
      &lt;li&gt;학습 데이터는 테스트 데이터와 달라야 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;블랙박스 알고리즘 (Black Box Algorithms)
    &lt;ul&gt;
      &lt;li&gt;설명력이 중요한 AI 예시 (탈세범 검출 - 위장 반입, 원산지 조작 등 세관에서 벌어지는 불법 행위 적발 AI)&lt;/li&gt;
      &lt;li&gt;실제 사례에서는 성능 뿐 아니라, &lt;strong&gt;설명력&lt;/strong&gt; 역시도 매우 중요한 부분&lt;/li&gt;
      &lt;li&gt;AI 모델의 결정에 설명력 더하기
        &lt;ul&gt;
          &lt;li&gt;High risk 결정에서는 설명력도 정확도 만큼이나 중요&lt;/li&gt;
          &lt;li&gt;Saliency map, SHAP와 같이 post-hoc explainability (사후 설명력)를 제공하는 기술&lt;/li&gt;
          &lt;li&gt;알고리즘의 내면을 가시화해서 보여주는 기술들의 등장&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;학습 결과가 바뀔 수 있는 위험성
        &lt;ul&gt;
          &lt;li&gt;One pixel attack의 예시에서는 픽셀 하나만 바뀔 경우에 알고리즘 학습 결과가 달라지는 문제점&lt;/li&gt;
          &lt;li&gt;AI 모델들이 노이즈에 굉장히 민감하게 반응하고 있음을 이해할 수 있는 예시&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Handling the Web Data
    &lt;ul&gt;
      &lt;li&gt;수집하는 SNS, 인터넷, 블로그 등의 글이 대중들의 의견을 대표할 수 있는 &lt;strong&gt;대표성&lt;/strong&gt;이 있는가?&lt;/li&gt;
      &lt;li&gt;의견의 대표성 (Spiral of silence) -&amp;gt; 편향 현상
        &lt;ul&gt;
          &lt;li&gt;인터넷 상의 의견이 대표성 있는 의견이 아닐 수도 있음을 인지&lt;/li&gt;
          &lt;li&gt;소셜 링크를 통한 빠른 정보 전파, 봇의 참여, 극단화 현상 주의&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;오정보의 빠른 확산으로 인한 인포데믹 현상
        &lt;ul&gt;
          &lt;li&gt;인포데믹 (Infodemic): 사실 정보와 더불어 오정보의 양이 늘어 구분이 어려워지는 정보 과부화 현상&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;데이터 사용과 서비스 개발에 사용자 어려움을 반영해야 함
        &lt;ul&gt;
          &lt;li&gt;원치 않은 광고&lt;/li&gt;
          &lt;li&gt;원치 않은 메일 수신&lt;/li&gt;
          &lt;li&gt;회원가입 시 너무 많은 개인정보 요구&lt;/li&gt;
          &lt;li&gt;유해 콘텐츠 노출&lt;/li&gt;
          &lt;li&gt;사이트마다 유사한 내용의 콘텐츠 제공&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The right to be forgotten (잊혀질 권리)&lt;/li&gt;
  &lt;li&gt;윤리에 대한 법적 제도 (GDPR, General Data Protection Regulation)&lt;/li&gt;
  &lt;li&gt;AI and Ethical Decisions (인공지능 알고리즘으로 인한 부작용 존재 - 챗봇, 채용 등)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;결론&quot;&gt;결론&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터의 확보, 전처리, 분석, 해석의 모든 과정이 중요: 고품질의 데이터가 입력되었을 때 학습 결과도 유의미하며, 데이터가 갖는 오차 범위와 특이점, 대표성에 대한 충분한 이해를 가지고 접근&lt;/li&gt;
  &lt;li&gt;알고리즘의 설명력, 편향, 신뢰의 문제에 주의: 블랙박스 알고리즘이 실제 적용되기 위해 설명력 보강이 필요하고, 노이즈와 데이터 가변성에도 대처 가능한 알고리즘을 개발하도록 노력해야 함. 더불어 AI가 다양한 서비스에서 인간 결정을 돕거나, 대체함에 따라 윤리적 의사결정이 확보되도록 점검&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-ai-ethics&quot;&gt;2. AI Ethics&lt;/h2&gt;

&lt;h3 id=&quot;인공지능-알고리즘과-윤리-이슈&quot;&gt;인공지능 알고리즘과 윤리 이슈&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;AI and Creativity
    &lt;ul&gt;
      &lt;li&gt;GAN 알고리즘을 이용한 미술, 음악 등 예술에서의 적용&lt;/li&gt;
      &lt;li&gt;자연언어처리 (NLP) 기술의 혁신 - BERT, GPT 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AI Art in Action
    &lt;ul&gt;
      &lt;li&gt;인공지능이 만들어낸 작품의 가격은 $432,500&lt;/li&gt;
      &lt;li&gt;AI 예술 작품은 학습 데이터 기반인데, 창작성 (Originality)이 있을까?&lt;/li&gt;
      &lt;li&gt;학습 데이터, 프로그래머, 기획자 사이에 저작권 이슈가 존재&lt;/li&gt;
      &lt;li&gt;NFT (Non-Fungible Token)의 시대에서도 영향을 미치고 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Copyright Issues
    &lt;ul&gt;
      &lt;li&gt;학습에 사용된 데이터를 제공한 사람에게도 혜택이 돌아가기 어려움&lt;/li&gt;
      &lt;li&gt;창작자인 AI는 법적 권리를 제공할 수 있는 법적 제도가 없음&lt;/li&gt;
      &lt;li&gt;현존하는 예술가의 스타일을 따라한 예술 작품을 만들 경우 상업적 피해 가능성&lt;/li&gt;
      &lt;li&gt;창작된 작품이 인간의 윤리적 규범을 따르지 않을 가능성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;AI Contributed Harm
    &lt;ul&gt;
      &lt;li&gt;아시모프의 로봇 3원칙
        &lt;ul&gt;
          &lt;li&gt;로봇은 인간을 다치게 해서는 안되며, 인간이 해를 입은 것을 방관해서는 안됨&lt;/li&gt;
          &lt;li&gt;첫 번째 법칙에 위배되지 않는 한, 로봇은 인간의 명령에 복종&lt;/li&gt;
          &lt;li&gt;첫 번째, 두 번째 법칙에 위배되지 않는 한, 로봇은 스스로 보호해야 함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;자율 주행 차량
        &lt;ul&gt;
          &lt;li&gt;사고가 났을 때, 누가 책임 져야 할까? -&amp;gt; 소유자, 회사, 개발자, 운전자, 자율주행차, 보행자?&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;로봇의 인격화: 로봇이 고통을 느끼지 못하더라도, 로봇 학대는 인류에 나쁠 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;결론-1&quot;&gt;결론&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;인간의 창조적 활동 영역으로 들어온 인공지능: AI가 기술 혁신과 창작 도구로 활용이 확대되어, 인간의 개입 없이 독자적 창작과 혁신 활동이 가능한 수준으로 발전하리라 전망&lt;/li&gt;
  &lt;li&gt;AI 시대 지적 재산, 법 인격, 처벌, 그리고 윤리 문제 부각: AI에 의한 발명과 저작 등에 대한 법제 정비, 오동작시 처벌과 윤리 규정 마련 등의 논의가 다양한 시민의 수요와 요구를 반영하도록 유의&lt;/li&gt;
  &lt;li&gt;AI에 대한 경계와 규제의 선택은 인류에 대한 재정의&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-세계적인-데이터-과학자가-되는-방법&quot;&gt;3. 세계적인 데이터 과학자가 되는 방법&lt;/h2&gt;

&lt;h3 id=&quot;데이터-과학자-인사이트&quot;&gt;데이터 과학자 인사이트&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Becoming a World-class Data Scientist
    &lt;ul&gt;
      &lt;li&gt;데이터에 대한 관심과 호기심&lt;/li&gt;
      &lt;li&gt;이종 데이터의 결합은 혁신을 가져올 수 있음&lt;/li&gt;
      &lt;li&gt;통신사 데이터를 대중교통 노선을 만드는데 활용&lt;/li&gt;
      &lt;li&gt;소셜 네트워크 데이터를 통해 정치 성향, 좋아하는 브랜드 등 유추가 가능&lt;/li&gt;
      &lt;li&gt;가짜 뉴스 탐지&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;새로운 기회
    &lt;ul&gt;
      &lt;li&gt;이종 (Heterogeneous) 빅데이터의 결합과 새로운 인공지능 기반 계산과학 방법의 적용&lt;/li&gt;
      &lt;li&gt;데이터 사이언스 기반 난제 해결&lt;/li&gt;
      &lt;li&gt;정책 결정 및 신규 산업 창출의 도약 대두&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;세계적 데이터 과학자는 어떻게 일할까?
    &lt;ul&gt;
      &lt;li&gt;데일리 루틴으로 데이터에 관심을 가지고, 조금씩 결과를 내는 것&lt;/li&gt;
      &lt;li&gt;변화가 곧 생존&lt;/li&gt;
      &lt;li&gt;계획한 것을 바로 실행&lt;/li&gt;
      &lt;li&gt;What gets scheduled, gets done
        &lt;ul&gt;
          &lt;li&gt;목표가 낮아서 너무 빨리 성취하는 오류&lt;/li&gt;
          &lt;li&gt;50%의 성공 확률을 가지는 설레는 목표&lt;/li&gt;
          &lt;li&gt;계속해서 재조정하기&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;실패를 두려워 하지 말라&lt;/li&gt;
      &lt;li&gt;레이 달리오, 성공의 원칙
        &lt;ul&gt;
          &lt;li&gt;목표가 무엇인지 안다.&lt;/li&gt;
          &lt;li&gt;문제를 찾아낸다.&lt;/li&gt;
          &lt;li&gt;근본적 원인을 발견한다.&lt;/li&gt;
          &lt;li&gt;극복하기 위한 계획을 세운다.&lt;/li&gt;
          &lt;li&gt;실행한다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 04 Jan 2023 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/LG-Aimers-AI-%EC%9C%A4%EB%A6%AC/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/LG-Aimers-AI-%EC%9C%A4%EB%A6%AC/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Education</category>
        
        <category>LG Aimers 2기</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 10주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 10주차 강의에 대한 정리입니다. &lt;br /&gt;
딥러닝의 심화 내용과 함께 CNN과 RNN에 대하여 학습합니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-딥러닝-기초&quot;&gt;1. 딥러닝 기초&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;영상 분류: 과거에는 매우 어렵고, 도전적 문제&lt;/li&gt;
  &lt;li&gt;ImageNet: 2만 2천여 부류에 대해 수백~수만장의 사진을 인터넷에서 수집하여 1500만여 장의 사진을 구축 및 공개&lt;/li&gt;
  &lt;li&gt;ILSVRC (ImageNet Large Scale Visual Recognition Competition) 대회 - CVPR 학술대회에서 개최
    &lt;ul&gt;
      &lt;li&gt;1000가지 부류에 대해 분류, 검출, 위치 지정 문제&lt;/li&gt;
      &lt;li&gt;120만 장의 훈련 집합, 5만 장의 검증 집합, 15만 장의 테스트 집합&lt;/li&gt;
      &lt;li&gt;우승: AlexNet (2012) -&amp;gt; GoogleNet &amp;amp; VGGNet (2014) -&amp;gt; ResNet (2015)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;우승한 모델은 코드와 학습된 가중치를 공개하여 널리 사용되는 표준 신경망이 됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;alexnet&quot;&gt;AlexNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;컨볼루션층 5개와 완전 연결 (Fully Connected, FC) 층 3개
    &lt;ul&gt;
      &lt;li&gt;8개 층에 290300-186624-64896-43264-4096-4096-1000개의 노드 배치&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;컨볼루션층은 200만개, FC층은 6500만개 가량의 매개 변수
    &lt;ul&gt;
      &lt;li&gt;FC층에 30배 많은 매개변수 (향후 CNN은 FC층의 매개변수를 줄이는 방향으로 발전함)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;구조
    &lt;ul&gt;
      &lt;li&gt;당시 GPU의 메모리 크기 제한으로 인해 GPU 1, 2로 분할하여 학습 수행&lt;/li&gt;
      &lt;li&gt;3번째 컨볼루션층은 두 개의 결과를 함께 사용 (Inter-GPU Connections)&lt;/li&gt;
      &lt;li&gt;컨볼루션층 큰 보폭으로 다운 샘플링&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;학습에 성공한 요인
    &lt;ul&gt;
      &lt;li&gt;외적요인: ImageNet 이라는 대규모 사진 데이터, GPU를 사용한 병럴 처리&lt;/li&gt;
      &lt;li&gt;내적요인: 활성 함수로 ReLU 사용, 지역 반응 정규화 기법 적용, 과잉적합 방지하는 규제 기법 적용
        &lt;ul&gt;
          &lt;li&gt;인간 신경망 측면 억제 모방, ReLU 활성화 규제&lt;/li&gt;
          &lt;li&gt;데이터 확대 (잘라내기 - Cropping, 반전 - Mirroring으로 2048배로 확대)&lt;/li&gt;
          &lt;li&gt;드롭아웃 (완전연결층에서 사용함)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;테스트 단계에서 앙상블 적용: 입력된 영상을 잘라내기와 반전을 통해 증가시켜서 2~3% 만큼 오류율 감소 효과&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;vggnet&quot;&gt;VGGNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;3*3의 작은 커널을 사용함
    &lt;ul&gt;
      &lt;li&gt;GoogleNet의 인셉션 모듈처럼 이후 깊은 신경망 구조에 영향&lt;/li&gt;
      &lt;li&gt;큰 크기의 커널은 여러 개의 작은 크기 커널로 분해 될 수 있음&lt;/li&gt;
      &lt;li&gt;매개변수의 수는 줄이면서 신경망은 깊어지는 효과
        &lt;ul&gt;
          &lt;li&gt;5 by 5 커널을 2층의 3 by 3 커널로 분해하여 구현
            &lt;ul&gt;
              &lt;li&gt;5 by 5 커널의 매개변수는 5 * 5 = 25&lt;/li&gt;
              &lt;li&gt;3 by 3 커널의 매개변수는 9 + 9 = 18&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;3 by 3 커널을 1 by 3 커널과 3 by 1 커널로 분해하여 구현
            &lt;ul&gt;
              &lt;li&gt;3 by 3 커널의 매개변수는 3 * 3 = 9&lt;/li&gt;
              &lt;li&gt;1 by 3 커널의 매개변수는 1 * 3 = 3&lt;/li&gt;
              &lt;li&gt;3 by 1 커널의 매개변수는 3 * 1 = 3&lt;/li&gt;
              &lt;li&gt;따라서 기존 9개보다 6개로 줄어드는 효과를 볼 수 있음&lt;/li&gt;
              &lt;li&gt;결국, 최종적으로 n이 클수록 매개변수의 수는 줄어드는 효과가 있음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망을 더욱 깊게 만듦 (신경망의 깊이가 어떤 영향을 주는지 확인)&lt;/li&gt;
  &lt;li&gt;컨볼루션층 8~16개를 두어 AlextNet의 5개에 비해서 2~3배 깊어짐&lt;/li&gt;
  &lt;li&gt;1 by 1 커널: VGGNet은 적용 실험만 하고, 최종 선택은 안함 (GoogleNet에서는 사용됨)&lt;/li&gt;
  &lt;li&gt;차원 통합, 차원 축소 효과&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;googlenet&quot;&gt;GoogleNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;핵심은 인셉션 모듈 (Inception) - 총 9개의 인셉션 모듈을 포함&lt;/li&gt;
  &lt;li&gt;Conv 레이어를 sparse 하게 연결하고, 행렬 연산은 dense 하게 처리함&lt;/li&gt;
  &lt;li&gt;수용장의 다양한 특징을 추출하기 위해 NIN의 구조를 확장하여 복수의 병렬적인 컨볼루션 층을 가짐
    &lt;ul&gt;
      &lt;li&gt;NIN 구조는 기존 컨볼루션 연산을 MLPConv 연산으로 대체하는 것
        &lt;ul&gt;
          &lt;li&gt;커널 대신 비선형 함수를 활성 함수로 포함하는 MLP를 사용하여 특징 추출에 유리&lt;/li&gt;
          &lt;li&gt;신경망의 미소 신경망 (Micro NN)이 주어진 수용장의 특징을 추상화 시도&lt;/li&gt;
          &lt;li&gt;전역 평균 풀링 (Global Average Pooling) 사용
            &lt;ul&gt;
              &lt;li&gt;FC층 대신 Global Average Pooling을 사용&lt;/li&gt;
              &lt;li&gt;전 층에서 나온 특징 맵들을 각각 평균 낸 것을 이어서 1차원 벡터 생성&lt;/li&gt;
              &lt;li&gt;FC층을 사용했을 때에 비해, 가중치의 개수를 상당히 줄일 수 있었음&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GoogleNet은 NIN 개념을 확장한 신경망
    &lt;ul&gt;
      &lt;li&gt;인셉션 모듈: 마이크로 네트워크로 MLPConv 대신 네 종류의 컨볼루션 연산 사용 (다양한 특징 추출)&lt;/li&gt;
      &lt;li&gt;1 by 1 컨볼루션을 사용하여 차원 축소: 매개변수의 수 (특징 맵의 수)를 줄임, 깊은 신경망&lt;/li&gt;
      &lt;li&gt;3 by 3, 5 by 5 같은 다양한 크기의 컨볼루션을 통해 다양한 특징을 추출&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;매개변수가 있는 층 22개, 없는 층 (풀링) 5개로 총 27개 층&lt;/li&gt;
  &lt;li&gt;완전 연결층은 1개에 불과 함 (1백만 개의 매개변수를 가지고, VGGNet의 완전 연결층에 비하면 1% 수준)&lt;/li&gt;
  &lt;li&gt;두 개의 보조 분류기 추가
    &lt;ul&gt;
      &lt;li&gt;네트워크가 깊어지면서 발생하는 기울기 소실 문제를 줄이기 위해 추가&lt;/li&gt;
      &lt;li&gt;원 분류기의 오류 역전파 결과와 보조 분류기의 오류 역전파 결과를 결합하여 경사 소멸 문제 완화&lt;/li&gt;
      &lt;li&gt;학습할 때 도우미 역할을 하고, 추론할 때 제거됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;resnet&quot;&gt;ResNet&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;잔류 (잔차) 학습이라는 개념을 이용하여 성능 저하를 피하면서 층 수를 대폭 늘림&lt;/li&gt;
  &lt;li&gt;지름길 연결을 두는 이유?
    &lt;ul&gt;
      &lt;li&gt;깊은 신경망도 최적화가 가능해짐&lt;/li&gt;
      &lt;li&gt;단순한 학습의 관점 변화를 통한 신경망 구조 변화&lt;/li&gt;
      &lt;li&gt;단순 구조의 변경으로 매개변수 수에는 영향이 없음&lt;/li&gt;
      &lt;li&gt;덧셈 연산만 증가하므로, 전체 연산량 증가도 거의 미비&lt;/li&gt;
      &lt;li&gt;깊어진 신경망으로 인해 정확도 개선 가능&lt;/li&gt;
      &lt;li&gt;경사 소멸 문제 해결&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;VGGNet과 같은 점: 3*3 커널 사용&lt;/li&gt;
  &lt;li&gt;VGGNet과 다른 점: 잔류 학습 사용, 전역 평균 풀링 사용 (FC층 제거), 배치 정규화 적용 (Dropout 필요 없음)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-cnn-models&quot;&gt;2. CNN Models&lt;/h2&gt;

&lt;h3 id=&quot;목적함수-교차-엔트로피와-로그-우도-그리고-소프트맥스-활성화-함수&quot;&gt;목적함수: 교차 엔트로피와 로그 우도 그리고 소프트맥스 활성화 함수&lt;/h3&gt;

&lt;h2 id=&quot;3-딥러닝-최적화&quot;&gt;3. 딥러닝 최적화&lt;/h2&gt;

&lt;h2 id=&quot;4-rnn&quot;&gt;4. RNN&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 프로그래머스 인공지능 데브코스 4기 10주차 강의 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/10%EC%A3%BC%EC%B0%A8&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 28 Nov 2022 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-10%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-10%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 9주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 9주차 강의에 대한 정리입니다. &lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-다층-퍼셉트론1&quot;&gt;1. 다층 퍼셉트론1&lt;/h2&gt;
&lt;h3 id=&quot;인공신경망과-생물신경망&quot;&gt;인공신경망과 생물신경망&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;사람의 뉴런: 두뇌의 가장 작은 정보 처리 단위&lt;/li&gt;
  &lt;li&gt;컴퓨터가 사람 뇌의 정보 처리를 모방하여 지능적 행위를 할 수 있는 인공지능 도전
    &lt;ul&gt;
      &lt;li&gt;뉴런의 동작 이해를 모방한 초기 인공 신경망 (ANN) 연구 시작&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;퍼셉트론&lt;/strong&gt;이 고안됨&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;신경망의 종류
    &lt;ul&gt;
      &lt;li&gt;전방 (Forward) 신경망, 순환 (Recurrent) 신경망&lt;/li&gt;
      &lt;li&gt;얕은 (Shallow) 신경망, 깊은 (Deep) 신경망&lt;/li&gt;
      &lt;li&gt;결정론 (Deterministic) 신경망: 모델의 매개변수와 조건에 의해 출력이 완전히 결정되는 신경망&lt;/li&gt;
      &lt;li&gt;확률론 (Stochastic) 신경망: 고유의 임의성을 가지고 매개변수와 조건이 같더라도 다른 출력을 갖는 신경망&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;퍼셉트론: 절 (Node), 가중치 (Weight), 층 (Layer)과 같은 새로운 개념의 구조 도입
    &lt;ul&gt;
      &lt;li&gt;제시된 퍼셉트론 구조의 학습 알고리즘을 제안&lt;/li&gt;
      &lt;li&gt;깊은 인공신경망 (Deep Learning)을 포함한 현대 인공신경망의 토대&lt;/li&gt;
      &lt;li&gt;입력 (편향 노드 포함) -&amp;gt; 입력과 출력 사이의 연산 -&amp;gt; 출력&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;일반적인 분류기의 학습 과정
    &lt;ol&gt;
      &lt;li&gt;과업 정의와 분류 과정의 수학적 정의 (가설 설정)&lt;/li&gt;
      &lt;li&gt;해당 분류기의 목적함수 정의&lt;/li&gt;
      &lt;li&gt;목적함수를 최소화 하는 값을 찾기 위한 최적화 수행
        &lt;ul&gt;
          &lt;li&gt;경사하강법을 통해 기울기를 미분하여 반복 탐색해 극값을 찾음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-다층-퍼셉트론2&quot;&gt;2. 다층 퍼셉트론2&lt;/h2&gt;
&lt;h3 id=&quot;다층-퍼셉트론&quot;&gt;다층 퍼셉트론&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;퍼셉트론: 선형 분류기 (Linear Classifier)의 한계
    &lt;ul&gt;
      &lt;li&gt;OR, AND 분류기는 가능하지만, XOR 문제는 해결하지 못함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다층 퍼셉트론의 핵심 아이디어
    &lt;ul&gt;
      &lt;li&gt;은닉층을 두어, 특징 공간을 분류하는데 유리한 새로운 특징 공간으로 변환&lt;/li&gt;
      &lt;li&gt;연성에서는 출력이 연속값이므로 시그모이드 함수를 활성화 함수로 도입&lt;/li&gt;
      &lt;li&gt;오류 역전파 알고리즘을 사용하여 한 층씩 그레디언트를 계산하고, 가중치를 갱신&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;특징 공간 변환
    &lt;ul&gt;
      &lt;li&gt;퍼셉트론 2개를 병렬 결합하면, 원래 공간을 새로운 특징 공간으로 변환 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;추가 퍼셉트론 1개를 순차 결합하면, &lt;strong&gt;다층 퍼셉트론&lt;/strong&gt;이 됨&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;활성화-함수&quot;&gt;활성화 함수&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;딱딱한 공간 분할과 부드러운 공간 분할
    &lt;ul&gt;
      &lt;li&gt;계단 함수는 딱딱한 의사결정: 영역을 점으로 변환&lt;/li&gt;
      &lt;li&gt;그 외에 활성화 함수는 부드러운 의사결정: 영역을 영역으로 변환
        &lt;ul&gt;
          &lt;li&gt;로지스틱 시그모이드&lt;/li&gt;
          &lt;li&gt;하이퍼볼릭 탄젠트 시그모이드&lt;/li&gt;
          &lt;li&gt;Softplus와 Rectifier (ReLU)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;활성화 함수에 따른 다층 퍼셉트론의 공간 분할 능력 변화 (경성 부분 변화)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;일반적으로 은닉층에서 &lt;strong&gt;로지스틱 시그모이드&lt;/strong&gt;를 활성화 함수로 많이 사용
    &lt;ul&gt;
      &lt;li&gt;S자 모양의 넓은 포화 곡선은 경사도 기반한 학습 (오류 역전파)을 어렵게 함&lt;/li&gt;
      &lt;li&gt;기울기 소실 (Gradient Vanishing) 문제 발생&lt;/li&gt;
      &lt;li&gt;따라서 깊은 신경망에서는 &lt;strong&gt;ReLU&lt;/strong&gt;를 활용
        &lt;ul&gt;
          &lt;li&gt;계단 활성화 함수의 범위는 -1과 1&lt;/li&gt;
          &lt;li&gt;로지스틱 활성화 함수의 범위는 0부터 1&lt;/li&gt;
          &lt;li&gt;하이퍼볼릭 탄젠트 활성화 함수의 범위는 -1부터 1&lt;/li&gt;
          &lt;li&gt;소프트플러스, 렉티파이어 (ReLU) 활성화 함수의 범위는 0부터 무한대&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;구조&quot;&gt;구조&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기존에는 입력층 -&amp;gt; 은닉층 -&amp;gt; 출력층의 2층 구조&lt;/li&gt;
  &lt;li&gt;입력층 -&amp;gt; 은닉층 -&amp;gt; 은닉층 -&amp;gt; 출력층의 3층 구조
    &lt;ul&gt;
      &lt;li&gt;p개의 은닉 노드: p는 하이퍼 매개변수&lt;/li&gt;
      &lt;li&gt;p가 너무 크면 과잉적합, 너무 작으면 과소적합&lt;/li&gt;
      &lt;li&gt;하이퍼 매개변수 (Hyper-paramenters) 최적화 필요&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;동작&quot;&gt;동작&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;특징 벡터 x를 출력 벡터 o로 사상 (Mapping) 하는 함수로 간주할 수 있음
    &lt;ul&gt;
      &lt;li&gt;2층 퍼셉트론: o = f2(f1(x))&lt;/li&gt;
      &lt;li&gt;3층 퍼셉트론: o = f3(f2(f1(x)))&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;은닉층은 특징 추출기
    &lt;ul&gt;
      &lt;li&gt;은닉층은 특징 벡터를 분류에 더 유리한 새로운 특징 공간으로 변환&lt;/li&gt;
      &lt;li&gt;현대 기계학습에서는 특징학습 (Feature Learning, Data-driven Learning) 이라 부름&lt;/li&gt;
      &lt;li&gt;심층학습은 더 많은 층을 거쳐 계층화 된 특징학습을 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;범용적 근사 이론 (Univeral Approximation Theorem)
    &lt;ul&gt;
      &lt;li&gt;하나의 은닉층은 함수의 근사를 표현&lt;/li&gt;
      &lt;li&gt;다층 퍼셉트론도 공간을 변환하는 근사 함수&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;얕은 은닉층의 구조: 일반적으로 깊은 은닉층의 구조가 좋은 성능을 가짐&lt;/li&gt;
  &lt;li&gt;입력층 -&amp;gt; 은닉층 (순방향 전파) -&amp;gt; 오차 계산 -&amp;gt; 은닉층 (역방향 전파) -&amp;gt; 오차 계산&lt;/li&gt;
  &lt;li&gt;학습 알고리즘은 오류 역전파를 반복하여 수행&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;성능-향상을-위한-경험의-중요성&quot;&gt;성능 향상을 위한 경험의 중요성&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;순수한 최적화 알고리즘으로는 높은 성능이 불가능
    &lt;ul&gt;
      &lt;li&gt;데이터 희소성, 잡음, 미숙한 신경망 구조 등 때문&lt;/li&gt;
      &lt;li&gt;성능 향상을 위한 다양한 경험 (Heuristics)을 개발하고 공유함
        &lt;ul&gt;
          &lt;li&gt;아키텍쳐, 초깃값, 학습률, 활성화 함수&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-다층-퍼셉트론3&quot;&gt;3. 다층 퍼셉트론3&lt;/h2&gt;
&lt;h3 id=&quot;목적-함수의-정의&quot;&gt;목적 함수의 정의&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;훈련집합
    &lt;ul&gt;
      &lt;li&gt;특징 벡터 집합 (X)과 부류 벡터 집합 (Y) - 지도학습&lt;/li&gt;
      &lt;li&gt;부류 벡터는 단발성 (One-hot) 코드로 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기계학습의 목표: 모든 샘플을 옳게 분류하는 함수 f를 찾는 것&lt;/li&gt;
  &lt;li&gt;목적 함수: 평균 제곱 오차 (Mean Squared Error, MSE)&lt;/li&gt;
  &lt;li&gt;전방 전파와 오류 역전파&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;오류-역전파-알고리즘의-설계&quot;&gt;오류 역전파 알고리즘의 설계&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;연쇄 법칙의 구현: 반복되는 부분식들 (Subexpressions)을 저장하거나 재연산을 최소화&lt;/li&gt;
  &lt;li&gt;목적 함수의 최저점을 찾아주는 &lt;strong&gt;경사 하강법&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;출력의 오류를 역방향 (왼쪽)으로 전파하여 경사도를 계산하는 알고리즘 (&lt;strong&gt;오류 역전파&lt;/strong&gt; 알고리즘)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;미니배치-확률론적-경사-하강법&quot;&gt;미니배치 확률론적 경사 하강법&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;미니배치 방식
    &lt;ul&gt;
      &lt;li&gt;한번에 t개의 샘플을 처리 (t는 미니배치 크기)&lt;/li&gt;
      &lt;li&gt;미니배치 방식은 보통 수십 ~ 수백
        &lt;ul&gt;
          &lt;li&gt;경사도의 잡음을 줄여주는 효과 때문에 수렴이 빨라짐&lt;/li&gt;
          &lt;li&gt;GPU를 사용한 병렬처리에도 유리함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;현대 기계학습은 미니배치 기반의 확률론적 경사 하강법을 표준처럼 널리 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-심층-학습-기초-1&quot;&gt;4. 심층 학습 기초 1&lt;/h2&gt;
&lt;h3 id=&quot;심층-학습-deep-learning&quot;&gt;심층 학습 (Deep Learning)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;다층 퍼셉트론에 은닉츠을 여러 개 추가하면, 깊은 신경망이 됨 (심층 학습은 깊은 신경망의 학습)&lt;/li&gt;
  &lt;li&gt;심층 학습은 새로운 응용을 창출하고, 인공지능 제품의 성능을 획기적으로 향상 (현대 기계학습 주도)&lt;/li&gt;
  &lt;li&gt;1980년대 이미 깊은 신경망 아이디어는 등장했으나, 당시에는 실현 불가능
    &lt;ul&gt;
      &lt;li&gt;경사 소멸 (Gradient Vanishing) -&amp;gt; 활성화 함수 변화를 통해 해결&lt;/li&gt;
      &lt;li&gt;작은 훈련 집합, 과다한 연산과 시간 소요 (낮은 연산의 범용 컴퓨터, 값 비싼 슈퍼 컴퓨터)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;일부 연구자들은 실망스러운 상황에서도 지속적인 연구
    &lt;ul&gt;
      &lt;li&gt;학습률에 따른 성능 변화 양상&lt;/li&gt;
      &lt;li&gt;모멘텀과 같은 최적 탐색 방법 모색&lt;/li&gt;
      &lt;li&gt;은닉 노드 수에 따른 성능 변화&lt;/li&gt;
      &lt;li&gt;데이터 전처리의 영향, 활성함수의 영향, 규제 기법의 영향&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;심층 학습의 성공 배경
    &lt;ul&gt;
      &lt;li&gt;혁신적 알고리즘 등장 (합성곱 신경망, CNN 구조)&lt;/li&gt;
      &lt;li&gt;경사 소멸 문제 해결을 위한 ReLU 활성 함수&lt;/li&gt;
      &lt;li&gt;과잉 적합을 방지하는데 효과적인 다양한 규제 기법&lt;/li&gt;
      &lt;li&gt;층별 예비 학습 (Pretraining) 기법 개발&lt;/li&gt;
      &lt;li&gt;값싼 GPGPU 등장, 학습 데이터 양과 질의 향상&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;표현-학습의-부각&quot;&gt;표현 학습의 부각&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;전통적인 다층 퍼셉트론 (은닉층은 특징 추출기)
    &lt;ul&gt;
      &lt;li&gt;얕은 구조이므로 가공하지 않은 획득한 원래 패턴을 그대로 입력하면 낮은 성능&lt;/li&gt;
      &lt;li&gt;따라서 사람이 수작업 특징을 선택하거나, 추출하여 신경망에 입력&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;현대 기계학습 (심층학습)
    &lt;ul&gt;
      &lt;li&gt;학습에 의해 자동적으로 데이터로붵 특징 추출 (표현 학습 = Representation Learning)&lt;/li&gt;
      &lt;li&gt;특징 벡터를 신경망의 입력 (종단간 학습 = End-to-End Learning)&lt;/li&gt;
      &lt;li&gt;깊은 신경망을 통한 계층적 표현 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;깊은 신경망의 표현 학습 (특징 학습)
    &lt;ul&gt;
      &lt;li&gt;낮은 단계 은닉층은 선이나 모서리 같은 간단한 저급 특징 추출&lt;/li&gt;
      &lt;li&gt;높은 단계 은닉층은 추상적 형태의 복잡한 고급 특징 추출&lt;/li&gt;
      &lt;li&gt;표현 학습이 강력해져서 기존 응용에서 획기적인 성능 향상
        &lt;ul&gt;
          &lt;li&gt;영상 인식, 음성 인식, 언어 번역 등&lt;/li&gt;
          &lt;li&gt;새로운 응용 창출 (화소 수준의 영상 분할, CNN과 LSTM의 혼합 학습 모델 등)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;깊은-다층-퍼셉트론-깊은-신경망&quot;&gt;깊은 다층 퍼셉트론 (깊은 신경망)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;깊은 다층 퍼셉트론의 구조와 동작
    &lt;ul&gt;
      &lt;li&gt;입력 (d+1 차원의 특징 벡터)과 출력 (c개 분류)&lt;/li&gt;
      &lt;li&gt;L-1개의 은닉층 (입력층은 0번째 은닉층, 출력층은 L번째 은닉층으로 간주)&lt;/li&gt;
      &lt;li&gt;DMLP (Deep Multi-Layers Perceptron)의 가중치 행렬&lt;/li&gt;
      &lt;li&gt;DMLP의 동작: MLP의 동작을 나타내는 식을 보다 많은 단계로 확장한 것&lt;/li&gt;
      &lt;li&gt;DMLP 학습은 기존 MLP 학습과 유사 (경사도 계산, 가중치 갱신을 더 많은 층에서 수행)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;깊은-다층-퍼셉트론의-학습&quot;&gt;깊은 다층 퍼셉트론의 학습&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;주요 알고리즘의 개선 및 합성곱 신경망 (CNN)의 부상
    &lt;ul&gt;
      &lt;li&gt;구조: 퍼셉트론 -&amp;gt; 다층 퍼셉트론 -&amp;gt; 깊은 다층 퍼셉트론&lt;/li&gt;
      &lt;li&gt;활성함수: 계단 함수 -&amp;gt; 시그모이드 함수 -&amp;gt; ReLU와 변형&lt;/li&gt;
      &lt;li&gt;목적함수: 평균 제곱 오차 -&amp;gt; 평균 제곱 오차 -&amp;gt; 교차 엔트로피 또는 로그우도&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;심층-학습은-왜-강력한가&quot;&gt;심층 학습은 왜 강력한가?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;종단간 최적화 된 학습 가능
    &lt;ul&gt;
      &lt;li&gt;고전적 방법에서는 사람의 직관에 따르기 때문에 성능에 한계&lt;/li&gt;
      &lt;li&gt;인식 대상이 달라지게 되면, 새로 처음부터 설계해야 했음&lt;/li&gt;
      &lt;li&gt;하지만 심층 학습은 전체 깊은 신경망을 동시에 최적화 (종단간 학습, End-to-End)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;깊이 (Depth)의 중요성 (더 깊어질수록, 더 정교한 분할)&lt;/li&gt;
  &lt;li&gt;계층적 특징 (Hierarchical Features)
    &lt;ul&gt;
      &lt;li&gt;깊은 신경망에서는 층의 역할이 잘 구분됨&lt;/li&gt;
      &lt;li&gt;반면 얕은 신경망은 하나 또는 두 개의 은닉층이 여러 형태의 특징을 모두 답당&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-convolutional-neural-network-cnn&quot;&gt;5. Convolutional Neural Network (CNN)&lt;/h2&gt;
&lt;h3 id=&quot;컨볼루션-신경망&quot;&gt;컨볼루션 신경망&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DMLP: 완전 연결 구조로 높은 복잡도, 학습이 느리고 과잉 적합이 발생할 수도 있음&lt;/li&gt;
  &lt;li&gt;컨볼루션 신경망 (CNN) -&amp;gt; 부분 연결 구조
    &lt;ul&gt;
      &lt;li&gt;격자 구조를 갖는 데이터에 적합&lt;/li&gt;
      &lt;li&gt;컨볼루션 연산을 수행하여 특징 추출&lt;/li&gt;
      &lt;li&gt;영상 분류나 문자 인식 등 인식 문제에 높은 성능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;컨볼루션 (Convolution): 해당하는 요소끼리 곱해서 결과를 모두 더하는 선형 연산&lt;/li&gt;
  &lt;li&gt;보폭 (Stride): 커널을 다음 컨볼루션 연산을 위해 이동시키는 칸 수&lt;/li&gt;
  &lt;li&gt;패딩 (Padding): 컨볼루션 결과의 크리를 조정하기 위해 입력 배열의 둘레를 확장하고, 0으로 채우는 연산&lt;/li&gt;
  &lt;li&gt;풀링: 일정 크기의 블록을 통합하여 하나의 대푯값으로 대체하는 연산
    &lt;ul&gt;
      &lt;li&gt;최댓값 풀링 (Max Pooling): 지정된 블록 내의 원소들 중에서 최댓값을 대푯값으로 선택&lt;/li&gt;
      &lt;li&gt;평균값 풀링 (Average Pooling): 블록 내의 원소들의 평균값을 대푯값으로 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;컨볼루션-신경망-1&quot;&gt;컨볼루션 신경망&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;특징 추출
    &lt;ul&gt;
      &lt;li&gt;컨볼루션 연산을 하는 Conv 층&lt;/li&gt;
      &lt;li&gt;ReLU 연산을 하는 ReLU&lt;/li&gt;
      &lt;li&gt;풀링 연산을 하는 Pool&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;추출된 특징을 통해 분류나 회귀를 수행하는 다층 퍼셉트론
    &lt;ul&gt;
      &lt;li&gt;전체 연결된 (Fully connected) FC 층 반복&lt;/li&gt;
      &lt;li&gt;분류의 경우 마지막 층에 소프트맥스 연산 수행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-심층-학습-기초-2&quot;&gt;6. 심층 학습 기초 2&lt;/h2&gt;
&lt;h3 id=&quot;컨볼루션-합성곱-신경망---cnn&quot;&gt;컨볼루션 (합성곱) 신경망 - CNN&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;영상 인식의 예: 픽셀 단위의 정보로부터 특정 사물 등을 인식하는 것&lt;/li&gt;
  &lt;li&gt;컴퓨터 비전의 어려운 점
    &lt;ul&gt;
      &lt;li&gt;동일한 객체라도 영상을 찍는 카메라 이동에 따라 모든 픽셀 값이 변화 됨&lt;/li&gt;
      &lt;li&gt;경계색 (보호색)으로 배경과 구분이 어려운 경우&lt;/li&gt;
      &lt;li&gt;조명에 따른 변화로 구분이 힘듦&lt;/li&gt;
      &lt;li&gt;기형적 형태의 영상 존재, 일부가 가려진 영상 존재&lt;/li&gt;
      &lt;li&gt;같은 종류 간의 변화가 큼 (같은 고양이라도 고양이의 크기가 다름)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;컨볼루션층 (CONV): 선형 함수인 컨볼루션과 비선형 함수인 활성 함수의 조합&lt;/li&gt;
  &lt;li&gt;풀링층 (POOL): 컨볼루션의 얻어진 특징을 통계적으로 압축&lt;/li&gt;
  &lt;li&gt;덧대기 (Padding): 가장 자리에서 영상의 크기가 줄어드는 효과 방지 (각 층의 입출력 특징 형상 유지)&lt;/li&gt;
  &lt;li&gt;가중치 공유 (묶은 가중치): 모든 노드가 동일한 커널을 사용하므로 매개변수는 3개에 불과 (모델 복잡도가 낮아짐)&lt;/li&gt;
  &lt;li&gt;다중 특징 맵 추출: 커널 값에 따라 커널이 추출하는 특징이 달라짐 (한 개의 커널만 사용하면 너무 빈약한 특징만 추출)&lt;/li&gt;
  &lt;li&gt;전체 구조: CONV - (ReLU) - POOL - … - FC&lt;/li&gt;
  &lt;li&gt;영상 분야에서 다양하게 활용 (분류, 검색, 검출, 분할 등)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dmlp와-cnn의-비교&quot;&gt;DMLP와 CNN의 비교&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;DMLP: 완전 연결 구조로 높은 복잡도, 학습이 느리고 과잉 적합 우려&lt;/li&gt;
  &lt;li&gt;CNN: 컨볼루션 연산을 이용한 부분 연결 (희소 연결) 구조로 복잡도 낮춤, 좋은 특징을 추출해서 학습
    &lt;ul&gt;
      &lt;li&gt;격자 구조 (영상, 음성 등)를 갖는 데이터에 적합&lt;/li&gt;
      &lt;li&gt;수용장은 인간의 시각과 유사&lt;/li&gt;
      &lt;li&gt;가변 크기의 입력 처리 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;CNN의 완전 연결 신경망과 차별
    &lt;ul&gt;
      &lt;li&gt;학습에 의해 결정된 복수의 커널 (혹은 필터)에 대응되는 특징을 추출하는 CONV 층
        &lt;ul&gt;
          &lt;li&gt;각 층의 입출력의 특징 형상을 유지시킴 (특징 맵)&lt;/li&gt;
          &lt;li&gt;영상의 공간 정보를 유지하면서 공간적으로 인접한 정보의 특징을 효과적으로 인식&lt;/li&gt;
          &lt;li&gt;각 커널 (필터)은 파라미터를 공유하여 완전 연결 신경망 대비 학습 파라미터가 적음&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;추출된 영상의 특징을 요약하고 강화하는 POOL 층&lt;/li&gt;
      &lt;li&gt;가변 크기의 데이터 다루기
        &lt;ul&gt;
          &lt;li&gt;완전 연결 신경망은 특징 벡터의 크기가 달라지면, 연산 불가능&lt;/li&gt;
          &lt;li&gt;CNN은 가변 크기를 다룰 수 있음 (컨볼루션 층, 풀링 층에서 커널 및 보폭 수정을 통한 특징 맵 크기 조절)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;컨볼루션-합성곱-연산&quot;&gt;컨볼루션 (합성곱) 연산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;컨볼루션은 해당하는 요소끼리 곱하고 결과를 모두 더하는 선형 연상&lt;/li&gt;
  &lt;li&gt;영상에서 특징을 추출하기 위한 용도로 사용 (공간 필터)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;컨볼루션층&quot;&gt;컨볼루션층&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;특징 학습
    &lt;ul&gt;
      &lt;li&gt;커널을 사람이 설계 하지 않고, 학습으로 찾음
        &lt;ul&gt;
          &lt;li&gt;2차원 영상이 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;7*7&lt;/code&gt; 커널을 64개 사용한다면, 학습은 (7*7+1) * 64 = 3200개의 매개변수를 찾아내야 함&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;DMLP와 마찬가지로 오류 역전파로 커널을 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;컨볼루션 연산에 따른 CNN 특성
    &lt;ul&gt;
      &lt;li&gt;이동에 동변 (신호가 이동하면, 이동 정보가 그대로 특징 맵에 반영)&lt;/li&gt;
      &lt;li&gt;병렬 분산 구조
        &lt;ul&gt;
          &lt;li&gt;각 노드는 독립적으로 계산하므로 병렬 구조&lt;/li&gt;
          &lt;li&gt;노드는 깊은 층을 거치면서 전체에 영향을 미치므로 분산 구조&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;큰 보폭에 의한 다운 샘플링: 일반적으로 보폭이 k이면, k개 마다 하나씩 샘플링하여 커널을 적용&lt;/li&gt;
  &lt;li&gt;텐서 적용: 3차원 이상의 구조에도 적용 가능&lt;/li&gt;
  &lt;li&gt;n차원 구조의 데이터 적용&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;풀링층&quot;&gt;풀링층&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;풀링 (Pooling) 연산
    &lt;ul&gt;
      &lt;li&gt;최대 풀링, 평균 풀링, 가중치 평균 풀링 등&lt;/li&gt;
      &lt;li&gt;보폭을 크게 하면, 다운 샘플링 효과&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;풀링 연산의 특성
    &lt;ul&gt;
      &lt;li&gt;풀링은 상세 내용에서 요약 혹은 평균 등의 통계적 대표성을 추출함&lt;/li&gt;
      &lt;li&gt;매개 변수가 없음&lt;/li&gt;
      &lt;li&gt;특징 맵의 수를 그대로 유지함 (크기 X)&lt;/li&gt;
      &lt;li&gt;연산 효율화 (연산 횟수, 연결 가중치 개수를 줄임)&lt;/li&gt;
      &lt;li&gt;작은 변화에 둔감 (물체 인식이나 영상 검색 등에 효과적)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;전체-구조&quot;&gt;전체 구조&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;빌딩 블록
    &lt;ul&gt;
      &lt;li&gt;CNN은 빌딩 블록을 이어 붙여서 깊은 구조로 확장&lt;/li&gt;
      &lt;li&gt;전형적 빌딩 블록의 예시: 컨볼루션층 -&amp;gt; 활성함수 (주로 ReLU) -&amp;gt; 풀링층&lt;/li&gt;
      &lt;li&gt;다중 커널을 사용하여 다중 특징 맵을 추출&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;컨볼루션 층의 출력 크기와 매개변수 수
    &lt;ul&gt;
      &lt;li&gt;입력: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W1 * H1 * D1&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;K개 F*F 커널, 보폭 S, 덧대기 P&lt;/li&gt;
      &lt;li&gt;출력의 크기: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;W2 * H2 * D2&lt;/code&gt;
        &lt;ul&gt;
          &lt;li&gt;W2 = (W1 - F + 2P) / S + 1&lt;/li&gt;
          &lt;li&gt;H2 = (H1 - F + 2P) / S + 1&lt;/li&gt;
          &lt;li&gt;D2 = K&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;매개변수의 수
        &lt;ul&gt;
          &lt;li&gt;커널마다 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(F*F*D1)&lt;/code&gt;개의 가중치와 1개의 바이어스를 가짐. 전체 매개변수 수는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;(F*F*D1) * K + K&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;일반적으로 F = 2, S = 2 혹은 F = 3, S = 1을 사용함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;초창기 CNN 사례는 LeNet-5
    &lt;ul&gt;
      &lt;li&gt;특징 추출
        &lt;ul&gt;
          &lt;li&gt;CONV - POOL - CONV - POOL - CONV의 다섯 층을 통해 28*28 명암 영상을 120차원의 특징 벡터로 변환&lt;/li&gt;
          &lt;li&gt;평균 풀링 사용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;분류: 은닉층이 하나인 MLP&lt;/li&gt;
      &lt;li&gt;CNN 첫 성공 사례; 필기 숫자 인식기를 만들어서 수표 인식 자동화 시스템 구현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 프로그래머스 인공지능 데브코스 4기 9주차 강의 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/09%EC%A3%BC%EC%B0%A8&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 21 Nov 2022 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-9%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-9%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 7주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 7주차 강의에 대한 정리입니다. &lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-ml-basics-probability&quot;&gt;1. ML Basics (Probability)&lt;/h2&gt;

&lt;h3 id=&quot;machine-learning-기초-소개&quot;&gt;Machine Learning 기초 소개&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Machine Learning (기계학습)
    &lt;ul&gt;
      &lt;li&gt;경험을 통해 자동으로 개선하는 컴퓨터 알고리즘의 연구&lt;/li&gt;
      &lt;li&gt;머신러닝 알고리즘의 결과는 목표값을 예측하는 함수 y(x)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;핵심 개념
    &lt;ul&gt;
      &lt;li&gt;학습 단계: 함수 y(x)를 학습 데이터에 기반해 결정하는 단계&lt;/li&gt;
      &lt;li&gt;시험셋(Test set): 모델을 평가하기 위해 사용하는 새로운 데이터&lt;/li&gt;
      &lt;li&gt;일반화 (Generalization): 새로운 데이터에 대해 올바른 예측을 수행하는 역량&lt;/li&gt;
      &lt;li&gt;지도학습: Target이 주어진 경우 (분류, 회귀)&lt;/li&gt;
      &lt;li&gt;비지도학습: Target이 없는 경우 (군집)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다항식 곡선 근사 (Polynomial Curve Fitting)
    &lt;ul&gt;
      &lt;li&gt;새로운 입력 벡터가 주어졌을 때, 목표값을 예측하는 것&lt;/li&gt;
      &lt;li&gt;확률이론: 예측값의 불확실성을 정량화 하여 표현할 수 있는 수학적 프레임워크 제공&lt;/li&gt;
      &lt;li&gt;결정이론: 확률적 표현을 바탕으로 최적의 예측을 수행할 수 있는 방법론 제공&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;오차함수, 과소적합, 과대적합, 규제화&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;확률변수-random-variable&quot;&gt;확률변수 (Random Variable)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;확률변수 X는 표본의 집합 S의 원소 e를 실수값 X(e) = x에 대응시키는 함수
    &lt;ul&gt;
      &lt;li&gt;대문자 X, Y…: 확률변수&lt;/li&gt;
      &lt;li&gt;소문자 x, y…: 확률변수가 가질 수 있는 값&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;연속확률변수 (Continuous Random Variable)
    &lt;ul&gt;
      &lt;li&gt;누적분포함수 (Cumulative Distribution Function, CDF)&lt;/li&gt;
      &lt;li&gt;누적분포함수와 확률밀도함수 사이의 관계&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;확률변수의 성질
    &lt;ul&gt;
      &lt;li&gt;덧셈법칙&lt;/li&gt;
      &lt;li&gt;곱셈법칙&lt;/li&gt;
      &lt;li&gt;베이즈 확률: 사후확률, 가능도 (우도), 사전확률&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;확률변수의 함수
    &lt;ul&gt;
      &lt;li&gt;확률변수 X의 함수 Y = f(X)도 확률변수 (함수의 함수)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-ml-basics-decision-theory-linear-regression&quot;&gt;2. ML Basics (Decision Theory, Linear Regression)&lt;/h2&gt;

&lt;h2 id=&quot;3-ml-basics-probability-distributions&quot;&gt;3. ML Basics (Probability Distributions)&lt;/h2&gt;

&lt;h2 id=&quot;4-ml-basics-probability-distributions&quot;&gt;4. ML Basics (Probability Distributions)&lt;/h2&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;5-7주차-돌아보기&quot;&gt;5. 7주차 돌아보기&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;기간: 2022. 10. 31 ~ 2022. 11. 04&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 프로그래머스 인공지능 데브코스 4기 6주차 강의 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/07%EC%A3%BC%EC%B0%A8&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 31 Oct 2022 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-7%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-7%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 6주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 6주차 강의에 대한 정리입니다. &lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-인공지능과-기계학습-소개&quot;&gt;1. 인공지능과 기계학습 소개&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;일상 속 인공지능
    &lt;ul&gt;
      &lt;li&gt;음성인식 (Siri), 추천 시스템 (Netfilx), 자율주행&lt;/li&gt;
      &lt;li&gt;실시간 객체 인식 (Face ID), 로봇, 번역 (papago)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 기반의 접근&lt;/li&gt;
  &lt;li&gt;기술에 집중하기 보다는, 인간 중심의 소통이 중요&lt;/li&gt;
  &lt;li&gt;인공지능 == 도구
    &lt;ul&gt;
      &lt;li&gt;도구를 만드는 방법을 배우는 것도 중요하지만, 도구를 사용하는 방법도 배워야 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;figure&gt;
    &lt;img src=&quot;https://itwiki.kr/images/0/0e/ABriefHistoryofAI.png&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;출처: https://itwiki.kr/w/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기계도 학습이 가능한가? 경험을 통해 점진적으로 성능이 향상되는 기계를 만들 수 있다!&lt;/li&gt;
  &lt;li&gt;인공지능: 인간의 학습, 추론, 지각, 자연언어 이해 등의 지능적 능력을 기기로 실현한 기술&lt;/li&gt;
  &lt;li&gt;학습: 경험의 결과로 나타나는, 비교적 지속적인 행동의 변화나 그 잠재력의 변화 또는 지식을 습득하는 과정&lt;/li&gt;
  &lt;li&gt;경험 E를 통해, 주어진 작업 T에 대한, 성능 P의 향상 (E * T = P)&lt;/li&gt;
  &lt;li&gt;인공지능의 주도권 전환
    &lt;ul&gt;
      &lt;li&gt;지식 기반 -&amp;gt; 기계 학습 -&amp;gt; 심층 학습 (표현 학습, Deep learning, Representation learning)&lt;/li&gt;
      &lt;li&gt;데이터 중심 접근 방식으로 전환&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;예측은 회귀 (Regression) 문제와 분류 (Classification) 문제로 나뉨
    &lt;ul&gt;
      &lt;li&gt;회귀는 목표치가 실수, 분류는 종류의 값&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;기계-학습-개념&quot;&gt;기계 학습 개념&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;훈련집합 (Training set)&lt;/li&gt;
  &lt;li&gt;가설: 눈대중으로 데이터 양상이 직선 형태를 보임 -&amp;gt; 모델을 직선으로 선택 가정&lt;/li&gt;
  &lt;li&gt;기계 학습의 훈련 (Train)
    &lt;ul&gt;
      &lt;li&gt;주어진 문제인 예측을 정확하게 할 수 있는 최적의 매개변수를 찾는 과정&lt;/li&gt;
      &lt;li&gt;처음에는 임의의 매개변수로 시작하지만, 개선하여 정량적인 최적 성능 (Performance)에 도달&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;훈련을 마치면, 추론 (Inference)을 수행
    &lt;ul&gt;
      &lt;li&gt;새로운 특징에 대응되는 목표치의 예측에 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기계 학습의 궁극적인 목표
    &lt;ul&gt;
      &lt;li&gt;훈련집합에 없는 새로운 데이터에 대한 오류를 최소화 (새로운 데이터 = 테스트 집합)&lt;/li&gt;
      &lt;li&gt;테스트 집합에 대한 높은 성능을 일반화 (Generalization) 능력이라 부름&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기계 학습의 필수 요소
    &lt;ul&gt;
      &lt;li&gt;학습할 수 있는 데이터가 있어야 함&lt;/li&gt;
      &lt;li&gt;데이터 규칙이 존재해야 함&lt;/li&gt;
      &lt;li&gt;수학적으로 설명이 불가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;차원의 저주 (Curse of Dimensionality): 차원이 높아짐에 따라 발생하는 현실적인 문제들&lt;/li&gt;
  &lt;li&gt;기술 추세
    &lt;ul&gt;
      &lt;li&gt;기계 학습 알고리즘과 응용의 다양화&lt;/li&gt;
      &lt;li&gt;표현 학습이 중요해짐&lt;/li&gt;
      &lt;li&gt;심층 학습이 기계 학습의 주류&lt;/li&gt;
      &lt;li&gt;심층 학습은 현대 인공지능 실현에 핵심 기술&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;인공지능의 단계: 초인공지능, 강인공지능, 약인공지능&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;데이터에-대한-이해&quot;&gt;데이터에 대한 이해&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 수집 -&amp;gt; 모델 정립 (가설) -&amp;gt; 예측&lt;/li&gt;
  &lt;li&gt;기계 학습: 데이터를 설명할 수 있는 학습 모델을 찾아내는 과정&lt;/li&gt;
  &lt;li&gt;주어진 과업에 적합한 다양한 데이터를 충분한 양만큼 수집 =&amp;gt; 과업 성능 향상&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;간단한-기계-학습의-예&quot;&gt;간단한 기계 학습의 예&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;목적 함수 (비용 함수, Objective function, Cost function)&lt;/li&gt;
  &lt;li&gt;과업을 달성하기 위해 모델의 성능이 개선되는지 객관적으로 확인할 수 있는 지표&lt;/li&gt;
  &lt;li&gt;비용 함수의 예 중 하나는 평균제곱오차 (Mean Squared Error, MSE)&lt;/li&gt;
  &lt;li&gt;비용 함수를 최소화 시킬 수 있는 파라미터를 찾는 것이 목표&lt;/li&gt;
  &lt;li&gt;조금 더 현실적인 상황: 실제 세계는 선형 데이터가 아니고 잡음이 섞임 (비선형 모델이 필요)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;모델-선택&quot;&gt;모델 선택&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;과소적합 (Underfitting): 모델의 용량 (자유도)이 작아서 오차가 클 수 밖에 없는 현상
    &lt;ul&gt;
      &lt;li&gt;과소적합을 극복하기 위해서는 더 많은 데이터를 사용하거나, 비선형 모델을 사용하라&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;과잉적합 (Overfitting): 고차원으로 근사한다면, 훈련 집합에 대해 거의 완벽하게 추정 가능
    &lt;ul&gt;
      &lt;li&gt;하지만 새로운 데이터를 예측하는 경우에는 문제가 발생할 수 있음&lt;/li&gt;
      &lt;li&gt;모델의 용량이 크기 때문에 학습 과정에서 잡음까지도 학습해버림&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;따라서 적절한 용량의 모델을 선택하는 모델 선택 작업이 필요함&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;편향 (Bias)과 분산 (변동, Variance) =&amp;gt; trade-off 관계
    &lt;ul&gt;
      &lt;li&gt;훈련집합을 여러 번 수집하여 1~12차에 반복해서 적용하는 실험&lt;/li&gt;
      &lt;li&gt;저차원에서는 오차가 크고, 편향도 큼. 하지만 비슷한 모델을 얻음 (낮은 변동)&lt;/li&gt;
      &lt;li&gt;고차원에서는 오차가 작고, 편향도 작음. 하지만 크게 다른 모델을 얻음 (높은 변동)&lt;/li&gt;
      &lt;li&gt;용량이 작은 모델 (과소적합)은 편향이 크고, 분산이 작음&lt;/li&gt;
      &lt;li&gt;용량이 복잡한 모델 (과대적합)은 편향이 작고, 분산이 큼&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기계 학습의 궁극적인 목표
    &lt;ul&gt;
      &lt;li&gt;낮은 편향과 낮은 분산을 가진 예측 모델을 만드는 것이 목표&lt;/li&gt;
      &lt;li&gt;하지만 모델의 편향과 분산은 상충 관계&lt;/li&gt;
      &lt;li&gt;따라서 편향을 최소로 유지하며, 분산도를 최대로 낮추는 전략이 필요함&lt;/li&gt;
      &lt;li&gt;모델의 용량이 증가한다는 것은 편향이 감소하고, 분산이 증가하는 경향이 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;검증집합을 이용한 모델 선택
    &lt;ul&gt;
      &lt;li&gt;훈련집합과 테스트집합과 다른 별도의 검증집합 (Validation set)을 가진 상황&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;부트스트랩 (Bootstrap)
    &lt;ul&gt;
      &lt;li&gt;임의의 복원 추출 샘플링 (Sampling with replacement) 반복&lt;/li&gt;
      &lt;li&gt;데이터 분포가 불균형 일 때 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;현대 기계 학습의 전략
    &lt;ul&gt;
      &lt;li&gt;용량이 충분히 큰 모델을 선택한 후에,&lt;/li&gt;
      &lt;li&gt;선택한 모델이 정상을 벗어나지 않도록 여러 규제 (Regularization) 기법을 적용
        &lt;ul&gt;
          &lt;li&gt;데이터 확대: 데이터를 많이 수집할수록 일반화 능력이 향상됨&lt;/li&gt;
          &lt;li&gt;가중치 감쇠: 개선된 목적함수를 이용하여 가중치를 작게 조절하는 규제 기법&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;지도-방식에-따른-유형&quot;&gt;지도 방식에 따른 유형&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;지도 학습 (Supervised Learning)
    &lt;ul&gt;
      &lt;li&gt;특징 벡터와 목표치 (정답)가 모두 주어진 상황&lt;/li&gt;
      &lt;li&gt;회귀와 분류 문제로 구분&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;비지도 학습 (Unsupervised Learning)
    &lt;ul&gt;
      &lt;li&gt;특징 벡터는 주어지는데, 목표치가 주어지지 않는 상황 (정답이 없음)&lt;/li&gt;
      &lt;li&gt;군집화 (Clustering), 밀도 추정 (Density estimation), 특징 공간 변환 (PCA) 과업&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;강화 학습 (Reinforcement Learning)
    &lt;ul&gt;
      &lt;li&gt;상대적인 목표치가 주어지는데, 지도 학습과 다른 형태 (보상)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;준지도 학습 (Semi-supervised Learning)
    &lt;ul&gt;
      &lt;li&gt;일부는 특징 벡터와 목표치를 모두 가지지만, 나머지는 특징 벡터만 가지는 상황&lt;/li&gt;
      &lt;li&gt;최근, 대부분의 데이터가 특징 벡터 수집은 쉽지만, 목표치는 수작업이 필요하여 최근 중요성 부각&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;다양한-기준에-따른-유형&quot;&gt;다양한 기준에 따른 유형&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;오프라인 학습과 온라인 학습 (Offline, Online Learning)
    &lt;ul&gt;
      &lt;li&gt;보통은 오프라인 학습을 다룸&lt;/li&gt;
      &lt;li&gt;온라인 학습은 IoT 등에서 추가로 발생하는 데이터 샘플을 가지고 점증적 학습 수행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결정론적 학습과 확률적 학습 (Deterministic, Stochastic Learning)
    &lt;ul&gt;
      &lt;li&gt;결정론적에서는 같은 데이터를 가지고 다시 학습하면 같은 예측 모델이 만들어짐&lt;/li&gt;
      &lt;li&gt;확률적 학습은 학습 과정에서 확률 분포를 사용하므로, 같은 데이터로 학습하면 다른 예측 모델이 나옴&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;분별 모델과 생성 모델 (Discriminative, Generative Models)
    &lt;ul&gt;
      &lt;li&gt;분별 모델은 부류 예측에만 관심. 즉, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(y|x)&lt;/code&gt;의 추정에 관심&lt;/li&gt;
      &lt;li&gt;생성 모델은 P(x) 또는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;P(x|y)&lt;/code&gt;를 추정하여 새로운 샘플을 생성하여 사용할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-기계학습과-수학-리뷰&quot;&gt;2. 기계학습과 수학 리뷰&lt;/h2&gt;

&lt;h3 id=&quot;기계학습에서-수학의-역할&quot;&gt;기계학습에서 수학의 역할&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;수학은 목적함수를 정의하고, 목적함수의 최저점을 찾아주는 최적화 이론 제공&lt;/li&gt;
  &lt;li&gt;최적화 이론에 학습률, 멈춤 조건과 같은 제어를 추가하여 알고리즘 구축&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;벡터와-행렬&quot;&gt;벡터와 행렬&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;백터 (Vector): 샘플을 특징 벡터 (Feature vector)로 표현&lt;/li&gt;
  &lt;li&gt;행렬 (Matrix): 여러 개의 벡터를 담음, 훈련 집합을 담은 행렬을 설계 행렬 (Design matrix)이라고 부름
    &lt;ul&gt;
      &lt;li&gt;전치 행렬 (Transpose matrix): 행 요소와 열 요소를 뒤바꾼 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행렬을 이용하면, 방정식을 간결하게 표현 가능&lt;/li&gt;
  &lt;li&gt;특수 행렬들
    &lt;ul&gt;
      &lt;li&gt;정사각행렬 (정방행렬, Square matrix)&lt;/li&gt;
      &lt;li&gt;대각행렬 (Diagonal matrix)&lt;/li&gt;
      &lt;li&gt;단위행렬 (Identity matrix)&lt;/li&gt;
      &lt;li&gt;대칭행렬 (Symmetrix matrix)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;행렬 연산: 행렬 곱셈, 벡터의 내적 (Inner product)&lt;/li&gt;
  &lt;li&gt;텐서 (Tensor): 3차원 이상의 구조를 가진 숫자 배열 (array)
    &lt;ul&gt;
      &lt;li&gt;0차: 수 (Scalar), 1차: 벡터 (Vector), 2차: 행렬 (Matrix)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;유사도 (Similarity)와 거리 (Distance): 벡터를 기하학적으로 해석 (코사인 유사도)&lt;/li&gt;
  &lt;li&gt;벡터와 행렬의 거리 (크기)를 놈 (Norm)으로 측정
    &lt;ul&gt;
      &lt;li&gt;행렬의 프로베니우스 놈 (Frobenius norm)으로 행렬의 크기 측정 가능&lt;/li&gt;
      &lt;li&gt;1차 놈 (Manhattan distance)&lt;/li&gt;
      &lt;li&gt;2차 놈 (Euclidean distance)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;퍼셉트론의-해석&quot;&gt;퍼셉트론의 해석&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;퍼셉트론 (Perceptron): 1958년에 고안한 분류기 (Classifier) 모델, 활성 함수는 계단 함수 사용&lt;/li&gt;
  &lt;li&gt;다중 퍼셉트론 (Multi-layer perceptron)&lt;/li&gt;
  &lt;li&gt;추론 (Inferring)은 학습을 마친 알고리즘을 현장의 새로운 데이터에 적용하는 작업&lt;/li&gt;
  &lt;li&gt;훈련 (Training)은 훈련 집합의 샘플에 대하여 가장 잘 설명할 수 있는 가중치를 찾아내는 작업
    &lt;ul&gt;
      &lt;li&gt;현대 기계학습에서 심층학습은 퍼셉트론을 여러 층으로 확장하여 만듦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;선형결합과-벡터공간&quot;&gt;선형결합과 벡터공간&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;벡터: 공간 상의 한 점으로 화살표 끝이 벡터의 좌표에 해당&lt;/li&gt;
  &lt;li&gt;선형결합이 만드는 벡터공간: 기저 벡터 a와 b의 선형 결합 (Linear combination)&lt;/li&gt;
  &lt;li&gt;선형결합으로 만들어지는 공간을 벡터공간 (Vector space)이라고 부름&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;역행렬&quot;&gt;역행렬&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;다음의 성질은 서로 필요충분조건
    &lt;ul&gt;
      &lt;li&gt;A는 역행렬을 갖음. 즉, 특이행렬이 아님&lt;/li&gt;
      &lt;li&gt;A는 최대계수를 갖음&lt;/li&gt;
      &lt;li&gt;A의 모든 행과 열이 선형독립임&lt;/li&gt;
      &lt;li&gt;A의 행렬식은 0이 아님&lt;/li&gt;
      &lt;li&gt;A의 고윳값은 모두 0이 아님&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;행렬-분해&quot;&gt;행렬 분해&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;분해 (Decomposition): 정수 3717은 특성이 보이지 않지만, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;3*3*7*59&lt;/code&gt;로 소인수 분해하면 특성이 보임&lt;/li&gt;
  &lt;li&gt;고윳값 (Eigenvalue), 고유 벡터 (Eigenvector)&lt;/li&gt;
  &lt;li&gt;고유 분해 (Eigen-decomposition)는 고윳값과 고유 벡터가 존재하는 정사각행렬에만 적용 가능
    &lt;ul&gt;
      &lt;li&gt;하지만 기계학습에서는 정사각행렬이 아닌 경우의 분해도 필요하기 때문에 고유 분해는 한계가 있음&lt;/li&gt;
      &lt;li&gt;특잇값 분해 (Singular Value Decomposition, SVD) 등장 - 정사각행렬이 아닌 행렬의 역행렬 계산에 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;확률과-통계&quot;&gt;확률과 통계&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기계학습이 처리할 데이터는 불확실한 세상에서 발생하므로, 불확실성 (Uncertainty)을 다루는 확률 및 통계가 필수&lt;/li&gt;
  &lt;li&gt;확률 변수 (Random variable)&lt;/li&gt;
  &lt;li&gt;확률 분포 (Probability distribution): 확률질량함수 &amp;amp; 이산확률변수, 확률밀도함수 &amp;amp; 연속확률변수&lt;/li&gt;
  &lt;li&gt;확률 벡터 (Random vector): 확률변수를 요소로 갖음&lt;/li&gt;
  &lt;li&gt;확률 기초: 곱 규칙 (Product rule), 합 규칙 (Sum rule)
    &lt;ul&gt;
      &lt;li&gt;조건부 확률, 연쇄법칙 (Chain rule), 독립 (Independence), 조건부 독립, 기댓값&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;베이즈-정리와-기계학습&quot;&gt;베이즈 정리와 기계학습&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;베이즈 정리 (Bayes’ rule)&lt;/li&gt;
  &lt;li&gt;사후 (Posteriori) 확률 = 우도 (Likelihood) 확률 * 사전 (Prior) 확률&lt;/li&gt;
  &lt;li&gt;사후 확률을 직접 추정하는 일은 아주 단순한 경우를 빼고 거의 불가능&lt;/li&gt;
  &lt;li&gt;따라서 베이즈 정리를 이용하여 추정&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;최대-우도&quot;&gt;최대 우도&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;매개변수 (모수, Parameter)를 모르는 상황에서 매개변수를 추정하는 문제&lt;/li&gt;
  &lt;li&gt;어떤 확률변수의 관찰된 값들을 토대로 그 확률변수의 매개변수를 구하는 방법 (최대 우도, Maximum Likelihood)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;평균과-분산&quot;&gt;평균과 분산&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터의 요약 정보로서 평균 (Mean)과 분산 (Variance)&lt;/li&gt;
  &lt;li&gt;평균 벡터 (치우침 정도)와 공분산 행렬 (Covariance matrix) - 확률변수의 상관정도&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;유용한-확률분포&quot;&gt;유용한 확률분포&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;가우시안 분포 (Gaussian distribution): 평균과 분산으로 정의
    &lt;ul&gt;
      &lt;li&gt;다차원 가우시안 분포: 평균 벡터와 공분산행렬로 정의&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;베르누이 분포 (Bernoulli distribution): 성공 확률 p이고, 실패 확률이 1-p인 분포&lt;/li&gt;
  &lt;li&gt;이항 분포 (Binomial distribution): 성공 확률이 p인 베르누이 실험을 m번 수행할 때, 성공할 횟수의 확률분포&lt;/li&gt;
  &lt;li&gt;로지스틱 시그모이드 함수 (Logistic Sigmoid function): 일반적으로 베르누이 분포의 매개변수를 조정을 통해 얻음&lt;/li&gt;
  &lt;li&gt;소프트플러스 함수 (Softplus function): 정규 분포의 매개변수의 조정을 통해 얻음&lt;/li&gt;
  &lt;li&gt;지수 분포 (Exponential distribution)&lt;/li&gt;
  &lt;li&gt;라플라스 분포 (Laplace distribution)&lt;/li&gt;
  &lt;li&gt;디랙 분포 (Dirac distribution)&lt;/li&gt;
  &lt;li&gt;혼합 분포들 (Mixture distribution): 3개의 요소를 가진 가우시안 혼합 분포&lt;/li&gt;
  &lt;li&gt;변수 변환 (Change of variables): 기존 확률변수를 새로운 확률변수로 바꾸는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;정보이론&quot;&gt;정보이론&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;정보이론: 사건 (Event)이 지닌 정보를 정량화 할 수 있을까?
    &lt;ul&gt;
      &lt;li&gt;정보이론의 기본 원리: 확률이 작을수록 많은 정보&lt;/li&gt;
      &lt;li&gt;자주 발생하는 사건보다, 잘 일어나지 않는 사건 (Unlikely event)의 정보량 (Informative)이 많음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;정보이론과 확률통계는 많은 교차점을 가짐, 확률통계는 기계학습의 기초적인 근간 제공&lt;/li&gt;
  &lt;li&gt;정보이론 관점에서도 기계학습을 접근 가능: 엔트로피, 교차 엔트로피, KL 발산, 상대 엔트로피&lt;/li&gt;
  &lt;li&gt;자기 정보 (Self information): 사건 (메시지)의 정보량&lt;/li&gt;
  &lt;li&gt;엔트로피 (Entropy): 확률변수 x의 불확실성을 나타내는 엔트로피, 모든 사건 정보량의 기댓값으로 표현
    &lt;ul&gt;
      &lt;li&gt;모든 사건이 동일한 확률을 가지 때 즉, 불확실성이 가장 높은 경우에 엔트로피가 최고임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;교차 엔트로피 (Cross entropy): 두 확률분포 P와 Q 사이의 교차 엔트로피
    &lt;ul&gt;
      &lt;li&gt;심층학습의 손실함수로 많이 사용&lt;/li&gt;
      &lt;li&gt;교차 엔트로피를 손실함수로 사용하는 경우, KL 발산의 최소화 함과 동일&lt;/li&gt;
      &lt;li&gt;KL 다이버전스: 두 확률분포 사이의 거리를 계산할 때 주로 사용&lt;/li&gt;
      &lt;li&gt;가지고 있는 데이터 분포 P(X)와 추정한 데이터 분포 Q(X) 간의 차이를 최소화하는데 교차 엔트로피 사용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;최적화&quot;&gt;최적화&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기계학습의 최적화는 단지 훈련집합이 주어지고,&lt;/li&gt;
  &lt;li&gt;훈련집합에 따라 정해지는 목적함수의 최저점으로 만드는 모델의 매개변수를 찾아야 함&lt;/li&gt;
  &lt;li&gt;주로 확률적 경사 하강법 (Stochastic gradient descent, SGD) 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;매개변수-공간의-탐색&quot;&gt;매개변수 공간의 탐색&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;특징 공간의 높은 차원에 비해 훈련 집합의 크기가 작아서 참인 확률분포를 구하는 일은 불가능&lt;/li&gt;
  &lt;li&gt;따라서 기계학습은 적절한 모델 (가설)을 선택하고, 목적함수를 정의하여 모델의 매개변수 공간을 탐색&lt;/li&gt;
  &lt;li&gt;그리고 목적함수가 최저가 되는 최적점을 찾는 전략 사용&lt;/li&gt;
  &lt;li&gt;특징 공간에서 해야 하는 일을 모델의 매개변수 공간에서 하는 일로 대치
    &lt;ul&gt;
      &lt;li&gt;최적화 문제 해결: 낱낱 탐색 (Exhaustive search) 알고리즘, 무작위 탐색 (Random search) 알고리즘
        &lt;ul&gt;
          &lt;li&gt;경사 하강법 (미분 활용)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;미분&quot;&gt;미분&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;미분에 의한 최적화: 1차 도함수는 함수의 기울기 (경사), 즉 값이 커지는 방향을 지시&lt;/li&gt;
  &lt;li&gt;편미분 (Partial derivative): 변수가 복수인 함수의 미분, 미분 값이 이루는 벡터를 경사도라고 부름&lt;/li&gt;
  &lt;li&gt;기계학습에서 편미분: 매개변수 집합은 복수 매개변수이므로, 편미분을 사용&lt;/li&gt;
  &lt;li&gt;최적화는 예측 단계가 아닌, 학습 단계에서 필요함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;경사-하강-알고리즘&quot;&gt;경사 하강 알고리즘&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;경사 하강법 (Gradient descent)는 낮은 곳을 찾아가는 원리
    &lt;ul&gt;
      &lt;li&gt;함수의 기울기 (경사)를 구하여 기울기가 낮은 쪽으로 반복적으로 이동하여 최솟값에 도달&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;집단 (무리, Batch) 경사 하강 알고리즘
    &lt;ul&gt;
      &lt;li&gt;샘플의 경사도를 구하고 평균한 후에 한꺼번에 갱신&lt;/li&gt;
      &lt;li&gt;훈련 집합 전체를 다 봐야 갱신을 일어나기 때문에 학습 과정이 오래 걸린다는 단점 존재&lt;/li&gt;
      &lt;li&gt;정확한 방향으로 수렴하긴 하지만, 속도가 느리다는 단점&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;확률론적 경사 하강 (SGD, Stochastic Gradient Descent) 알고리즘
    &lt;ul&gt;
      &lt;li&gt;한 샘플 혹은 작은 집단 (무리, Mini-batch)의 경사도를 계산한 후 즉시 갱신&lt;/li&gt;
      &lt;li&gt;수렴이 다소 해맬 수도 있긴 하지만, 속도가 빠르다는 장점&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-ml-basics---e2e&quot;&gt;3. ML Basics - E2E&lt;/h2&gt;

&lt;h3 id=&quot;end-to-end-머신러닝-프로젝트&quot;&gt;End to End 머신러닝 프로젝트&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;큰 그림 보기
    &lt;ul&gt;
      &lt;li&gt;문제 정의: 지도학습 &amp;amp; 비지도학습, 분류문제 &amp;amp; 회귀문제, 배치학습 &amp;amp; 온라인학습&lt;/li&gt;
      &lt;li&gt;성능측정지표: RMSE 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터를 구하기
    &lt;ul&gt;
      &lt;li&gt;데이터 가져오기&lt;/li&gt;
      &lt;li&gt;데이터 구조 훑어보기&lt;/li&gt;
      &lt;li&gt;train, test 데이터셋 나누기 (데이터 변경이 생겨도 일관성 있는 기준을 유지하도록 하라)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터로부터 통찰을 얻기 위해 탐색하고, 시각화 해보기
    &lt;ul&gt;
      &lt;li&gt;상관관계 확인&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;머신러닝 알고리즘을 위해 데이터를 준비하기
    &lt;ul&gt;
      &lt;li&gt;데이터 수동 변환 보다는, 함수를 만들어서 자동 변환하는 것이 더 좋음&lt;/li&gt;
    &lt;/ul&gt;
    &lt;ul&gt;
      &lt;li&gt;새로운 데이터에 대한 변환을 손쉽게 재생산 할 수 있음&lt;/li&gt;
      &lt;li&gt;향후 재사용할 수 있는 라이브러리 구축 가능
      - 데이터 정제 (Data Cleansing)&lt;/li&gt;
      &lt;li&gt;누락된 특성 다루는 방법: 행 제거, 열 제거, 특정 값으로 채우기 (0, 평균, 중간값 등)&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;SimpleImputer&lt;/code&gt; 함수 활용하기&lt;/li&gt;
      &lt;li&gt;One hot 인코딩 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OrdinalEncoder, OneHotEncoder&lt;/code&gt; 함수 활용하기)
      - 특성 스케일링 (Feature Scaling): Min max scaling, Standardization
      - 변환 파이프라인 (Transformation Pipeline): 순차적 변환 시, 사용 (Pipeline class)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;모델을 선택하고 훈련시키기&lt;/li&gt;
  &lt;li&gt;모델을 상세하게 조정하기&lt;/li&gt;
  &lt;li&gt;솔루션을 제시하기&lt;/li&gt;
  &lt;li&gt;시스템을 론칭하고, 모니터링 및 유지보수 하기&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;4-ml-basics---linear-algebra&quot;&gt;4. ML Basics - Linear Algebra&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;행렬의 곱셈&lt;/li&gt;
  &lt;li&gt;중요한 연산과 성질들 (정방행렬, 삼각행렬, 대각행렬, 단위행렬)&lt;/li&gt;
  &lt;li&gt;Norms&lt;/li&gt;
  &lt;li&gt;선형독립과 Rank&lt;/li&gt;
  &lt;li&gt;역행렬, 직교행렬&lt;/li&gt;
  &lt;li&gt;행렬식&lt;/li&gt;
  &lt;li&gt;이차형식&lt;/li&gt;
  &lt;li&gt;고유값, 고유벡터&lt;/li&gt;
  &lt;li&gt;행렬미분&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;5-6주차-돌아보기&quot;&gt;5. 6주차 돌아보기&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;기간: 2022. 10. 24 ~ 2022. 10. 29&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 프로그래머스 인공지능 데브코스 4기 6주차 강의 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/06%EC%A3%BC%EC%B0%A8&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 24 Oct 2022 00:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-6%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-6%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>AI 학습용 데이터 라벨링 교육 - 데이터 기획 과정</title>
        <description>&lt;p&gt;AI Data에서 제공하는 AI 학습용 데이터 라벨링 교육의 데이터 기획 과정에 대한 강의 기록. &lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-인공지능-학습용-데이터-기획-개요&quot;&gt;1. 인공지능 학습용 데이터 기획 개요&lt;/h2&gt;

&lt;h3 id=&quot;데이터-기획-개요-rfp&quot;&gt;데이터 기획 개요 RFP&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;어떤 데이터가 필요한가?&lt;/li&gt;
  &lt;li&gt;어떻게 데이터를 수집 및 획득할 것인가?&lt;/li&gt;
  &lt;li&gt;어떻게 데이터를 정제할 것인가?&lt;/li&gt;
  &lt;li&gt;어떠한 라벨을 이용하여 가공할 것인가?&lt;/li&gt;
  &lt;li&gt;완성된 인공지능 학습용 데이터를 어떻게 활용할 수 있는가?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;데이터-기획이-왜-필요한지&quot;&gt;데이터 기획이 왜 필요한지?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기술 구현을 위한 데이터 수요가 폭증&lt;/li&gt;
  &lt;li&gt;산업 및 공공의 발전을 견인하는 데이터 기획 필요성이 증대&lt;/li&gt;
  &lt;li&gt;똑똑한 인공지능을 만들기 위해서는 품질 좋은 대량의 학습 데이터가 필요함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rfp-request-for-proposal-제안요청서&quot;&gt;RFP (Request For Proposal, 제안요청서)&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;과제의 수행에 필요한 요구사항을 체계적으로 정리&lt;/li&gt;
  &lt;li&gt;사용자의 제안이 잘 실행되고 있는지 판단하기 쉽게 만들어줌&lt;/li&gt;
  &lt;li&gt;RFP가 구체적일수록 제안서의 품질이 높아진다고 할 수 있음&lt;/li&gt;
  &lt;li&gt;RFP = 제안서 = 계약서&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;인공지능-학습용-데이터-구축-rfp&quot;&gt;인공지능 학습용 데이터 구축 RFP&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 개요: 어떤 데이터?, 무엇을 위한 것?, 어떻게 구축?, 얼마만큼 구축?&lt;/li&gt;
  &lt;li&gt;데이터 구축 목적: 연구 목적, 산업 목적, 활용 방안&lt;/li&gt;
  &lt;li&gt;데이터 구축 방법: 데이터 구성, 수집 장비 및 방법, 가공 방법, 비식별화 방법&lt;/li&gt;
  &lt;li&gt;데이터 규모: 데이터 수량 및 형태, 비용 산정&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-인공지능-학습용-데이터-기획-수행-방법&quot;&gt;2. 인공지능 학습용 데이터 기획 수행 방법&lt;/h2&gt;

&lt;h3 id=&quot;개요&quot;&gt;개요&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;유형 및 도메인 선정: 글로벌 기술 동향 및 시장 전망 참고&lt;/li&gt;
  &lt;li&gt;과제 발굴: 산학연 수요에 맞는 데이터 과제 발굴 (Top-down, Bottom-up)&lt;/li&gt;
  &lt;li&gt;RFP 작성: RFP 자문단 운영 및 과제별 요구사항 구체화&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;데이터-구축-공정-단계별-고려사항&quot;&gt;데이터 구축 공정 단계별 고려사항&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 생애주기는 계획, 구축, 운영, 활용 영역으로 구분&lt;/li&gt;
  &lt;li&gt;구축 프로세스 품질 관리&lt;/li&gt;
  &lt;li&gt;구축 데이터 품질 관리&lt;/li&gt;
  &lt;li&gt;개방 데이터 품질 관리&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;데이터-활용-목적-설정&quot;&gt;데이터 활용 목적 설정&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;범용 데이터 학습용 데이터 구축&lt;/li&gt;
  &lt;li&gt;세부 도메인의 특수 목적의 인공지능 학습용 데이터 구축&lt;/li&gt;
  &lt;li&gt;국가 전략과의 Alignment&lt;/li&gt;
  &lt;li&gt;AI 학습용 데이터의 활용성 고려&lt;/li&gt;
  &lt;li&gt;AI 기술 발전 트렌드를 고려&lt;/li&gt;
  &lt;li&gt;공공성&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;목적에-맞는-데이터-조사&quot;&gt;목적에 맞는 데이터 조사&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 유형은?&lt;/li&gt;
  &lt;li&gt;데이터 규모는?&lt;/li&gt;
  &lt;li&gt;원천 데이터와 라벨은 어떻게 구성해야?&lt;/li&gt;
  &lt;li&gt;개인정보에 대한 비식별화가 필요한지?&lt;/li&gt;
  &lt;li&gt;중복성 조사도 중요!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-인공지능-학습용-데이터&quot;&gt;3. 인공지능 학습용 데이터&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;인공지능 서비스는 데이터를 기반으로 모델을 생성하고, 최종 서비스 제공
    &lt;ol&gt;
      &lt;li&gt;데이터 설계: 데이터 구축 공정을 개발하여 데이터 작업자에게 제공&lt;/li&gt;
      &lt;li&gt;데이터 수집: 가공할 원천 데이터를 온오프라인 수집, 제작, 축적&lt;/li&gt;
      &lt;li&gt;데이터 가공: AI가 인지하고 판단할 정보를 라벨링&lt;/li&gt;
      &lt;li&gt;데이터 확장: 고차원 정보를 추가하여 데이터 정확도, 규모 확대&lt;/li&gt;
      &lt;li&gt;데이터 검증: 데이터 품질을 정기적으로 검증, 적합성 평가&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-구축-공정-개요&quot;&gt;4. 구축 공정 개요&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;임무정의: 구축 계획서&lt;/li&gt;
  &lt;li&gt;데이터 획득: 원시 데이터&lt;/li&gt;
  &lt;li&gt;데이터 정제: 원천 데이터&lt;/li&gt;
  &lt;li&gt;데이터 라벨링: 라벨링 데이터&lt;/li&gt;
  &lt;li&gt;데이터 학습: 학습 데이터셋&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-학습용-데이터-가치-평가&quot;&gt;5. 학습용 데이터 가치 평가&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;시장 가치: 시장의 수요 (필요성)&lt;/li&gt;
  &lt;li&gt;기술: 산업 파급 효과, 데이터 구축 용이성, AI 서비스 기술 구현 가능성, 법제도적 제약&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;정책: 공공성&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;획득 시 개인정보가 포함되어 있는 경우&lt;/li&gt;
  &lt;li&gt;획득 시 저작권, 지적재산권, 초상권 등 이용에 제한 있는 경우&lt;/li&gt;
  &lt;li&gt;데이터를 직접 제작해야 하는 경우&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: AI Data 2022년 인공지능 학습용 데이터 라벨링 전문 교육 -&amp;gt; &lt;a href=&quot;http://aidata.elancer.co.kr/student/main.php&quot;&gt;강의 소개 홈페이지&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 17 Oct 2022 13:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A15/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A15/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>데이터 라벨링</category>
        
        
        <category>Education</category>
        
        <category>인공지능 학습용 데이터 라벨링 교육</category>
        
      </item>
    
      <item>
        <title>AI 학습용 데이터 라벨링 교육 - 보안 과정</title>
        <description>&lt;p&gt;AI Data에서 제공하는 AI 학습용 데이터 라벨링 교육의 보안 과정에 대한 강의 기록. &lt;br /&gt;
데이터 품질 오남용을 예방하기 위하여 보안 교육을 받음.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-인공지능-데이터-보호의-개요&quot;&gt;1. 인공지능 데이터 보호의 개요&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;인공지능 데이터 보호
    &lt;ul&gt;
      &lt;li&gt;프라이버시 침해, 데이터 유출로 인한 피해를 막기 위한 방법&lt;/li&gt;
      &lt;li&gt;학습용 데이터를 토대로 만들어진 인공지능 모델에 대한 보호&lt;/li&gt;
      &lt;li&gt;학습용 데이터 품질에 있어 오남용 (Abuse)을 예방하기 위한 신뢰성 프레임워크 적용 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;인공지능 데이터 보호의 필요성&lt;/li&gt;
  &lt;li&gt;인공지능 데이터 보호의 목적&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;학습 데이터셋의 오남용 방지를 위한 신뢰성 확보의 필요성&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;AI 서비스 ‘이루다’의 개인정보 유출 사례: ‘이루다’의 혐오표현이 학습되어 데이터 보호가 되지 못함&lt;/li&gt;
  &lt;li&gt;AI 서비스 ‘심심이’의 부적절한 학습 데이터셋을 이용한 결과: 성차별적인 문제를 일으킴&lt;/li&gt;
  &lt;li&gt;AI를 이용한 정치인 얼굴 합성 변조 사례&lt;/li&gt;
  &lt;li&gt;AI를 개발을 위한 이미지 무단 도용 사례: 개인정보 오남용 페이스북의 얼굴 인식, ‘태그’ 중단&lt;/li&gt;
  &lt;li&gt;페이스북의 개인정보 오남용 사례 기술: 얼굴인식 서식 (템플릿) 사례 분석&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-인공지능-데이터-보안-항목&quot;&gt;2. 인공지능 데이터 보안 항목&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;인공지능 학습용 데이터 구축 프로세스
    &lt;ul&gt;
      &lt;li&gt;임무 정의 -&amp;gt; 데이터 획득 및 수집 -&amp;gt; 데이터 정제 -&amp;gt; 데이터 라벨링 -&amp;gt; 데이터 학습&lt;/li&gt;
      &lt;li&gt;데이터 획득 및 수집, 데이터 정제 단계에서는 개인정보나 민감정보 등에 대한 검토 및 수정 필요&lt;/li&gt;
      &lt;li&gt;데이터 라벨링 단계에서는 허수에 의한 거짓 데이터를 학습하는 행위 등을 조심해야 함&lt;/li&gt;
      &lt;li&gt;데이터 학습 단계에서는 과적합 학습 배제가 필요함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-인공지능-학습용-데이터-도메인-및-유형&quot;&gt;3. 인공지능 학습용 데이터 도메인 및 유형&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;데이터&lt;/strong&gt;란, 수, 영상, 단어 등의 형태로 이루어진 의미 단위로 정보를 구성하는 자료&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;학습용 데이터&lt;/strong&gt;란, 머신러닝, 딥러닝 등 AI 모델 학습을 위해 활용되는 데이터를 총칭&lt;/li&gt;
  &lt;li&gt;이미지: 정사각형 모양의 아주 작은 픽셀들의 집합
    &lt;ul&gt;
      &lt;li&gt;축소하거나, 확대하면 이미지의 질이 손상&lt;/li&gt;
      &lt;li&gt;한 면적에 픽셀을 얼마나 넣는가에 따라 해상도, 용량에 영향&lt;/li&gt;
      &lt;li&gt;JPG, PNG, TIFF, GIF 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;동영상: 여러 개의 정지 사진을 연속적으로 보여주는 것
    &lt;ul&gt;
      &lt;li&gt;하나의 정지 사진이 프레임 (Frame)&lt;/li&gt;
      &lt;li&gt;초당 프레임 (Frame per Second)가 높을수록 부드럽게 영상이 재생&lt;/li&gt;
      &lt;li&gt;AVI, MP4, WMV, MKV 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;텍스트: 이미지, 영상, 오디오 등 아무것도 포함되어 있지 않고 순수하게 글만 있는 데이터
    &lt;ul&gt;
      &lt;li&gt;글꼴, 기울임, 글씨 크기 등과 같은 데이터도 포함되어 있지 않음&lt;/li&gt;
      &lt;li&gt;글을 추출하는 파싱과 데이터에 포함되어 있는 단어를 숫자로 표현 (임베딩)하여 사용&lt;/li&gt;
      &lt;li&gt;TXT, CSV, XML, HTML 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;오디오: 소리, 진동, 파형에 대한 정보를 포함하고 있는 데이터
    &lt;ul&gt;
      &lt;li&gt;물체의 진동을 통해서 소리가 발생하고 전달됨&lt;/li&gt;
      &lt;li&gt;물체의 진동을 파형으로 표현하고 연속적인 그래프로 표현할 수 있음&lt;/li&gt;
      &lt;li&gt;MP3, ACC&amp;lt; FLAC, WAV 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;센서와 통계: 다양한 센서를 통해서 얻은 정보를 디지털 값으로 변환한 데이터
    &lt;ul&gt;
      &lt;li&gt;환경 변화나 사건을 감지하여 다른 전자장치에서 감지된 정보를 인식할 수 있게 해주는 시스템&lt;/li&gt;
      &lt;li&gt;임베디드 시스템을 통해 출력 값을 숫자로 받거나, 파형의 형태로 출력할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;객체 생성 및 영상 제작: 영상에 있는 물체 혹은 사람이 무엇인지 인지하기 위한 데이터
    &lt;ul&gt;
      &lt;li&gt;경계 박스를 표시하여 객체를 상자 안에 가둠&lt;/li&gt;
      &lt;li&gt;경계 박스에 있는 객체에 대한 자세한 라벨을 달 수 있고, 비슷한 객체를 생성할 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-인공지능-학습용-데이터-획득수집-시-보안&quot;&gt;4. 인공지능 학습용 데이터 획득/수집 시 보안&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 수집부터 시작하는 Abuse를 예방하기 위한 보안
    &lt;ul&gt;
      &lt;li&gt;데이터 버전 관리, 기관 인증, 생명주기 등이 함께 관리되어야 함&lt;/li&gt;
      &lt;li&gt;따라서 데이터 거버넌스 프레임워크가 필요하다고 할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 거버넌스: 데이터의 가용성, 유용성, 통합성, 보안성을 관리하기 위한 정책 및 프로세스를 수립하는 것
    &lt;ul&gt;
      &lt;li&gt;유용한 데이터 유형 및 품질 표준 정의&lt;/li&gt;
      &lt;li&gt;데이터 관리에 대한 역할 할당 및 책임 정의&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 거버넌스 구현 계획
    &lt;ul&gt;
      &lt;li&gt;데이터의 가용성 보장&lt;/li&gt;
      &lt;li&gt;데이터의 무결성 보장&lt;/li&gt;
      &lt;li&gt;데이터 정책에 대한 책임 및 준수 강화&lt;/li&gt;
      &lt;li&gt;지속적인 피드백 및 모니터링&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;데이터 거버넌스를 위해서 메타 데이터 관리가 필요함&lt;/li&gt;
  &lt;li&gt;메타 데이터 관리를 위한 데이터 레이크&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-인공지능-학습-데이터와-민감정보-비식별화&quot;&gt;5. 인공지능 학습 데이터와 민감정보 비식별화&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;개인정보: 특정 개인에 관한 정보, 개인을 알아볼 수 있게 하는 정보 (이름, 주민등록번호 등)&lt;/li&gt;
  &lt;li&gt;가명정보: 추가 정보의 사용 없이는 특정 개인을 알아볼 수 없게 조치한 정보&lt;/li&gt;
  &lt;li&gt;익명정보: 더이상 개인을 알아볼 수 없게 복원 불가능할 정도로 조치한 정보&lt;/li&gt;
  &lt;li&gt;개인정보 비식별화: 개인정보에서 개인식별 요소를 제거하여 특정 개인을 알아볼 수 없는 형태로 만드는 조치
    &lt;ul&gt;
      &lt;li&gt;가명처리, 총계처리, 데이터 삭제, 데이터 범주화, 데이터 마스킹 등 여러가지 기법을 단독 또는 복합적으로 사용&lt;/li&gt;
      &lt;li&gt;가명처리 기법만 단독 활용된 경우에는 충분한 비식별화 조치로 보기 어려움&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;비식별화 관련 대표 모델
    &lt;ul&gt;
      &lt;li&gt;k-익명성: 단순 삭제, 헤드라인 간단하게. 한 개인이 k명의 다른 사람과 구별되지 않아야 함&lt;/li&gt;
      &lt;li&gt;L-다양성: 데이터 테이블의 필드값이 적어도 L개의 다양한 민감 정보를 가지고 있어야 함&lt;/li&gt;
      &lt;li&gt;T-근접성: 민감한 정보의 분포와 전체 데이터의 민감한 정보의 분포 차이를 T 이하로 만들어서 프라이버시 보호&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-비식별화-오픈소스-도구&quot;&gt;6. 비식별화 오픈소스 도구&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;ARX (Data Anonymization Tool): 텍스트 기반 비식별화 도구 제공, 유료로 이미지 영상 비식별화 지원&lt;/li&gt;
  &lt;li&gt;Amnesia: 텍스트 기반 비식별화 도구 제공, 유료로 이미지 영상 비식별화 지원&lt;/li&gt;
  &lt;li&gt;sdcMicro: 텍스트 기반 비식별화 도구 제공, 유료로 이미지 영상 비식별화 지원&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;7-데이터-관리-클라이언트-보안&quot;&gt;7. 데이터 관리 클라이언트 보안&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;클라이언트의 보안
    &lt;ul&gt;
      &lt;li&gt;사용자 권한과 접근을 제어하고, 로그 기록 등으로 내부 및 외부 접근을 통제&lt;/li&gt;
      &lt;li&gt;인증 (Authentication): 접근 통제 요소, 식별, 인증, 인가, 책임추적성&lt;/li&gt;
      &lt;li&gt;데이터 보호: 허락되지 않은 사용자나 시스템 접근을 통제하여 데이터 노출을 막는 기밀성 유지 필요
        &lt;ul&gt;
          &lt;li&gt;기밀성 (Confidentiality), 무결성 (Integrity), 가용성 (Availability)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;데이터 암호와: 데이터 송수신시에는 암호화가 필수 복호화 되지 않도록 암호화&lt;/li&gt;
      &lt;li&gt;접근 통제: 인증된 사용자가 접근할 수 있는 데이터를 통제하는 것&lt;/li&gt;
      &lt;li&gt;모니터링: 접근 통제에 대한 정책을 계속해서 모니터링 하고 알림을 수신 받아 이상 징후 식별, 탐지&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;8-과적합-overfitting&quot;&gt;8. 과적합 (Overfitting)&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;과대적합이라고도 하고, 인공지능 모델을 학습할 때, 인공지능 학습용 데이터를 과하게 학습하는 것&lt;/li&gt;
  &lt;li&gt;학습용 데이터에서 최적의 결과를 만들었지만, 인공지능 학습용 데이터 외 새로운 데이터에서는 오차가 커지는 문제&lt;/li&gt;
  &lt;li&gt;학습이 충분히 이뤄지지 않은 과소적합 (Underfitting)과 반대되는 개념&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;과적합-방지-방법&quot;&gt;과적합 방지 방법&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 증식: 학습용 데이터를 추가적으로 더 수집하는 것
    &lt;ul&gt;
      &lt;li&gt;이미지 데이터: 이미지 회전, 세로 및 가로로 늘이기, 이미지 상하좌우 반전&lt;/li&gt;
      &lt;li&gt;텍스트 데이터: 역번역, 특정 단어 유의어 교체, 임의의 단어를 삽입하거나 삭제&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;학습 데이터의 대표성: 실제 세상의 데이터 표본으로 여길 수 있을 만큼의 통계적 유사성을 가져야 함&lt;/li&gt;
  &lt;li&gt;전이학습 (Transfer Learning): 데이터의 다양성 및 대표성을 보완할 수 있는 보조적인 기법&lt;/li&gt;
  &lt;li&gt;조기 종료: 학습용 데이터에서 학습 데이터 외에 검증 데이터를 준비하여 검증 데이터에 대한 오차를 계산하고, 오차가 감소하다가 다시 증가하는 구간에서 학습 조기 종료&lt;/li&gt;
  &lt;li&gt;가중치 규제: 딥러닝에서 사용되는 과적합 방지 방법 (손실함수에 패널티를 추가)
    &lt;ul&gt;
      &lt;li&gt;L1: 가중치들의 절댓값을 손실 함수에 추가하는 방법&lt;/li&gt;
      &lt;li&gt;L2: 모든 가중치의 제곱합을 손실 함수에 추가하는 방법&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dropout: 딥러닝 학습을 진행할 때, 생성되는 노드들을 무작위로 비활성화 시키는 방법&lt;/li&gt;
  &lt;li&gt;DropConnect: 랜덤으로 노드를 비활성화 하는 것이 아닌, 가중치를 0으로 만들어서 가중치 값을 생략&lt;/li&gt;
  &lt;li&gt;노이즈 추가: 인공지능 학습용 데이터 외에 학습에 방해가 될 수 있는 요소를 일부러 넣어주는 방법&lt;/li&gt;
  &lt;li&gt;배치: 학습용 데이터 중 일부 데이터셋을 뜻하는 것 (Batch 단위로 학습하여 속도를 개선)&lt;/li&gt;
  &lt;li&gt;정규화: 학습이 진행 될 때, 입력 값의 범위가 너무 크면 계산하는데 시간이 오래 걸리고, 오차가 커짐&lt;/li&gt;
  &lt;li&gt;배치 정규화: 배치 단위로 정규화 시키는 것&lt;/li&gt;
  &lt;li&gt;원핫 인코딩: 답에는 1, 나머지는 0으로 표현하는 방식&lt;/li&gt;
  &lt;li&gt;라벨 스무딩: 확실하게 0과 1로 라벨링 된 값을 0에서 1 사이의 값을 변형하여 라벨을 부드럽게 만듦&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: AI Data 2022년 인공지능 학습용 데이터 라벨링 전문 교육 -&amp;gt; &lt;a href=&quot;http://aidata.elancer.co.kr/student/main.php&quot;&gt;강의 소개 홈페이지&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 17 Oct 2022 13:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A14/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A14/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>데이터 라벨링</category>
        
        
        <category>Education</category>
        
        <category>인공지능 학습용 데이터 라벨링 교육</category>
        
      </item>
    
      <item>
        <title>AI 학습용 데이터 라벨링 교육 - 이미지/영상 입문 과정</title>
        <description>&lt;p&gt;AI Data에서 제공하는 AI 학습용 데이터 라벨링 교육의 이미지/영상 입문 과정에 대한 강의 기록. &lt;br /&gt;
이미지와 영상 분야에서 데이터 라벨링 기법과 적용 사례를 학습하고, datamaker의 라벨링 도구 사용해보기.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-데이터-라벨러-직무-및-전망&quot;&gt;1. 데이터 라벨러 직무 및 전망&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 라벨러: 데이터 정제 및 데이터 라벨링을 수행하는 사람&lt;/li&gt;
  &lt;li&gt;AI가 학습할 수 있도록 데이터에 정보 (어노테이션)를 부착하는 활동&lt;/li&gt;
  &lt;li&gt;인공지능은 라벨링 데이터를 바탕으로 개발되기 때문에 데이터 라벨러의 역할은 매우 중요&lt;/li&gt;
  &lt;li&gt;데이터 라벨러는 체계적인 교육과정 운영과 산업 현장에 필요로 하는 인재 양성을 위해 NCS 정의가 진행 중&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;데이터 라벨러, 데이터 검수자, QM, PM&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;인공지능 개발 프로세스 이해, 프로젝트 이해, 라벨링 가이드라인 이해, 데이터 라벨링 수행, 관리자와 소통&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-데이터-학습-처리-과정&quot;&gt;2. 데이터 학습 처리 과정&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;원시 데이터 (Raw data): 기계학습을 목적으로 획득 단계에서 수집 및 생성한 음성, 이미지, 영상, 텍스트 등의 데이터&lt;/li&gt;
  &lt;li&gt;원천 데이터 (Source data): 원시 데이터를 라벨링 공정에 투입하기 위해 필요한 전처리 등 정제 작업을 수행한 데이터
    &lt;ul&gt;
      &lt;li&gt;원천 데이터는 라벨링 데이터가 부여되지 않은 상태의 데이터&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기계학습 (Machine Learning): 인간이 자연적으로 수행하는 학습 능력을 컴퓨터에서 실현하려는 기술, 방법&lt;/li&gt;
  &lt;li&gt;인공지능 (Artificial Intelligence): 인간의 지능이 갖는 학습, 추리, 적응 등의 기능을 갖춘 컴퓨터 시스템&lt;/li&gt;
  &lt;li&gt;데이터 라벨링 (Data labeling): 기계학습에 활용되도록 기능, 목적에 부합하는 정보를 원천 데이터에 부착하는 활동&lt;/li&gt;
  &lt;li&gt;데이터 라벨러 (Data labeler): 데이터 라벨링을 수행하는 사람&lt;/li&gt;
  &lt;li&gt;PM (Project Manager): 프로젝트 전반의 전략 수립과 운영을 맡아 관리하는 직책&lt;/li&gt;
  &lt;li&gt;QM (Quality Manager): 데이터 수집, 가공 및 검수, 인력 관리를 맡아 데이터 품질을 관리하는 직책&lt;/li&gt;
  &lt;li&gt;라벨 (Label): 데이터와 그에 부착된 라벨링 정보들 (어노테이션)을 지칭하는 용어&lt;/li&gt;
  &lt;li&gt;어노테이션 (Annotation): 라벨링 공정에서 인간이 부여한 식별 기준을 기계가 이해하도록 데이터에 추가한 정보&lt;/li&gt;
  &lt;li&gt;라벨링 데이터 (Labeled data): 원천 데이터에 부여한 파일 형식, 해상도, 설명, 주석 등의 어노테이션 집합&lt;/li&gt;
  &lt;li&gt;클래스 (Class, 카테고리): 분류 및 탐지하고자 하는 대상을 카테고리화 한 것으로, 분류체계를 의미&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-인공지능-개발-프로세스의-이해&quot;&gt;3. 인공지능 개발 프로세스의 이해&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;데이터 수집: 개발할 AI의 목적에 맞게 현실 세계에서 필요한 데이터를 수집 및 생성&lt;/li&gt;
  &lt;li&gt;데이터 가공: 라벨링 규칙에 따라 원천 데이터에 정보 (어노테이션)를 부착&lt;/li&gt;
  &lt;li&gt;데이터 검수: 라벨링 데이터가 규칙에 맞게 라벨링 되었는지 검수&lt;/li&gt;
  &lt;li&gt;인공지능 모델 학습: 데이터와 라벨링 데이터로 기계학습을 진행하여 인공지능 모델을 생성&lt;/li&gt;
  &lt;li&gt;AI 서비스 개발: AI 서비스를 개발하고, 배포&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;4-데이터-라벨링-기법-및-적용-사례&quot;&gt;4. 데이터 라벨링 기법 및 적용 사례&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;바운딩 박스: 객체의 범위를 사각형 박스로 지정하는 라벨링 기법, 객체 탐지 모델에 주로 사용&lt;/li&gt;
  &lt;li&gt;3D 바운딩 박스 (Cuboid): 객체의 범위를 직육면체 박스로 지정하는 라벨링 기법, 너비, 높이, 깊이, 방향 정보 포함&lt;/li&gt;
  &lt;li&gt;OCR (Optic Character Recognition): 이미지, 영상 속 문자를 기계가 읽을 수 있는 문자로 변환하는 라벨링 기법&lt;/li&gt;
  &lt;li&gt;키포인트: 객체의 주요 지점 (특징)을 점으로 지정하는 라벨링 기법, 이미지 배칭 및 안면 인식, 골격 추출 등에 활용&lt;/li&gt;
  &lt;li&gt;폴리라인: 선형 객체의 경계나 위치 등을 연속선으로 지정하는 라벨링 기법&lt;/li&gt;
  &lt;li&gt;폴리곤: 객체의 범위 또는 경계를 다각형으로 지정하는 라벨링 기법, 정교한 인공지능 모델 개발에 사용&lt;/li&gt;
  &lt;li&gt;시멘틱 세그멘테이션: 이미지의 모든 픽셀에 클래스를 부여, 높은 정확도를 요구하는 CV 기반 응용 프로그램에 사용&lt;/li&gt;
  &lt;li&gt;비디오 어노테이션: 영상에서 구간 정제, 분류, 객체 태깅 방법. 객체 인식, 객체 추적 등에 주로 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-데이터-라벨링-도구-소개&quot;&gt;5. 데이터 라벨링 도구 소개&lt;/h2&gt;

&lt;h3 id=&quot;용어-정리&quot;&gt;용어 정리&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;프로젝트: 특정 목표를 성취하기 위해 데이터, 라벨링, 자원/품질 관리 등을 실행하는 과제 단위&lt;/li&gt;
  &lt;li&gt;저작도구 (Authoring tool, Annotator): 저작에 사용되는 소프트웨어&lt;/li&gt;
  &lt;li&gt;라벨링 가이드라인: 라벨링 작업 방식과 기준이 기재된 문서&lt;/li&gt;
  &lt;li&gt;객체 (Object): 라벨링 대상&lt;/li&gt;
  &lt;li&gt;검수 (Review): 기준에 적합하게 라벨링 되었는지 검사. 작업 완료된 라벨은 검수를 거쳐 반료/완료로 전환&lt;/li&gt;
  &lt;li&gt;반려 (Return): 기준에 적합하게 라벨링 되지 않아 검수를 통과하지 못한 라벨로, 수정하여 다시 검수해야 함&lt;/li&gt;
  &lt;li&gt;코멘트: 반려된 라벨을 수정 시 반려 사유를 기재한 평가글&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;도구-open-source-소개&quot;&gt;도구 (Open source) 소개&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;CVAT (Computer Vision Annotation Tool): Intel에서 개발한 웹 (크롬) 기반 저작도구&lt;/li&gt;
  &lt;li&gt;Diffgram: 웹형/설치형 저작도구, 데이터셋 및 워크플로우 관리 기능&lt;/li&gt;
  &lt;li&gt;Label box: 설치형 저작도구, 자동 라벨링 기능, 관리 및 협업 기능&lt;/li&gt;
  &lt;li&gt;Labellmg: 설치형 저작도구, 바운딩 박스만 지원&lt;/li&gt;
  &lt;li&gt;Label Studio: 설치형 저작도구, 다양한 자동 라벨링, 커뮤니티 활성화&lt;/li&gt;
  &lt;li&gt;VIA (VGG Image Annotator): 설치형 저작도구, 안면 추적 기능, 이미지 리스트에 대한 효과적인 라벨링 가능&lt;/li&gt;
  &lt;li&gt;VoTT (Visual Object Tagging Tool): Microsoft에서 개발한 설치형 저작도구, 자동 라벨링 기능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: AI Data 2022년 인공지능 학습용 데이터 라벨링 전문 교육 -&amp;gt; &lt;a href=&quot;http://aidata.elancer.co.kr/student/main.php&quot;&gt;강의 소개 홈페이지&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 17 Oct 2022 13:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A13/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A13/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>데이터 라벨링</category>
        
        
        <category>Education</category>
        
        <category>인공지능 학습용 데이터 라벨링 교육</category>
        
      </item>
    
      <item>
        <title>AI 학습용 데이터 라벨링 교육 - 음성/텍스트 입문 과정</title>
        <description>&lt;p&gt;AI Data에서 제공하는 AI 학습용 데이터 라벨링 교육의 음성/텍스트 입문 과정에 대한 강의 기록. &lt;br /&gt;
음성/텍스트 전사 (라벨링)에 사용되는 저작도구에 대한 사용법 학습 후 실습을 통해 기본적인 작업 학습. &lt;br /&gt;
맞춤법, 정제 저작도구, 전사 규칙 등에 대해 학습.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-용어-개념-정의&quot;&gt;1. 용어 개념 정의&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;전사: 말소리를 음성 문자로 옮겨 적음&lt;/li&gt;
  &lt;li&gt;속기: 빨리 적다, 속기법으로 적은 기록&lt;/li&gt;
  &lt;li&gt;음성 싱크 작업: 재생되는 음성과 문자의 내용을 일치시켜 주는 작업&lt;/li&gt;
  &lt;li&gt;비식별화 작업: 이름이나 전화번호 등 개인정보가 있는 데이터를 전사 단계에서 특정 기호로 표기하는 방법&lt;/li&gt;
  &lt;li&gt;이중 전사 작업: 비표준어가 음성 파일에서 나타났을 때, 표준어와 함께 표기하는 작업&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-음성텍스트-데이터의-학습-처리-과정&quot;&gt;2. 음성/텍스트 데이터의 학습 처리 과정&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;수집 (녹음) -&amp;gt; 정제 -&amp;gt; 검사 -&amp;gt; 전사 -&amp;gt; 검사 -&amp;gt; 최종 검수 과정을 거쳐 학습 데이터로 완성&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
  &lt;li&gt;원시 데이터 수집 (녹음): 음성 데이터를 얻기 위한 단계. 녹음된 파일도 저작권이 있어서 사전에 이용 허락을 받음&lt;/li&gt;
  &lt;li&gt;데이터 정제: 음성 원시 데이터 다운로드 및 정제 후, 관리 FTP에 결과물 등록&lt;/li&gt;
  &lt;li&gt;1&amp;amp;2차 검수: 정성적, 정량적 평가를 통해 데이터의 유효성을 판별&lt;/li&gt;
  &lt;li&gt;전사: 음성 정제 데이터 다운로드, 전사 작업, 관리 FTP에 결과물 등록&lt;/li&gt;
  &lt;li&gt;최종 검수: 3차 검수, JSON 변환, 관리 FTP에 결과물 등록&lt;/li&gt;
  &lt;li&gt;학습: 전처리, 음향모델 학습, 언어모델 학습, 추론&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;3-데이터-정제전사-저작도구-및-규칙&quot;&gt;3. 데이터 정제/전사 저작도구 및 규칙&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;음성 데이터 정제 도구 - Audacity&lt;/li&gt;
  &lt;li&gt;전사 데이터 저작 도구 - 전사툴&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-음성텍스트-데이터의-학습-처리-유형&quot;&gt;4. 음성/텍스트 데이터의 학습 처리 유형&lt;/h2&gt;

&lt;h3 id=&quot;음성-데이터-전사-받아쓰기-방법에-따른-분류&quot;&gt;음성 데이터 전사 (받아쓰기) 방법에 따른 분류&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;일반 전사: 사람이 말한 그대로 문자화하여 전사하는 방법 (발음 전사)&lt;/li&gt;
  &lt;li&gt;이중 전사: 한글 맞춤법 표기에 따른 발음과 차이가 있는 경우에 발음 전사와 철자 전사를 병행&lt;/li&gt;
  &lt;li&gt;화자 전사: 음성 데이터 상에서 등장하는 사람이 여럿일 때 음성마다 화자를 구분하는 작업&lt;/li&gt;
  &lt;li&gt;배경음 및 화자 감정 태깅: 드라마 등에서 배경음이 나오는 구간을 음성 싱크를 설정하여 태깅하고, 감정이 섞인 음성을 발화했을 때 해당 구간만 음성 싱크를 설정하고, 발화 내용을 전사하는 작업&lt;/li&gt;
  &lt;li&gt;방송 영상 자막 사전 제작: 청각 장애인을 위해 영화와 같은 프로그램의 내용의 자막을 사전에 제작하는 작업&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;음성-데이터-전사-받아쓰기-주체에-따른-분류&quot;&gt;음성 데이터 전사 (받아쓰기) 주체에 따른 분류&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;사람에 의한 전사: 사람이 직접 전사하는 작업&lt;/li&gt;
  &lt;li&gt;STT (Speech To Text): 기계가 직접 전사하는 작업&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;음성-데이터-기관에-따른-분류&quot;&gt;음성 데이터 기관에 따른 분류&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;연구 기관의 과제 전사: AI 엔진의 학습을 위해 정부 연구 기관에서 진행하는 과제에서 음성 자료를 전사하는 것&lt;/li&gt;
  &lt;li&gt;기업의 콜센터 녹취 전사: 기업에서 보유하고 있는 AI 엔진의 학습을 위해 상담원과 고객의 통화 데이터를 전사하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-음성텍스트-데이터-학습-처리-사례&quot;&gt;5. 음성/텍스트 데이터 학습 처리 사례&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;민원 (콜센터) 질의-응답 데이터: 상담원들이 전문 상담에 집중하여 원활한 업무가 진행될 수 있도록 상담사의 업무를 보조할 수 있는 서비스 구축&lt;/li&gt;
  &lt;li&gt;상담 음성 데이터: AI 상담 센터를 위한 음성 상담, 음성 인식 기술, 언어 생성 연구 및 서비스 개발 분야로 활용&lt;/li&gt;
  &lt;li&gt;자유대화 음성 (일반남녀) 데이터: 자유대화를 효과적으로 인식하기 위해 인공지능 기반 한국어 자유대화 (일상대화) 데이터 구축&lt;/li&gt;
  &lt;li&gt;자유대화 음성 (노인남녀) 데이터: 사투리, 억양 등의 발화 특성이 타 연령대와 다른 특성이 존재하여 노인 대상 음성 서비스를 위해 데이터 구축&lt;/li&gt;
  &lt;li&gt;자유대화 음성 (소아, 유아) 데이터: 소아들의 음성인식 관련 서비스가 증가되지만, 소아들의 발화 특성을 반영한 음성 데이터가 부족하여 데이터 구축&lt;/li&gt;
  &lt;li&gt;한국인 대화 음성: 다양한 환경 (연령, 원거리, 노이즈 등)을 인식할 수 있는 대화 및 음성 데이터셋 구축&lt;/li&gt;
  &lt;li&gt;한국인 외래어 발화: 인공지능 기반 한국어 음성인식 서비스의 활성화를 위한 자유대화 지식 데이터 구축&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-음성텍스트-데이터-학습-처리-사례&quot;&gt;6. 음성/텍스트 데이터 학습 처리 사례&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;WAV, TXT, JSON 포멧의 파일 형태&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;7-언어-모델의-정의&quot;&gt;7. 언어 모델의 정의&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;언어 모델&lt;/strong&gt;은 주어진 단어나 문장을 통해 다음에 어떤 단어가 등장할지에 대한 확률을 예측하는 모형
    &lt;ul&gt;
      &lt;li&gt;음향 모델: 아침을 먹구 학교에 갔다.&lt;/li&gt;
      &lt;li&gt;언어 모델: 아침을 먹고 학교에 갔다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;주어진 시나리오에 맞는 Q&amp;amp;A 대화 제작&lt;/li&gt;
  &lt;li&gt;제시된 지문을 읽고 질문을 만들거나 질문에 대한 답을 찾는 작업&lt;/li&gt;
  &lt;li&gt;일반인을 대상으로 한 텍스트 데이터 수집&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;8-텍스트-데이터-학습-처리-사례&quot;&gt;8. 텍스트 데이터 학습 처리 사례&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;논문 자료 요약: 다양한 주제의 한국어 학술 논문, 특허명세서에서 요약문을 도출하도록 AI를 훈련하기 위한 데이터셋&lt;/li&gt;
  &lt;li&gt;도서 자료 요약: 한국어 도서 원문으로부터 생성 요약문을 도출하도록 AI를 훈련하기 위한 데이터셋&lt;/li&gt;
  &lt;li&gt;도서 자료 기계 독해: 다양한 주제의 도서 자료를 활용한 기계 독해용 데이터셋 구축&lt;/li&gt;
  &lt;li&gt;일반 상식: 한국어 위키백과 내 주요 문서 15만 개에 포함된 지식을 추출하여 데이터셋 구축&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;9-음성-데이터-학습-처리에-필요한-맞춤법&quot;&gt;9. 음성 데이터 학습 처리에 필요한 맞춤법&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;띄어쓰기 -&amp;gt; ‘잘’, ‘안’, ‘못’, ‘안 돼’와 ‘안돼’의 띄어쓰기 유의&lt;/li&gt;
  &lt;li&gt;헷갈리는 단어 -&amp;gt; ‘이에요’, ‘예요’, ‘-오’, ‘-요’, ‘되’, ‘돼’, ‘안’, ‘않’, ‘안되다’, ‘안 되다’의 단어 유의&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: AI Data 2022년 인공지능 학습용 데이터 라벨링 전문 교육 -&amp;gt; &lt;a href=&quot;http://aidata.elancer.co.kr/student/main.php&quot;&gt;강의 소개 홈페이지&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 17 Oct 2022 13:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A12/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A12/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>데이터 라벨링</category>
        
        
        <category>Education</category>
        
        <category>인공지능 학습용 데이터 라벨링 교육</category>
        
      </item>
    
      <item>
        <title>AI 학습용 데이터 라벨링 교육 - 필수과정 (인공지능 윤리와 법)</title>
        <description>&lt;p&gt;AI Data에서 제공하는 AI 학습용 데이터 라벨링 교육의 필수과정 (인공지능 윤리와 법)에 대한 강의 기록. &lt;br /&gt;
AI 학습용 데이터 라벨러들에게 개인정보 비식별화, 저작권, 초상권 등의 인공지능 윤리에 대한 지식과 그 필요성을 학습.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-인공지능-개요&quot;&gt;1. 인공지능 개요&lt;/h2&gt;
&lt;h3 id=&quot;ai-학습용-데이터셋-구축-사업&quot;&gt;AI 학습용 데이터셋 구축 사업&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AI 제품 서비스 및 기술 개발에 활용 가치가 높은 대규모 AI 학습용 데이터 구축 및 개방, 응용 개발&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.aihub.or.kr/&quot;&gt;AI-hub 데이터 플랫폼&lt;/a&gt;에서 데이터 활용 건수가 계속해서 증가하고 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2-ai-학습용-데이터셋-구축-프로세스&quot;&gt;2. AI 학습용 데이터셋 구축 프로세스&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 생애 (Lifecycle) 관점&lt;/li&gt;
  &lt;li&gt;학습용 데이터의 생애주기는 크게 계획, 구축, 운영, 활용 영역으로 구분&lt;/li&gt;
  &lt;li&gt;각 영역의 세부 활동은 SW 프로세스 계층, 데이터 프로세스 계층, 데이터 계층, 데이터 서비스 계층 등으로 구분&lt;/li&gt;
  &lt;li&gt;비정형 보다는, 정형 데이터 위주로 데이터셋을 구축하도록 노력하고 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-인공지능-윤리-및-이해&quot;&gt;2. 인공지능 윤리 및 이해&lt;/h2&gt;

&lt;h3 id=&quot;ai의-양면성-편향성-윤리적-딜레마&quot;&gt;AI의 양면성, 편향성, 윤리적 딜레마&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;기계학습 모델을 학습시키는데 사용되는 데이터가 사람이나 사회가 가지는 편견을 포함하고 있는 것을 의미&lt;/li&gt;
  &lt;li&gt;편향성을 가진 데이터를 사용하여 학습한 인공지능은 편향된 결과를 출력할 수 밖에 없고, 차별을 가져올 수도 있음&lt;/li&gt;
  &lt;li&gt;AI는 새로운 기술의 혜택을 누릴 수 있도록 도울 수 있지만, 오용되는 등의 다양한 윤리적 문제를 야기할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;국내외-주요-인공지능-윤리-기준&quot;&gt;국내외 주요 인공지능 윤리 기준&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AI에 대한 윤리적 권고사항과 개인정보 보호 지침을 발표하며, 활발히 논의 중&lt;/li&gt;
  &lt;li&gt;사람이 중심이 되는 인공지능 윤리 기준&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-인공지능-개인정보보호&quot;&gt;3. 인공지능 개인정보보호&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;신기술 확산으로 인해 개인정보 침해 가능성이 확대되고 있음&lt;/li&gt;
  &lt;li&gt;사생활 침해, 데이터 프라이버시 논란 문제&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;인공지능과-개인정보-비식별화&quot;&gt;인공지능과 개인정보 비식별화&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;개인정보 비식별화란, 개인정보에서 개인식별 요소를 제거하여 특정 개인을 알아 볼 수 없는 형태로 만드는 조치&lt;/li&gt;
  &lt;li&gt;다른 정보와 결합하여도 특정 개인을 식별하기 어렵도록 하는 일련의 조치&lt;/li&gt;
  &lt;li&gt;익명 정보: 정보 수집 단계에서 근원적으로 개인을 식별할 수 없는 형태로 수집한 정보&lt;/li&gt;
  &lt;li&gt;비식별화 정보: 개인을 식별할 수 있는 상태에서 비식별화 과정을 통해 개인을 식별할 수 없게 처리한 정보&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-인공지능-저작권과-안면인식-초상권&quot;&gt;4. 인공지능 저작권과 안면인식 초상권&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;저작권법: 저작자의 권리와 이에 인접하는 권리를 보호하고, 저작물의 공정한 이용을 도모&lt;/li&gt;
  &lt;li&gt;AI 모델 학습에 사용되는 여러가지 데이터셋의 활용에 대한 이슈 대두&lt;/li&gt;
  &lt;li&gt;해당 데이터셋들의 주인은 누구인가, 동의 후에 사용할 수 있는가&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-인공지능-지식재산권&quot;&gt;5. 인공지능 지식재산권&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;AI가 점차 문화, 예술의 영역으로 활동 범위를 넓혀가며, AI가 만든 결과물에 저작권을 부여할 수 있는가?&lt;/li&gt;
  &lt;li&gt;AI 데이터를 구축 및 공개함에 있어서 타인의 지적재산권을 침해하지 않도록 구매, 사용계약 체결 등 적정한 조치 필요&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;6-인공지능-관련-경력-개발-경로-및-비전&quot;&gt;6. 인공지능 관련 경력 개발 경로 및 비전&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;AI 윤리, 법적 책임성 등 공통 교육을 통한 인공지능 기본 지식 함양&lt;/li&gt;
  &lt;li&gt;인공지능에 필수적인 여러 유형의 라벨링 기술 습득 필요 (다양한 데이터)&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;전문 라벨러 양성을 통한 고품질 데이터 양산 가능 (고품질 데이터 확보)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;인공지능 데이터 구축 및 활용의 성공을 위해서는 양질의 고품질 데이터와 데이터 가공에 숙련된 라벨러가 필요&lt;/li&gt;
  &lt;li&gt;데이터 라벨링: 인공지능이 기계학습에 활용하도록 기능이나, 목적에 부합하는 정보를 원천 데이터에 부착하는 활동&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;라벨링 데이터: 원천 데이터에 부여한 파일형식, 해상도 등의 속성, 설명이나 주석 등이 포함된 &lt;strong&gt;어노테이션&lt;/strong&gt;의 집합&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;국가직무능력표준 (NCS) 설계 진행 중&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: AI Data 2022년 인공지능 학습용 데이터 라벨링 전문 교육 -&amp;gt; &lt;a href=&quot;http://aidata.elancer.co.kr/student/main.php&quot;&gt;강의 소개 홈페이지&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 17 Oct 2022 13:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A11/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/AI-%ED%95%99%EC%8A%B5%EC%9A%A9-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EB%9D%BC%EB%B2%A8%EB%A7%81-%EA%B5%90%EC%9C%A11/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>데이터 라벨링</category>
        
        
        <category>Education</category>
        
        <category>인공지능 학습용 데이터 라벨링 교육</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 5주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 5주차 강의에 대한 정리입니다. &lt;br /&gt;
이번 주에는 파이썬을 활용한 프레임워크인 Django의 세부적인 내용들을 추가적으로 학습합니다. &lt;br /&gt;
또한 특별히 지금까지 학습했던 내용들을 모두 통합하여 프로젝트를 할 수 있는 Monthly project가 있습니다. &lt;br /&gt;
파이썬으로 데이터를 EDA 하고, 이를 웹 페이지로 구성하여 배포까지 하는 것으로 프로젝트의 목표를 잡습니다. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 - 다크 모드가 아닌 화이트 모드로 보시면 사진 자료를 편하게 확인 가능합니다!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-web-application-with-django&quot;&gt;1. Web Application with Django&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Django: Python 기반 웹 프레임워크&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;django
django-admin startproject &amp;lt;project_name&amp;gt; &lt;span class=&quot;c&quot;&gt;# django 프로젝트 만들기&lt;/span&gt;
django-admin startapp &amp;lt;app_name&amp;gt;         &lt;span class=&quot;c&quot;&gt;# django 앱 만들기&lt;/span&gt;
python manage.py runserver               &lt;span class=&quot;c&quot;&gt;# django 실행하기&lt;/span&gt;

python manage.py makemigrations  &lt;span class=&quot;c&quot;&gt;# DB migration 만들기&lt;/span&gt;
python manage.py migrate         &lt;span class=&quot;c&quot;&gt;# default로 만든 DB 정보 반영하기 (DB 연동)&lt;/span&gt;
python manage.py createsuperuser &lt;span class=&quot;c&quot;&gt;# 관리자 계정 생성하기&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Django project and app
    &lt;ul&gt;
      &lt;li&gt;한 project는 아래와 같이 여러 app으로 구성&lt;/li&gt;
      &lt;li&gt;project
        &lt;ul&gt;
          &lt;li&gt;app1&lt;/li&gt;
          &lt;li&gt;app2&lt;/li&gt;
          &lt;li&gt;app3&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Django의 MVT pattern
    &lt;ul&gt;
      &lt;li&gt;MVC: Model, View, Controller&lt;/li&gt;
      &lt;li&gt;MVT: Model, View, Template
        &lt;ul&gt;
          &lt;li&gt;URL: urls.py&lt;/li&gt;
          &lt;li&gt;View: views.py&lt;/li&gt;
          &lt;li&gt;Model: DB, ORM&lt;/li&gt;
          &lt;li&gt;Template: html, template 언어&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Django에서 view 추가하기
    &lt;ol&gt;
      &lt;li&gt;views.py 파일에 함수 만들기&lt;/li&gt;
      &lt;li&gt;urls.py 파일의 urlpatterns에 path 추가하기&lt;/li&gt;
      &lt;li&gt;settings.py 파일에서 INSTALLED_APPS에 페이지 이름 추가하기&lt;/li&gt;
      &lt;li&gt;Django에서는 default로 admin 페이지 확인 가능 (&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;127.0.0.1:8000/admin&lt;/code&gt;)&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;Django에서 template 추가하기
    &lt;ol&gt;
      &lt;li&gt;views.py 파일과 연동할 수 있는 index.html 파일 만들기&lt;/li&gt;
      &lt;li&gt;views.py 파일에서 render 함수의 인자로 1번에서 만든 index.html 받아주기&lt;/li&gt;
      &lt;li&gt;settings.py 파일에서 TEMPLATES 변수에 (index.html 파일이 있는) DIR 추가하기&lt;/li&gt;
      &lt;li&gt;template 언어를 활용하면, views.py에서 데이터를 받아올 수 있음&lt;/li&gt;
      &lt;li&gt;template 태그를 활용하면, for, if 등을 활용할 수 있음&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-django로-동적-웹페이지-만들기&quot;&gt;2. Django로 동적 웹페이지 만들기&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Remind: Django의 MVT pattern
    &lt;ul&gt;
      &lt;li&gt;URL: urls.py&lt;/li&gt;
      &lt;li&gt;View: views.py&lt;/li&gt;
      &lt;li&gt;Template: html, css, javascript&lt;/li&gt;
      &lt;li&gt;Model: models.py
        &lt;ul&gt;
          &lt;li&gt;데이터베이스 (RDB - Relational Database, SQL)&lt;/li&gt;
          &lt;li&gt;ORM: Object (객체)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;
    &lt;img src=&quot;https://www.javatpoint.com/django/images/django-mvt-based-control-flow.png&quot; alt=&quot;Logo&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Django에서 admin을 활용하여 DB 연결해보기
    &lt;ol&gt;
      &lt;li&gt;models.py에 class로 DB 테이블을 정의하고, 간단한 스키마를 작성&lt;/li&gt;
      &lt;li&gt;admin.py에 1번에서 만든 class 이름을 admin.site.register로 추가&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;정리
    &lt;ol&gt;
      &lt;li&gt;views.py에 보여주고자 하는 내용을 담은 함수 정의&lt;/li&gt;
      &lt;li&gt;DB와 연결하고 싶다면, models.py에 DB를 class로 정의&lt;/li&gt;
      &lt;li&gt;페이지에서 보여주고자 하는 내용을 template 디렉토리 내에 html 파일로 저장&lt;/li&gt;
      &lt;li&gt;urls.py에 보여주고자 하는 view 내용을 1번에서 정의한 함수로 URL과 함께 정의&lt;/li&gt;
      &lt;li&gt;settings.py에 INSTALLED_APPS와 TEMPLATES에 내용 추가&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-monthly-project&quot;&gt;3. Monthly project&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;데이터 시각화 웹 페이지 만들기&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;데이터 EDA를 했던 결과를 웹 상에서 확인할 수 있도록 하는 웹 페이지 만들어보기&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;내가 고민했던 것들
    &lt;ol&gt;
      &lt;li&gt;어떤 데이터로 하면 좋을까?&lt;/li&gt;
      &lt;li&gt;EDA로 어떤 것들을 보여주면 좋을까?&lt;/li&gt;
      &lt;li&gt;Bootstrap 등 오픈 소스를 활용해볼 수 있을까?&lt;/li&gt;
      &lt;li&gt;내가 과거에 했었거나, 지금 하고 있는 프로젝트로 적용해볼까?&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;고민에 대한 생각들
    &lt;ul&gt;
      &lt;li&gt;사이드 프로젝트로 하는 것도 결국 웹 구현까지 계획 중이니, 지금 하고 있는 프로젝트로 해보자&lt;/li&gt;
      &lt;li&gt;수집한 텍스트 데이터로 어떠한 것들을 보여주면 좋을까?
        &lt;ul&gt;
          &lt;li&gt;Title, Desc, URL, Issued, View_count, Download_count, Region, Label, Keywords&lt;/li&gt;
          &lt;li&gt;기본적인 데이터 조회가 가능한 News 게시판 같은 형태로 href 걸어서 메인 페이지 구성&lt;/li&gt;
          &lt;li&gt;Issued된 날짜 별 통계 시각화&lt;/li&gt;
          &lt;li&gt;Region에 따른 지역 기반 시각화&lt;/li&gt;
          &lt;li&gt;Label 별 구성 비율에 대한 시각화 (finviz 참고)&lt;/li&gt;
          &lt;li&gt;View_count 및 Download_count에 따른 조회 상태에 대한 통계 시각화&lt;/li&gt;
          &lt;li&gt;Desc 전처리해서 각 데이터 별 핵심 키워드 추출하고, 그것을 보여줄 수 있는 시각화&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;실제 구현
    &lt;ul&gt;
      &lt;li&gt;index.html: Main page (Menu 구성)
        &lt;ul&gt;
          &lt;li&gt;Overview: 전반적인 프로젝트 (웹페이지)에 대한 소개
            &lt;ul&gt;
              &lt;li&gt;Ref: cards.html, general.html, typography.html, blank-page-header.html&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Data Detail: 데이터에 대한 EDA를 기록하는 페이지
            &lt;ul&gt;
              &lt;li&gt;Ref: data-tables.html, general-table.html&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Data Search: 데이터를 얻을 수 있는 여러 소스들을 모아서 소개해주기
            &lt;ul&gt;
              &lt;li&gt;Ref: influencer-finder.html&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
          &lt;li&gt;Commnuity: 유저들끼리 서로 커뮤니케이션 할 수 있는 공간
            &lt;ul&gt;
              &lt;li&gt;Ref: influencer-profile.html&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/05%EC%A3%BC%EC%B0%A8/monthly_prj&quot;&gt;최종 결과물 Github&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;4-5주차-돌아보기&quot;&gt;4. 5주차 돌아보기&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;기간: 2022. 10. 17 ~ 2022. 10. 22&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;데브코스를 시작하고 벌써 한 달이 지났고, 올해도 거의 마무리가 되어 가고 있다. 최근에는 갑작스럽게 날씨도 많이 추워져서 연말이 다가오는게 피부로도 느껴지는 듯하다. 다시는 돌아오지 않을 나에게 주어진 조금 특별한 이 시간들을 성실하게 잘 사용해보려고 여러가지로 애를 쓰고 있는데 잘하고 있는지는 잘 모르겠다. 여러가지 하려고 하는 욕심이 마음을 앞서다보니 지금 내가 잘하고 있는건가 하는 의문도 들기도 한다. 이럴때 필요한 것들 중 하나는, 내가 가고자 하는 방향이 올바른지를 돌아보는 것이 아닐까 생각해본다. 생각하지 않으면, 흘러가는대로 살아간다고 하는 것처럼 지금 이 시기를 단순히 그렇게 보내고 싶지는 않다. 내 삶의 주체성을 가지고, 내가 계획한 것들을 성취하며 나아가는 사람이 되고 싶다.&lt;/p&gt;

&lt;p&gt;특별히 이번주에는 지금까지 데브코스에서 배웠던 내용들을 바탕으로 최종적으로 Monthly project를 진행했다. 꽤 오래 전에 아주 가볍게 배웠던 django를 이번에도 학습하게 되었는데, 그때는 아무것도 모르고 따라하기만 했었다면 이번에는 나름 스스로가 의미있다고 생각될 정도로 결과물이 나온 것 같아서 나름 뿌듯한 것 같다. 물론 당연히, 여전히 부족한게 많이 있겠지만 천천히 하나씩 채워가는 재미도 있는 것 같다.&lt;/p&gt;

&lt;p&gt;물론 이번 프로젝트를 하면서 추가적으로 느꼈던 것도 당연히 있었다. 하루, 이틀만에 내가 생각했던 것을 코드로 구현하고 최종적으로 웹 페이지라는 눈에 보이는 결과물로 만들었던 것은 꽤나 재밌고, 뿌듯했던 시간이었다. 그러다보니 스스로 거기에 심취(?)해 있던 것도 없지 않았던 것 같다. 조금 시야를 달리보면 내가 회사와 같이 이러한 기술을 사용하는 곳에서는 단순히 내가 이런 프로젝트를 해봤다의 수준이 아닌, 스스로 온전히 프로젝트를 이끌어 갈 수 있을 만큼의 수준을 요구할 것이다. 과거에는 여러가지를 다방면에서 다룰 줄 아는 Generalist의 모습이 좋았던 것 같아 여러가지를 많이 시도해봤던 것 같다. 물론 그 능력도 필요하고, 대단하겠지만 최근에는 오히려 특정 분야의 Specialist가 되어야 할까 하는 생각이 든다.&lt;/p&gt;

&lt;p&gt;오늘도 여전히 내 진로와 미래에 대해 끝없이 고민하고 생각을 정리해본다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 프로그래머스 인공지능 데브코스 4기 5주차 강의 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/05%EC%A3%BC%EC%B0%A8&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 17 Oct 2022 10:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-5%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-5%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>밑시딥1 4강. 신경망 학습</title>
        <description>&lt;p&gt;이번 글에서는 밑바닥부터 시작하는 딥러닝1 책의 4강에 대한 리뷰를 시작합니다. &lt;br /&gt;
딥러닝의 퍼셉트론과 신경망에 대해 학습한 후, 이번에는 어떻게 학습이 진행되는지 학습합니다. &lt;br /&gt;
손실 함수를 통해 파라미터가 갱신되는 과정을 확인해보며, 그것을 가능케 하는 경사 하강법을 배웁니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Chapter&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Title&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Main Topics&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;헬로 파이썬&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;파이썬 기초 문법 소개, numpy, matplotlib&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;퍼셉트론&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AND, NAND, OR 게이트&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;신경망&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;활성화 함수, 다차원 배열 계산, 출력층 설계, MNIST&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;4강&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;신경망 학습&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;손실 함수, 경사 하강법&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;오차역전파법&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;역전파, 활성화 함수 구현&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;학습 관련 기술들&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;매개변수 갱신, 배치 정규화, 하이퍼파라미터 값 찾기&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;합성곱 신경망 (CNN)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;합성곱 계층, 풀링 계층, CNN 구현&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;딥러닝 (Deep learning)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;초기 역사, 딥러닝 활용&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Appendix&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Softmax with loss 계층의 계산 그래프&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yes24.com/Product/Goods/34970929&quot;&gt;밑바닥부터 시작하는 딥러닝 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/WegraLee/deep-learning-from-scratch&quot;&gt;밑바닥부터 시작하는 딥러닝 1 Github 링크&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 - 다크 모드가 아닌 화이트 모드로 보시면 자료를 편하게 확인 가능합니다!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;chapter-4-신경망-학습&quot;&gt;&lt;font color=&quot;orange&quot;&gt;Chapter 4. 신경망 학습&lt;/font&gt;&lt;/h2&gt;

&lt;h3 id=&quot;41-데이터에서-학습한다&quot;&gt;4.1 데이터에서 학습한다!&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;신경망의 특징은 데이터를 보고, 학습 할 수 있다는 점&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;학습&lt;/strong&gt;이란, 훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 획득하는 것&lt;/li&gt;
  &lt;li&gt;학습의 목표는 &lt;strong&gt;손실 함수의 결과값을 가장 작게 만드는 가중치 매개변수를 찾는 것&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;411-데이터-주도-학습&quot;&gt;4.1.1 데이터 주도 학습&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;기계학습&lt;/strong&gt;은 데이터에서 답을 찾고, 데이터에서 패턴을 발견하고 데이터로 이야기를 만드는 것&lt;/li&gt;
  &lt;li&gt;따라서 기계학습의 중심에는 &lt;strong&gt;데이터&lt;/strong&gt;가 존재한다고 할 수 있음&lt;/li&gt;
  &lt;li&gt;기계학습에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;사람의 개입을 최소화&lt;/code&gt;하면서 수집한 데이터로 패턴을 찾으려 시도&lt;/li&gt;
  &lt;li&gt;MNIST 모델에서 숫자를 인식하는 알고리즘이 동작하기 위해서,
    &lt;ul&gt;
      &lt;li&gt;이미지에서 특징 (Feature)을 추출하여 그 특징의 패턴을 기계학습 기술로 학습&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;특징&lt;/strong&gt;은 입력 데이터에서 본질적인 데이터를 정확히 추출할 수 있도록 설계된 변환기&lt;/li&gt;
      &lt;li&gt;이미지의 특징은 일반적으로 ‘벡터’로 변환하여 학습&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기계학습에서는 모아진 데이터로부터 규칙을 찾아내는 역할을 &lt;strong&gt;기계&lt;/strong&gt;가 한다고 할 수 있음&lt;/li&gt;
  &lt;li&gt;사람이 직접 설계하는 것에 비해서 부담은 적지만, 이미지를 벡터로 변환하는 등의 과정은 사람이 해야 함&lt;/li&gt;
  &lt;li&gt;문제에 적합한 특징을 설계하지 (전처리) 못한다면, 좋은 결과를 획득하기도 어렵다고 할 수 있음&lt;/li&gt;
  &lt;li&gt;입력부터 출력까지 사람의 개입 없이 동작하여 &lt;strong&gt;종단간 기계학습 (end-to-end machine learning)&lt;/strong&gt;이라고도 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;412-훈련-데이터와-시험-데이터&quot;&gt;4.1.2 훈련 데이터와 시험 데이터&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;기계학습 문제는 &lt;strong&gt;훈련 데이터 (Training data)와 시험 데이터 (Test data)&lt;/strong&gt;로 나눠 학습과 실험을 수행&lt;/li&gt;
  &lt;li&gt;훈련 데이터를 통해서 학습하면서 최적의 매개변수를 찾음&lt;/li&gt;
  &lt;li&gt;그 후에 시험 데이터를 사용하여 앞서 훈련한 모델의 성과를 평가&lt;/li&gt;
  &lt;li&gt;결국 모델링의 궁극적인 목적은 범용적으로 사용할 수 있는 &lt;strong&gt;일반화된 모델&lt;/strong&gt;이므로, 훈련 데이터와 시험 데이터로 구분&lt;/li&gt;
  &lt;li&gt;이를 위해서 한번도 보지 못했던 훈련에 포함되지 않은 데이터로 성능을 측정&lt;/li&gt;
  &lt;li&gt;한 데이터셋에만 지나치게 최적화 된 상태를 &lt;strong&gt;오버피팅 (Overfitting)&lt;/strong&gt;이라고 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;42-손실-함수&quot;&gt;4.2 손실 함수&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;신경망 학습에서는 현재의 상태를 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;하나의 지표&lt;/code&gt;로 표현&lt;/li&gt;
  &lt;li&gt;그 지표를 가장 좋게 만들어주는 가중치 매개변수 값을 찾는 것이 목적&lt;/li&gt;
  &lt;li&gt;신경망 학습에서 사용하는 지표는 &lt;strong&gt;손실 함수 (Loss function)&lt;/strong&gt;라고 정의&lt;/li&gt;
  &lt;li&gt;일반적으로 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;오차제곱합과 교차 엔트로피 오차&lt;/code&gt;를 사용
    &lt;ul&gt;
      &lt;li&gt;손실 함수는 신경망의 성능의 나쁨을 나타내는 지표&lt;/li&gt;
      &lt;li&gt;즉, 현재 신경망이 훈련 데이터를 얼마나 잘 처리하지 못하는지의 성능을 담고 있음&lt;/li&gt;
      &lt;li&gt;따라서 손실 함수의 값이 클수록 좋지 않는 성능을 갖고 있다고 할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;421-오차제곱합-sum-of-square-for-error-sse&quot;&gt;4.2.1 오차제곱합 (Sum of Square for Error, SSE)&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[E = \frac{1}{2}{\sum_{k} (y_k - t_k)^2}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;가장 많이 쓰이는 손실 함수는 &lt;strong&gt;오차제곱합&lt;/strong&gt;이고, 수식은 위와 같음&lt;/li&gt;
  &lt;li&gt;$y_k$는 신경망의 출력 (신경망이 출력한 값), $t_k$는 정답 레이블, $k$는 데이터의 차원 수를 나타냄&lt;/li&gt;
  &lt;li&gt;즉, 오차제곱합은 각 원소의 출력과 정답 레이블의 차를 제곱한 후에 그 총합을 구하는 것&lt;/li&gt;
  &lt;li&gt;파이썬 코드로 오차제곱합을 구현하면, 아래와 같음을 알 수 있음
    &lt;ul&gt;
      &lt;li&gt;MNIST에서 정답이 ‘2’라고 했을 때, 모델의 출력 결과가 맞았을 때와 틀렸을 때의 손실함수 값 확인&lt;/li&gt;
      &lt;li&gt;출력 결과가 맞을 때, 손실 함수 값이 0.0975로 작게 나오는 것을 확인할 수 있음&lt;/li&gt;
      &lt;li&gt;반면, 출력 결과가 틀렸을 때, 손실 함수 값이 0.5975로 크게 나오는 것을 확인할 수 있음&lt;/li&gt;
      &lt;li&gt;즉, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;손실 함수의 값이 작을수록 정답에 수렴한다&lt;/code&gt;고 할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sum_squares_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 정답은 '2'
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 예1: '2'일 확률이 가장 높을 때
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;sum_squares_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0.09750000000000003
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 예2: '7'일 확률이 가장 높을 때
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;sum_squares_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0.5975
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;422-교차-엔트로피-오차-cross-entropy-error-cee&quot;&gt;4.2.2 교차 엔트로피 오차 (Cross Entropy Error, CEE)&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[E = -{\sum_{k} t_klog_ey_k}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위 수식은 교차 엔트로피 오차의 수식&lt;/li&gt;
  &lt;li&gt;log는 밑이 e인 자연로그, $y_k$는 신경망의 출력, $t_k$는 정답 레이블이면서 정답 인덱스만 1 (원-핫 인코딩)&lt;/li&gt;
  &lt;li&gt;따라서 실질적으로 정답일 때 추정의 자연로그를 계산하는 식 (다른 경우에는 모두 0이기 때문)&lt;/li&gt;
  &lt;li&gt;즉, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;교차 엔트로피 오차는 정답일 때의 출력이 전체 값을 결정&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;파이썬 코드로 교차 엔트로피 오차를 구현하면, 아래와 같음을 알 수 있음
    &lt;ul&gt;
      &lt;li&gt;MNIST에서 정답이 ‘2’라고 했을 때, 모델의 출력 결과가 맞았을 때와 틀렸을 때의 손실함수 값 확인&lt;/li&gt;
      &lt;li&gt;출력 결과가 맞을 때, 손실 함수 값이 0.5108로 작게 나오는 것을 확인할 수 있음&lt;/li&gt;
      &lt;li&gt;반면, 출력 결과가 틀렸을 때, 손실 함수 값이 2.3025로 크게 나오는 것을 확인할 수 있음&lt;/li&gt;
      &lt;li&gt;즉, 위의 오차제곱합과 동일하게 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;손실 함수의 값이 작을수록 정답에 수렴한다&lt;/code&gt;고 할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 정답은 '2'
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 예1: '2'일 확률이 가장 높을 때
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;cross_entropy_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0.510825457099338
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# # 예2: '7'일 확률이 가장 높을 때
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.05&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;cross_entropy_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2.3025840929945454
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;423-미니배치-학습&quot;&gt;4.2.3 미니배치 학습&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[E = -\frac{1}{N}{\sum_{n}}{\sum_{k} t_klog_ey_k}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위 수식은 &lt;strong&gt;훈련 데이터 모두&lt;/strong&gt;에 대한 손실 함수 값을 구하는 수식&lt;/li&gt;
  &lt;li&gt;앞선 수식과 비슷하고, 데이터 하나에 대한 손실 함수를 단순히 N개의 데이터로 확장한 것&lt;/li&gt;
  &lt;li&gt;마지막에는 N으로 나눠서 정규화 (평균 손실 함수의 역할)&lt;/li&gt;
  &lt;li&gt;하지만 현실적으로 &lt;strong&gt;빅데이터 안에서 이 모든 데이터를 대상으로 값을 계산하는 것은 비효율적&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;따라서 데이터 일부를 추려 전체의 근사치로 이용하는 &lt;strong&gt;미니배치 (Mini-batch)&lt;/strong&gt; 방법을 사용 (일부만 골라 학습)&lt;/li&gt;
  &lt;li&gt;아래 코드는 MNIST 데이터셋에서 np.random.choice 함수를 통해 미니배치로 계산하는 과정을 소개&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/Users/paul/Desktop/github/deep-learning-from-scratch-master/'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset.mnist&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_mnist&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### MNIST 데이터셋 불러오기
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
    &lt;span class=&quot;nf&quot;&gt;load_mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (60000, 784)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (60000, 10)
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### 훈련 데이터에서 무작위로 10장만 추출하기
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;424-배치용-교차-엔트로피-오차-구현하기&quot;&gt;4.2.4 (배치용) 교차 엔트로피 오차 구현하기&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ndim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
&lt;span class=&quot;c1&quot;&gt;#     return -np.sum(t * np.log(y + 1e-7)) / batch_size # 원-핫 인코딩이 되어 있는 경우
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 원-핫 인코딩이 되어 있지 않은 경우
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;배치 데이터를 지원하는 교차 엔트로피 오차 구현&lt;/li&gt;
  &lt;li&gt;y는 신경망의 출력, t는 정답 레이블&lt;/li&gt;
  &lt;li&gt;정답에 해당하는 신경망의 출력만으로 교차 엔트로피 오차를 계산할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;425-왜-손실-함수를-설정하는가&quot;&gt;4.2.5 왜 손실 함수를 설정하는가?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;모델의 궁극적인 목적은 높은 정확도를 끌어내는 매개변수 값을 찾아내는 것&lt;/li&gt;
  &lt;li&gt;신경망 학습에서는 최적의 매개변수를 탐색할 때 손실 함수의 값을 가능한 작게 하는 매개변수 값을 찾음&lt;/li&gt;
  &lt;li&gt;이때 매개변수의 미분 (기울기)을 계산하여, 그 값을 단서로 값을 서서히 갱신하는 과정을 반복&lt;/li&gt;
  &lt;li&gt;가중치 매개변수의 손실 함수 미분이라는 것:
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;가중치 매개변수 값을 아주 조금 변화시켰을 때, 손실 함수가 어떻게 변하는지&lt;/strong&gt;에 대한 것&lt;/li&gt;
      &lt;li&gt;미분 값이 음수면, 그 가중치 매개변수를 양의 방향으로 변화시켜서 손실 함수의 값을 줄일 수 있음&lt;/li&gt;
      &lt;li&gt;반대로 미분 값이 양수면, 가중치 매개변수를 음의 방향으로 변화시켜서 손실 함수 값을 줄일 수 있음&lt;/li&gt;
      &lt;li&gt;하지만 미분 값이 0이면, 가중치 매개변수를 어느 쪽으로 움직여도 손실 함수 값은 변화하지 않음&lt;/li&gt;
      &lt;li&gt;따라서 매개변수 갱신은 중단&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;정확도를 지표로 삼게 되면, 결과값이 연속적으로 변화하지 못하고, 불연속적으로 띄엄띄엄한 값으로 변화&lt;/li&gt;
  &lt;li&gt;한편, 손실 함수를 지표로 삼으면, 매개변수 값이 변할 때마다 그에 반응하여 손실 함수도 연속적으로 값이 변화&lt;/li&gt;
  &lt;li&gt;따라서 활성화 함수로 계단 함수를 사용하지 않는 이유 역시, 미분값이 불연속적으로 계산되기 때문임&lt;/li&gt;
  &lt;li&gt;시그모이드 함수 같은 경우에는, 출력이 연속적으로 변하면서 곡선의 기울기 역시 연속적으로 변화함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;43-수치-미분&quot;&gt;4.3 수치 미분&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;경사법에서는 기울기 (경사) 값을 기준으로 나아갈 방향을 정함&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;431-미분&quot;&gt;4.3.1 미분&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;미분&lt;/strong&gt;은 한순간의 변화량을 표시한 것, &lt;strong&gt;수치 미분&lt;/strong&gt;은 아주 작은 차분으로 미분하는 것&lt;/li&gt;
  &lt;li&gt;미분을 수식으로 표현하면 아래와 같은데, 이 뜻은 x의 작은 변화가 함수 $f(x)$를 얼마나 변화시키느냐에 대한 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[\frac{df(x)}{dx} = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0.0001
&lt;/span&gt;    &lt;span class=&quot;nf&quot;&gt;return &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;432-수치-미분의-예&quot;&gt;4.3.2 수치 미분의 예&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$y = 0.01x^2 + 0.1x$를 그래프로 표현하면 아래와 같음&lt;/li&gt;
  &lt;li&gt;해당 식에서 x가 5일 때와 10일 때의 미분 결과는 각각 0.2, 0.3 정도&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pylab&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;20.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0에서 20까지 0.1 간격의 배열 x를 만듦
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'x'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'f(x)'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;numerical_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 0.1999999999990898
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;numerical_diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0.2999999999986347
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;433-편미분&quot;&gt;4.3.3 편미분&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$f(x_0, x_1) = x_0^2 + x_1^2$ 라는 식을 파이썬으로 구현하면 아래와 같음&lt;/li&gt;
  &lt;li&gt;이 식을 미분하려고 할때, 주의해야 하는 것은 변수가 2개이므로, 어느 변수에 대한 미분인지가 중요&lt;/li&gt;
  &lt;li&gt;이처럼 변수가 여럿인 함수에 대한 미분을 &lt;strong&gt;편미분&lt;/strong&gt;이라고 정의&lt;/li&gt;
  &lt;li&gt;편미분은 변수가 하나인 미분과 마찬가지로 특정 장소의 기울기를 구하는 것&lt;/li&gt;
  &lt;li&gt;단, 여러 변수들 중에서 목표 변수 하나에 초점을 맞추고, 다른 변수는 값을 고정함&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;function_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;44-기울기-gradient&quot;&gt;4.4 기울기 (Gradient)&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;모든 변수의 편미분을 벡터로 정리한 것&lt;/li&gt;
  &lt;li&gt;아래 그림은 $x_0^2 + x_1^2$의 기울기를 나타내는 그림&lt;/li&gt;
  &lt;li&gt;아래 그림에서 확인할 수 있듯, 기울기는 함수의 &lt;strong&gt;가장 낮은 장소 (최솟값)&lt;/strong&gt;를 가리키는 것 같은 모양&lt;/li&gt;
  &lt;li&gt;가장 낮은 곳에서 멀어질수록 화살표의 크기가 커짐&lt;/li&gt;
  &lt;li&gt;하지만 정확하게 말한다면, 기울기는 각 지점에서 낮아지는 방향을 가리킴
    &lt;ul&gt;
      &lt;li&gt;즉, 기울기가 가리키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향&lt;/li&gt;
      &lt;li&gt;Global Optima, Local Optima 개념 참고&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;amp;fname=https%3A%2F%2Fk.kakaocdn.net%2Fdn%2FbADIrW%2Fbtra6IQukSn%2Fu5FHuUcArKf3FKp5V6skik%2Fimg.png&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 0.001
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# x와 형상이 같은 배열 생성
&lt;/span&gt;    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# f(x + h) 계산
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;tmp_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fxh1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# f(x - h) 계산
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;fxh2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fxh1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fxh2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp_val&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 값 복원
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# array([6., 8.])
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# array([0., 4.])
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# array([6., 0.])
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;441-경사법-경사-하강법&quot;&gt;4.4.1 경사법 (경사 하강법)&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;기계학습에서는 학습 단계에서 최적의 매개변수 (가중치와 편향)를 학습에서 찾음&lt;/li&gt;
  &lt;li&gt;최적이라는 것은 손실함수가 최솟값이 될 때의 매개변수 값&lt;/li&gt;
  &lt;li&gt;하지만 손실함수는 복잡하기 때문에, 어디가 최솟값인지 찾기 힘듦&lt;/li&gt;
  &lt;li&gt;따라서 기울기를 잘 이용하여 함수의 (가능한 최대한 작은) 최솟값을 찾으려는 것이 &lt;strong&gt;경사법&lt;/strong&gt;의 개념&lt;/li&gt;
  &lt;li&gt;각 지점에서 함수의 값을 낮추는 방안을 제시하는 지표가 &lt;strong&gt;기울기&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;하지만 기울기가 가리키는 곳에 함수의 최솟값이 있는지 보장할 수는 없음&lt;/li&gt;
      &lt;li&gt;복잡한 함수에서는 기울기가 가리키는 방향에 최솟값이 없을 수도 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;그렇기 때문에 기울기 정보를 단서로 하여 나아갈 방향을 정하는 경사법 개념이 도입&lt;/li&gt;
  &lt;li&gt;경사법은 현 위치에서 기울어진 방향으로 일정 거리만큼 이동한 후, 이동한 곳에서 기울기를 구하고, 이 과정을 반복&lt;/li&gt;
  &lt;li&gt;이렇게 함수의 값을 점차 줄이는 것이 &lt;strong&gt;경사법 (Gradient method)&lt;/strong&gt;이고, 기계학습을 최적화 하는데 사용
    &lt;ul&gt;
      &lt;li&gt;경사법에서 최솟값을 찾는 과정을 &lt;strong&gt;경사 하강법 (Gradient descent method)&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;경사법에서 최댓값을 찾는 과정을 &lt;strong&gt;경사 상승법 (Gradient ascent method)&lt;/strong&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[x_0 = x_0 - \eta \frac{\partial f}{\partial x_0}\]

\[x_1 = x_1 - \eta \frac{\partial f}{\partial x_1}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;경사법을 수식으로 표현하면 위와 같음&lt;/li&gt;
  &lt;li&gt;$\eta$는 갱신하는 양을 나타내고, 신경망 학습에서는 &lt;strong&gt;학습률 (Learning rate)&lt;/strong&gt;이라고 정의&lt;/li&gt;
  &lt;li&gt;한 번의 학습으로 얼마만큼 학습해야 할지, 즉 매개변수 값을 얼마나 갱신하는지를 정하는 것&lt;/li&gt;
  &lt;li&gt;일반적으로 학습률은 0.01, 0.001 등의 값으로 정하는데, 이 값이 너무 크거나 작으면 좋은 장소로 찾아가기 힘듦&lt;/li&gt;
  &lt;li&gt;학습률과 같은 매개변수를 &lt;strong&gt;하이퍼파라미터 (Hyper parameter)&lt;/strong&gt;라고 정의
    &lt;ul&gt;
      &lt;li&gt;신경망의 가중치 매개변수는 훈련 데이터와 알고리즘으로 자동으로 획득되는 매개변수&lt;/li&gt;
      &lt;li&gt;하지만 학습률 같은 하이퍼파라미터는 사람이 직접 설정해야 하는 매개변수&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# f = 최적화 하려는 함수, init_x = 초깃값, lr = 학습률, step_num = 경사법에 따른 반복 횟수
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step_num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 학습률에 따른 경사하강법의 결과
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;3.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step_num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# array([-6.11110793e-10,  8.14814391e-10])
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;10.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step_num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# array([2.34235971e+12, -3.96091057e+12])
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;function_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step_num&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# array([-2.99999994,  3.99999992])
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;442-신경망에서의-기울기&quot;&gt;4.4.2 신경망에서의 기울기&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;기울기는 가중치 매개변수에 대한 손실 함수의 기울기&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/Users/paul/Desktop/github/deep-learning-from-scratch-master/'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;common.functions&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy_error&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;common.gradient&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numerical_gradient&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;simpleNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# x = 입력 데이터, t = 정답 레이블
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 정규분포로 초기화
&lt;/span&gt;    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;simpleNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;             &lt;span class=&quot;c1&quot;&gt;# 가중치 매개변수
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                 &lt;span class=&quot;c1&quot;&gt;# [1.25184467 0.34731474 1.30469603]
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;             &lt;span class=&quot;c1&quot;&gt;# 최댓값의 인덱스 = 2
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 정답 레이블
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;# 2.125447431676497 
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;45-학습-알고리즘-구현하기&quot;&gt;4.5 학습 알고리즘 구현하기&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;전제: 신경망에는 적용 가능한 가중치와 편향이 있음. 이 값들을 훈련 데이터에 적응하도록 조정하는 것을 학습이라고 정의&lt;/li&gt;
  &lt;li&gt;1단계: 미니배치
    &lt;ul&gt;
      &lt;li&gt;훈련 데이터 중 일부를 무작위로 가져오기&lt;/li&gt;
      &lt;li&gt;선별한 데이터를 미니배치라고 하고, 그 미니배치의 손실함수 값을 줄이는 것이 목표&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;2단계: 기울기 산출
    &lt;ul&gt;
      &lt;li&gt;미니배치의 손실함수 값을 줄이기 위해 각 가중치 매개변수의 기울기 구하기&lt;/li&gt;
      &lt;li&gt;기울기는 손실함수의 값을 가장 작게 하는 방향 제시&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3단계: 매개변수 갱신
    &lt;ul&gt;
      &lt;li&gt;가중치 매개변수를 기울기 방향으로 아주 조금 갱신&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;4단계: 1~3단계를 반복&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이 과정이 신경망 학습이 이뤄지는 순서. 경사하강법으로 매개변수를 갱신하는 방법&lt;/li&gt;
  &lt;li&gt;이때 데이터를 미니배치로 무작위로 선정하므로, &lt;strong&gt;확률적 경사 하강법 (Stochastic gradient descent, SGD)&lt;/strong&gt;로 정의&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;451-2층-신경망-클래스-구현하기&quot;&gt;4.5.1 2층 신경망 클래스 구현하기&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/Users/paul/Desktop/github/deep-learning-from-scratch-master/'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;common.functions&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cross_entropy_error&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;common.gradient&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numerical_gradient&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_init_std&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.01&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# 가중치 초기화
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_init_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_init_std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# x = 입력 데이터, t = 정답 레이블
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt; 
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cross_entropy_error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss_W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss_W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grads&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (784, 100)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (100, )
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (100, 10)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (10, )
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;452-미니배치-학습-구현하기&quot;&gt;4.5.2 미니배치 학습 구현하기&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;미니배치 학습: 훈련 데이터 중 일부를 무작위로 꺼내고 (미니배치), 그 미니배치에 대해 경사법으로 매개변수를 갱신&lt;/li&gt;
  &lt;li&gt;아래 예시에서는 미니배치 크기를 100으로 설정&lt;/li&gt;
  &lt;li&gt;60,000개의 훈련 데이터에서 임의로 100개 데이터를 추리고, 이 데이터로 확률적 경사하강법으로 매개변수를 갱신&lt;/li&gt;
  &lt;li&gt;갱신 횟수 (반복 횟수)는 10,000번으로 설정하고, 갱신 할때마다 계산되는 손실 함수 값을 배열에 추가&lt;/li&gt;
  &lt;li&gt;결과를 보면, 학습 횟수가 늘어날수록 손실 함수의 값이 줄어드는 것을 알 수 있음&lt;/li&gt;
  &lt;li&gt;이 결과는 신경망의 가중치 매개변수가 서서히 데이터에 적응하면서 학습되고 있다는 것을 뜻함&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset.mnist&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_mnist&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch04.two_layer_net&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
    &lt;span class=&quot;nf&quot;&gt;load_mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_loss_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 하이퍼파라미터
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 반복횟수
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 미니배치 크기
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 미니배치 획득
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 기울기 계산
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# grad = network.gradient(x_batch, t_batch) # 성능 개선판
&lt;/span&gt;    
    &lt;span class=&quot;c1&quot;&gt;# 매개변수 갱신
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
    &lt;span class=&quot;c1&quot;&gt;# 학습 경과 기록
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_loss_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;453-시험-데이터로-평가하기&quot;&gt;4.5.3 시험 데이터로 평가하기&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;앞서 훈련된 모델은 훈련 데이터에 대한 결과이므로, 다른 데이터셋에서도 동작하는지 확인이 필요함&lt;/li&gt;
  &lt;li&gt;즉, 훈련 데이터에서만 학습한 결과인 오버피팅 되었는지를 확인해봐야 함&lt;/li&gt;
  &lt;li&gt;신경망 학습의 목표는 범용적인 능력을 익히는 것 즉, 일반화라고 할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset.mnist&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_mnist&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ch04.two_layer_net&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TwoLayerNet&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
    &lt;span class=&quot;nf&quot;&gt;load_mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;TwoLayerNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hidden_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 하이퍼파라미터
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters_num&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10000&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 반복횟수
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 미니배치 크기
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;train_loss_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_acc_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_acc_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 1 에폭 당 반복 수
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iter_per_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iters_num&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 미니배치 획득
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;choice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;t_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 기울기 계산
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;numerical_gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# grad = network.gradient(x_batch, t_batch) # 성능 개선판
&lt;/span&gt;    
    &lt;span class=&quot;c1&quot;&gt;# 매개변수 갱신
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;params&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        
    &lt;span class=&quot;c1&quot;&gt;# 학습 경과 기록
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_loss_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;c1&quot;&gt;# 1 에폭 당 정확도 계산
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;iter_per_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;accuracy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_acc_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;test_acc_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;train acc, test acc: &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;', '&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;46-정리&quot;&gt;4.6 정리&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;손실함수를 기준으로 그 값이 가장 작아지는 가중치 매개변수를 찾아내는 것이 신경망 학습의 목표&lt;/li&gt;
  &lt;li&gt;기계학습에서 사용하는 데이터셋은 훈련 데이터와 시험 데이터로 나눠 사용&lt;/li&gt;
  &lt;li&gt;훈련 데이터로 학습한 모델의 범용 능력을 시험 데이터로 평가&lt;/li&gt;
  &lt;li&gt;가중치 매개변수를 갱신할 때는 가중치 매개변수의 기울기를 이용하고, 기울어진 방향으로 가중치 값을 갱신 반복&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 밑바닥부터 시작하는 딥러닝1 책 리뷰 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/Deep-learning-from-scratch/blob/main/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5.ipynb&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 14 Oct 2022 22:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-4%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D%ED%95%99%EC%8A%B5/</guid>
        
        <category>AI</category>
        
        <category>밑시딥1</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Review - IT Book</category>
        
        <category>밑바닥부터 시작하는 딥러닝1</category>
        
      </item>
    
      <item>
        <title>대학원생 때 알았더라면 좋았을 것들 책 Review</title>
        <description>&lt;table align=&quot;center&quot; width=&quot;100&quot; height=&quot;50&quot;&gt;
    &lt;tr&gt;
        &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;http://image.yes24.com/goods/72231788/XL&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;간단한-책-소개&quot;&gt;간단한 책 소개&lt;/h2&gt;
</description>
        <pubDate>Thu, 13 Oct 2022 01:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EB%8C%80%ED%95%99%EC%9B%90%EC%83%9D-%EB%95%8C-%EC%95%8C%EC%95%98%EB%8D%94%EB%9D%BC%EB%A9%B4-%EC%A2%8B%EC%95%98%EC%9D%84-%EA%B2%83%EB%93%A4/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EB%8C%80%ED%95%99%EC%9B%90%EC%83%9D-%EB%95%8C-%EC%95%8C%EC%95%98%EB%8D%94%EB%9D%BC%EB%A9%B4-%EC%A2%8B%EC%95%98%EC%9D%84-%EA%B2%83%EB%93%A4/</guid>
        
        <category>대학원</category>
        
        <category>석사</category>
        
        <category>박사</category>
        
        <category>진학</category>
        
        
        <category>Review - Book</category>
        
        <category>대학원생 때 알았더라면 좋았을 것들</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 4주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 4주차 강의에 대한 정리입니다. &lt;br /&gt;
지금까지 기초 파이썬과 함께 데이터를 다루는 Pandas, Numpy, Matplotlib 등을 학습했습니다. &lt;br /&gt;
이번 주에는 앞서 배웠던 내용들을 바탕으로 Flask라는 프레임워크와 함께 데이터를 EDA 하는 방법과, &lt;br /&gt;
AWS 클라우드를 활용하여 머신러닝 모델을 Serving 하는 API를 만드는 방법에 대해 배웁니다. &lt;br /&gt;
지금까지는 모델을 만드는 방법만을 학습했는데, 실제 서비스를 위해 필요한 내용들을 배울 수 있을 것 같아 기대가 됩니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-web-application-with-flask&quot;&gt;1. Web Application with Flask&lt;/h2&gt;

&lt;h3 id=&quot;flask-설치하기&quot;&gt;Flask 설치하기&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;지금까지 학습한 것
    &lt;ul&gt;
      &lt;li&gt;Data Structure, Algorithm&lt;/li&gt;
      &lt;li&gt;Numpy, Pandas, Matplotlib, EDA&lt;/li&gt;
      &lt;li&gt;Calculus, Linear Algebra, Probability &amp;amp; Statistics&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Flask: Python 기반 마이크로 웹 프레임워크&lt;/li&gt;
  &lt;li&gt;가상환경 설치하기&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;virtualenv       &lt;span class=&quot;c&quot;&gt;# 가상환경을 만들 수 있는 virtualenv 패키지 설치&lt;/span&gt;
virtualenv venv              &lt;span class=&quot;c&quot;&gt;# 가상환경 이름이 venv인 가상환경 만들기&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;venv/bin/activate     &lt;span class=&quot;c&quot;&gt;# venv라는 가상환경에 진입하기 (mac OS)&lt;/span&gt;
./venv/Scripts/activate.bat  &lt;span class=&quot;c&quot;&gt;# venv라는 가상환경에 진입하기 (Windows OS)&lt;/span&gt;

pip freeze                   &lt;span class=&quot;c&quot;&gt;# 현재 가상환경에 설치된 패키지 확인&lt;/span&gt;
pip &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;flask            &lt;span class=&quot;c&quot;&gt;# 들어온 가상환경에 flask 패키지를 설치&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;인터넷과-웹&quot;&gt;인터넷과 웹&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;인터넷 (Internet): 전 세계 컴퓨터를 하나로 합치는 거대한 통신망&lt;/li&gt;
  &lt;li&gt;웹 (Web): 인터넷에 연결된 사용자들이 정보를 공유할 수 있는 공간&lt;/li&gt;
  &lt;li&gt;Web의 동작 방식
    &lt;ul&gt;
      &lt;li&gt;웹은 클라이언트와 서버 사이의 소통
        &lt;ul&gt;
          &lt;li&gt;클라이언트가 서버에 정보를 요청 (HTTP Request)&lt;/li&gt;
          &lt;li&gt;서버는 이 요청받은 정보에 대한 처리를 진행&lt;/li&gt;
          &lt;li&gt;서버가 클라이언트에게 요청에 대해 응답 (HTTP Response)&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rest-api&quot;&gt;REST API&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;API (Application Programming Interface): 프로그램들이 서로 상호작용 하는 것을 도와주는 매개체&lt;/li&gt;
  &lt;li&gt;RESTful (Representational State Transfer)
    &lt;ul&gt;
      &lt;li&gt;웹 서버가 요청을 응답하는 방법론 중 하나&lt;/li&gt;
      &lt;li&gt;데이터가 아닌, 자원 (Resource)의 관점으로 접근&lt;/li&gt;
      &lt;li&gt;클라이언트의 Context를 서버에서 유지하지 않음 (Stateless - 무상태성)
        &lt;ul&gt;
          &lt;li&gt;GET 요청이 들어왔다고 해서, 서버 입장에서 요청 받지 않았던 POST 요청을 실행하지 않음&lt;/li&gt;
          &lt;li&gt;GET 요청이 들어왔다면, GET 요청에 대한 Response만 보냄&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;HTTP URI를 통해 자원을 명시하고, HTTP Method를 통해 해당 자원에 대한 CRUD를 진행
        &lt;ul&gt;
          &lt;li&gt;HTTP Method: GET, POST, PUT, DELETE&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Postman: API를 테스트 할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-클라우드를-활용한-머신러닝-모델-serving-api-개발&quot;&gt;2. 클라우드를 활용한 머신러닝 모델 Serving API 개발&lt;/h2&gt;

&lt;h3 id=&quot;클라우드-기초&quot;&gt;클라우드 기초&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;과거에 인터넷 환경에서 서비스를 제공하려면 서비스 호스팅에 필요한 모든 것을 직접 구축했어야 함&lt;/li&gt;
  &lt;li&gt;하지만 서버를 직접 구축 및 운영하는 자원과 인력 비용이 크고, 변화에 대응하는 것이 어려움&lt;/li&gt;
  &lt;li&gt;따라서 서버 운영에 필요한 공간, 네트워크 등을 제공하는 &lt;strong&gt;IDC (Internet Data Center) 서비스&lt;/strong&gt; 등장&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;하지만 IDC 역시 서버 임대를 통해 자원을 효율적으로 이용하고 비용 절감이 가능하지만, 유연성이 떨어진다는 한계&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;기존의 서버 구축 및 운영 방식으로는 적절한 시간에 필요한 서비스를 사용자에게 제공하기 힘들었음&lt;/li&gt;
  &lt;li&gt;사용자가 늘면서 다양한 서비스를 제공하면서 필요할 때, 필요한 만큼 서버를 증설하기 원하는 온디맨드 수요 증가&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ex) 접속량이 늘어서 컴퓨팅 수요 증가시에 오토 스케일링 필요, 평상 시에 사용하지 않는 유휴 자원은 비용에서 제거&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;클라우드 컴퓨팅 (Colud Computing)&lt;/strong&gt;은 언제 어디서나 필요한 만큼의 컴퓨팅 자원을 필요한 시간만큼 활용 가능&lt;/li&gt;
  &lt;li&gt;아마존이 2006년 클라우드를 통한 저장공간 및 연산 자원 제공 서비스인 S3과 EC2를 개시하면서 등장&lt;/li&gt;
  &lt;li&gt;클라우드를 통해 빅데이터 수집, 저장, 분석을 위한 방대한 컴퓨팅 자원과 인공지능 개발을 위한 IT 환경 마련 가능
    &lt;ul&gt;
      &lt;li&gt;속도, 접근성, 확장성, 생산성, 보안 및 안정성, 측정 가능성 등의 장점을 갖음&lt;/li&gt;
      &lt;li&gt;도커와 같은 가상화 기술을 통해 GPU 활용과 소프트웨어 설치 및 배포 등의 작업에 비용과 시간 절감&lt;/li&gt;
      &lt;li&gt;클라우드 컴퓨팅 운용 모델: 구축 및 배포 유형에 따라 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Public, Private, Hybrid&lt;/code&gt;로 구분
        &lt;ul&gt;
          &lt;li&gt;Public: 모든 인프라와 IT 기술을 클라우드에서 사용, IT 관리 인력 및 인프라 구축 비용 없는 경우 유용&lt;/li&gt;
          &lt;li&gt;Private: 고객이 자체 데이터센터에서 직접 클라우드 서비스를 구축, 보안이 좋고 커스터마이제이션 가능&lt;/li&gt;
          &lt;li&gt;Hybrid: 고객이 핵심 시스템은 내부에 두면서 외부 클라우드 활용, Public 경제성과 Private 보안성 활용&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;클라우드 서비스 제공 방식에 따라서 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IaaS, PaaS, SaaS&lt;/code&gt; 형태로 구분
        &lt;ul&gt;
          &lt;li&gt;On-Premises: Owning a car&lt;/li&gt;
          &lt;li&gt;IaaS: Leasing a car&lt;/li&gt;
          &lt;li&gt;PaaS: Taking a taxi&lt;/li&gt;
          &lt;li&gt;SaaS: Going by bus&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;클라우드 서비스 제공 사업자: AWS, GCP, Azure, NCP (네이버 클라우드 플랫폼) 등&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;api-to-serve-ml-model&quot;&gt;API to serve ML model&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Architecture: AWS EC2와 Flask 기반 모델 학습 및 추론을 요청, 응답하는 API 서버 개발&lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221010_1.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Interface: 사용자는 기계와 소프트웨어를 제어하기 위해 인터페이스를 정해진 메뉴얼에 따라 원하는 경험 획득&lt;/li&gt;
  &lt;li&gt;API (Application Programming Interface): 기계&amp;amp;기계, 소프트웨어&amp;amp;소프트웨어 커뮤니케이션을 위한 인터페이스&lt;/li&gt;
  &lt;li&gt;RESTful API for ML/DL Model Interface
    &lt;ul&gt;
      &lt;li&gt;RESTful API: REST 아키텍처를 따르는 API&lt;/li&gt;
      &lt;li&gt;데이터나 정보의 교환, 요청 등을 위한 인터페이스를 REST 아키텍쳐를 따라 구현한 API&lt;/li&gt;
      &lt;li&gt;데이터 값을 담아 요청하고, 모델이 추론한 결과에 대한 return을 json 형태로 반환하도록 설계&lt;/li&gt;
      &lt;li&gt;RESTful API는 요청 메시지만 봐도 어떤 내용으로 되어 있는지 알 수 있도록 표현&lt;/li&gt;
      &lt;li&gt;HTTP URI를 통해 자원을 명시하고, HTTP Method를 통해 필요한 연산을 요청하고 반환하는 API&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Practical process of machine learning
    &lt;ul&gt;
      &lt;li&gt;문제 정의, 데이터 준비, 모델 학습 및 검증, 모델 배포, 모니터링 등의 과정을 통해 ML/DL 모델 적용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221010_2.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model Serving
    &lt;ul&gt;
      &lt;li&gt;학습된 모델을 REST API로 배포하기 위해 학습된 모델의 Serialization과 웹 프레임워크를 통해 배포 준비 필요&lt;/li&gt;
      &lt;li&gt;모델을 서빙할 때는 학습 시의 데이터 분포나 처리 방법과의 연속성 유지 필요&lt;/li&gt;
      &lt;li&gt;모델을 배포하는 환경에 따라 다양한 Serving Framework를 고려하여 활용&lt;/li&gt;
      &lt;li&gt;Model Training -&amp;gt; Serializing Model -&amp;gt; Serving Model&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Serialization &amp;amp; De-serialization: 학습한 모델의 재사용 및 배포를 위해 저장하고 불러오는 것
    &lt;ul&gt;
      &lt;li&gt;Serialization을 통해 ML/DL 모델 object를 disk에서 불러와 어디든 전송하고 불러올 수 있는 형태로 변환&lt;/li&gt;
      &lt;li&gt;De-serialization을 통해 파이썬 혹은 다른 환경에서 모델을 불러와서 추론 및 학습에 사용&lt;/li&gt;
      &lt;li&gt;모델을 배포하는 환경을 고려하여 환경에 맞는 올바른 방법으로 Serialization 해야지 De-serialization 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Model Serving을 위한 다양한 Frameworks: TensorFlow serving, TorchServe, TensorRT
    &lt;ul&gt;
      &lt;li&gt;Flask 같은 웹 프레임워크는 클라이언트로부터 요청을 처리하기 위해 주로 사용&lt;/li&gt;
      &lt;li&gt;별도의 모델 추론을 위한 API 서버를 운용하여 내부 혹은 외부 통신을 통해 예측, 추론값 반환&lt;/li&gt;
      &lt;li&gt;대용량 데이터 배치 처리와 딥러닝 모델 활용이 늘면서 multi node &amp;amp; GPU 환경에서 안정적 모델 서빙을 위함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221010_3.png&quot; /&gt;
&lt;/p&gt;

&lt;h2 id=&quot;3-데이터-씹고-뜯고-맛보고-즐기기---eda&quot;&gt;3. 데이터 씹고 뜯고 맛보고 즐기기 - EDA&lt;/h2&gt;

&lt;h3 id=&quot;eda&quot;&gt;EDA&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;데이터 그 자체만으로부터 인사이트를 얻어내는 접근법&lt;/li&gt;
  &lt;li&gt;EDA의 Process
    &lt;ul&gt;
      &lt;li&gt;분석의 목적과 변수 확인&lt;/li&gt;
      &lt;li&gt;데이터 전체적으로 살펴보기 (결측치, 상관관계 확인 등)&lt;/li&gt;
      &lt;li&gt;데이터의 개별 속성 파악하기 (각 feature에 대해 특징 확인 등)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;타이타닉 데이터를 통한 EDA
    &lt;ul&gt;
      &lt;li&gt;각 데이터는 어떤 자료형을 가지고 있는지?&lt;/li&gt;
      &lt;li&gt;데이터에 결측치는 없는지, 있다면 어떻게 해야 할지?&lt;/li&gt;
      &lt;li&gt;데이터의 자료형을 바꿔줄 필요가 있는지 (범주형의 One-hot encoding)&lt;/li&gt;
      &lt;li&gt;데이터에 대한 가설 세워보기&lt;/li&gt;
      &lt;li&gt;가설을 검증하기 위한 증거를 찾아보기&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-eda-project&quot;&gt;4. EDA Project&lt;/h2&gt;

&lt;h2 id=&quot;5-weekly-assignment&quot;&gt;5. Weekly Assignment&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;6-4주차-돌아보기&quot;&gt;6. 4주차 돌아보기&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;기간: 2022. 10. 10 ~ 2022. 10. 15&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;데브코스가 시작한지 벌써 한 달이 지났다니 실감이 나지 않는 것 같다. 시간이 정말 빠르다. 퇴사 후에 스스로의 시간을 경영하는 방법을 배우고 있는 것 같은데, 여러가지 계획들과 해야 할 것들을 고민하는 것 역시 좋다고 생각한다. 하지만 가장 중요한 것은 생각하는 것들을 직접 행하는 것이 가장 중요할 것이다. 생각만 하지 말고, 말만 하지 말고 그것들을 실천에 옮길 수 있는 사람이 되자. 기존과 다르게 스스로에게 주어진 많은 시간 속에 행복해하며, 이것도 하고, 저것도 하자는 거창한 계획들 속에서 가장 중요한 것은 그것을 행하는 것이다.&lt;/p&gt;

&lt;p&gt;데브코스를 진행하면서, 개인적으로 관심사가 비슷한 분들과 함께 사이드 프로젝트도 진행을 해보고 있다. 기술이 가지는 가치 중 하나는, 우리가 불편했지만 그냥 넘겼던 것들에 대해 다시 생각해보면서 기술로써 그 불편함을 해결할 수 있다는 것 같다. 실제로 우리가 평소에 사용하고 있는 대부분의 서비스들은 많은 부연 설명 없이 아주 간단하게 컨셉을 설명할 수 있는 서비스들이다. 이렇듯 이 사이드 프로젝트에서도 우리가 평소에 느꼈던 문제를 공감하고, 그것들을 배우고 있고, 배웠던 기술들을 활용하여 해결해보고자 한다.&lt;/p&gt;

&lt;p&gt;또 이와 관련하여 공모전도 함께 참여할 계획을 가지고 있는데, 차근히 잘 준비하고 이 블로그에서 또 해당 내용들을 열심히 차근히 기록해보도록 하겠다. 다음 주도 새로운 배움으로 가득하길 기대하자!!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 프로그래머스 인공지능 데브코스 4기 4주차 강의 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/04%EC%A3%BC%EC%B0%A8&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 10 Oct 2022 10:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-4%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-4%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
      <item>
        <title>SQL 코딩의 기술 1강. 데이터 모델 설계</title>
        <description>&lt;table align=&quot;center&quot; width=&quot;100&quot; height=&quot;50&quot;&gt;
    &lt;tr&gt;
        &lt;td align=&quot;center&quot;&gt;&lt;img src=&quot;http://image.yes24.com/goods/56947533/XL&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;h2 id=&quot;간단한-책-소개&quot;&gt;간단한 책 소개&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;이 책은 데이터베이스를 다룰 수 있는 SQL의 개념과 사용 방법을 소개하고 있습니다. &lt;br /&gt;
대표적인 DBMS인 오라클, SQL Server, MySQL, PostgreSQL 등을 비교하면서 보여줍니다. &lt;br /&gt;
회사를 다녀보니, 데이터를 다루는 과정에서는 파이썬보다 SQL과 DB에 대한 이해가 더 필요하다는 것을 느꼈습니다. &lt;br /&gt;
따라서 데이터베이스를 다룰 수 있는 (거의 유일한) SQL을 학습하며, DB에 대한 이해도를 높이려고 합니다. &lt;br /&gt;
이 책에서는 다양한 DBMS를 소개하고 있지만, 실제로 가장 많이 사용되고 있고 제 필요에 따라서, &lt;br /&gt;
DB의 기초 내용과 MySQL, PostgreSQL 등의 내용을 중점적으로 살펴 볼 예정입니다.&lt;/p&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&quot;http://www.yes24.com/Product/Goods/56947533&quot;&gt;SQL 코딩의 기술&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&quot;https://github.com/gilbutITbook/006882&quot;&gt;SQL 코딩의 기술 Github 링크&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Chapter&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Title&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Main Topics&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 모델 설계&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;기본키, 외래키, 중복 데이터 제거, 정규화, 역정규화&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;인덱스 설계와 프로그램적 처리&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;인덱스, 트리거, 선언적 제약 조건&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 모델 설계를 변경할 수 없는 경우&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;뷰, ETL, 요약 테이블, UNION&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 필터링과 검색&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;관계 대수, CASE, 다중 조건 문제, 데이터 분할, 사거블 쿼리&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;집계&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;GROUP BY, HAVING, DISTINCT, 윈도우 함수, 이동 집계&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;서브쿼리&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;서브쿼리, CTE, 조인&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;메타데이터 획득, 분석&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;메타데이터 수집 방법, 실행 계획의 작동 원리&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;카티전 곱&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;로우 조합&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;탤리 테이블&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;윈도우 함수, 날짜 테이블, 피벗&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;10강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;계층형 데이터 모델링&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;인접 리스트 모델, 중첩 집합&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Appendix&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;데이터 타입, 산술 연산, 함수&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;chapter-1-데이터-모델-설계&quot;&gt;&lt;font color=&quot;orange&quot;&gt;Chapter 1. 데이터 모델 설계&lt;/font&gt;&lt;/h2&gt;

&lt;h3 id=&quot;better-way-1---모든-테이블에-기본키가-있는지-확인하자&quot;&gt;&lt;font color=&quot;skyblue&quot;&gt;Better way 1 - 모든 테이블에 기본키가 있는지 확인하자&lt;/font&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;관계형 모델을 따르기 위해서는 한 테이블에 있는 특정 row와 다른 row를 구별할 수 있어야 함&lt;/li&gt;
  &lt;li&gt;테이블에 기본키가 없으면, 일관성 없는 데이터가 쌓여 쿼리 속도가 느리고, 정확한 정보 조회가 불가능 할 수도 있음&lt;/li&gt;
  &lt;li&gt;So, 모든 테이블에는 column 한 개 이상으로 구성된 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;기본키 (Primary Key)&lt;/code&gt;가 필요
    &lt;ul&gt;
      &lt;li&gt;기본키는 row마다 유일해야 함&lt;/li&gt;
      &lt;li&gt;기본키는 null 값을 가질 수 없음&lt;/li&gt;
      &lt;li&gt;기본키는 안정적인 값이어야 함 (값을 갱신할 필요가 없음)&lt;/li&gt;
      &lt;li&gt;기본키는 가능한 간단한 형태이어야 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기본키 설정을 하기 위한 가장 일반적인 방법은 의미 없는 숫자 데이터로 자동 생성되는 컬럼을 기본키로 만드는 것
    &lt;ul&gt;
      &lt;li&gt;RDBMS에 따라 이름이 구분되는데, DB2, SQL Server, 오라클 12c에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;IDENTITY&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;액세스에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AutoNumber&lt;/code&gt;, MySQL,에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AUTO_INCREMENT&lt;/code&gt;, PostgreSQL에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;serial&lt;/code&gt; 컬럼&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;RDBMS에서 &lt;strong&gt;참조 무결성 (Reference Integrity, RI)&lt;/strong&gt;이라는 개념은 매우 중요
    &lt;ul&gt;
      &lt;li&gt;RI를 준수한다는 것은 null이 아닌 외래키 (Foreign Key)가 설정된 자식 테이블의 각 레코드와 일치하는 레코드가 부모 테이블에 존재한다는 것&lt;/li&gt;
      &lt;li&gt;ex) Orders 테이블에서 고객 정보 컬럼에 외래키를 설정하여 Customers 테이블의 기본키와 연결된 것&lt;/li&gt;
      &lt;li&gt;이렇게 되면, 같은 이름을 가진 고객이 있어도 Customers 테이블의 각 로우는 유일하기 때문에 고객 식별 가능&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;복합 기본키 (Compound Primary Key)는 다음의 이유로 효율성이 떨어지므로 지양하기
    &lt;ul&gt;
      &lt;li&gt;기본키를 정의 할 때, 해당 컬럼에 유일한 인덱스를 만드는데, 컬럼 두 개 이상에 인덱스를 만드는게 비효율적&lt;/li&gt;
      &lt;li&gt;기본키로 조인을 수행하는데, 기본키가 여러 컬럼으로 구성되면 쿼리가 복잡하고 느려짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;better-way-2---중복으로-저장된-데이터-항목을-제거하자&quot;&gt;&lt;font color=&quot;skyblue&quot;&gt;Better way 2 - 중복으로 저장된 데이터 항목을 제거하자&lt;/font&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;데이터가 중복으로 저장된다면?
    &lt;ul&gt;
      &lt;li&gt;일관되지 않은 데이터 이슈&lt;/li&gt;
      &lt;li&gt;비정상적인 삽입, 갱신, 삭제 처리 이슈&lt;/li&gt;
      &lt;li&gt;디스크 공간 낭비 이슈&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;정규화 (Normalization)&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;중복 데이터를 저장하면서 발생하는 문제를 없애려고 정보를 주제 (subject) 별로 분할하는 프로세스&lt;/li&gt;
      &lt;li&gt;여기서 ‘중복’이라는 것은 사용자가 동일한 데이터를 한 군데 이상에서 입력하는 것을 뜻함&lt;/li&gt;
      &lt;li&gt;정규화의 목표는 한 DB에서 동일한 테이블이든, 다른 테이블이든 반복되는 데이터를 최소화하는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;외래키 제약 조건: 복수의 테이블 사이에 관계를 선언함으로써 데이터의 무결성을 보장해 주는 역할&lt;/li&gt;
&lt;/ul&gt;

&lt;figure&gt;
    &lt;img src=&quot;https://thebook.io/img/006882/038.jpg&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;그림 1-3 단일 테이블의 중복 데이터&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure&gt;
    &lt;img src=&quot;https://thebook.io/img/006882/040.jpg&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;그림 1-4 주제 별로 데이터를 테이블에서 분리&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;위 1-3 그림에서 볼 수 있는 것처럼 기존에는 정규화 되어 있지 않고, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CustomerSales&lt;/code&gt; 라는 하나의 테이블로 고객 정보와 종업원 정보, 구매 정보 등이 관리되고 있었음. 이 경우에는 고객이 물건을 구매하는 경우에 PurchaseDate에 관련된 정보만 추가되는 것이 아니라, 다른 모든 컬럼들도 영향을 받고 있음. 게다가 고객 정보가 똑같은데, 데이터가 다르게 들어오는 경우에는 처리하는게 복잡하게 될 수 있음.&lt;/p&gt;

&lt;p&gt;따라서 1-4 그림처럼 주제 (subject) 별로 분할하는 &lt;strong&gt;정규화&lt;/strong&gt; 과정을 수행하여, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;Customers, Employees, AutomobileModels, SalesTransactions&lt;/code&gt; 테이블로 나눠서 관리할 수 있음. 그러면서 기본키와 외래키를 통해 데이터의 중복을 제거하고, 효율적으로 관리할 수 있음. 또한 아래 쿼리를 통해 데이터를 다시 1-3 그림처럼 복구할 수도 있음.&lt;/p&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SalesID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CustFirstName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CustLastName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Address&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;City&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Phone&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PurchaseDate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModelYear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SalesPerson&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SalesTransactions&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;INNER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;JOIN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Customers&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CustomerID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CustomerID&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;INNER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;JOIN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Employees&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;EmployeeID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SalesPersonID&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;INNER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;JOIN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AutomobileModels&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ON&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModelID&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;st&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModelID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;better-way-3---반복-그룹을-제거하자&quot;&gt;&lt;font color=&quot;skyblue&quot;&gt;Better way 3 - 반복 그룹을 제거하자&lt;/font&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;영향도 (비용) 측면에서 컬럼은 비싸고, 로우는 싸다.&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;DB 정규화의 목표는 데이터의 반복 그룹을 제거하고, 스키마 변경을 최소화 하는 것&lt;/li&gt;
  &lt;li&gt;데이터의 반복 그룹을 제거하면, 인덱싱을 사용하여 데이터 중복을 방지하고, 쿼리도 간소화 할 수 있음&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;UNION, UNION ALL&lt;/code&gt; 쿼리를 활용&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;better-way-4---컬럼당-하나의-특성만-저장하자&quot;&gt;&lt;font color=&quot;skyblue&quot;&gt;Better way 4 - 컬럼당 하나의 특성만 저장하자&lt;/font&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;관계형 용어에서 관계 (테이블)는 오직 한 주제나 액션만 기술해야 함&lt;/li&gt;
  &lt;li&gt;단일 컬럼에 특성 값을 두 개 이상 저장하는 것은 올바르지 못함
    &lt;ul&gt;
      &lt;li&gt;검색을 하거나, 값을 집계할 때 특성 값을 분리하기 어렵기 때문&lt;/li&gt;
      &lt;li&gt;중요한 개별 특성은 자체 컬럼에 넣는 것을 고려해야 함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth ID&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth Name&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth Address&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;John L. Viescas&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;144 Boulevard Saint-Germain, 75006, Paris, France&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Douglas J. Steele&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;555 Sherbourne St., Toronto, ON M4X 1W6, Canada&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ben Clothier&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2015 Monterey St., San Antonio, TX 78207, USA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tom Wickerath&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2317 185th Place NE, Redmond, WA 98052, USA&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;위 테이블에는 다음과 같은 문제점 존재
    &lt;ul&gt;
      &lt;li&gt;불가능하지는 않지만, ‘성’을 찾기 힘듦&lt;/li&gt;
      &lt;li&gt;이름을 검색할 때, 효율성이 떨어지는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LIKE, substring 연산자&lt;/code&gt;를 사용해 이름을 추출해야 함&lt;/li&gt;
      &lt;li&gt;거리 이름, 도시, 주, 우편번호를 쉽게 찾을 수 없음&lt;/li&gt;
      &lt;li&gt;데이터를 그룹으로 묶으면, 그룹화 된 데이터에서 우편번호, 주, 국가를 추출하기 힘듦&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth ID&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth First&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth Mid&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth Last&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth St Num&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth Street&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth City&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth St Prov&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth Postal&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Auth Country&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;John&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;L.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Viescas&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;144&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Boulevard Saint-Germain&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Paris&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;75006&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;France&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Douglas&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;J.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Steele&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;555&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Sherbourne St.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Toronto&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;ON&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;M4X 1W6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Canada&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Ben&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Clothier&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2015&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Monterey St.&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;San Antonio&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;TX&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;78207&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;USA&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Tom&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Wickerath&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2317&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;185th Place NE&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Redmond&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;WA&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;98052&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;USA&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;이렇게 테이블을 수정하면 컬럼당 특성이 한 개만 있음&lt;/li&gt;
  &lt;li&gt;따라서 하나 이상의 개별 특성에서 검색이나 그룹핑을 쉽게 할 수 있음&lt;/li&gt;
  &lt;li&gt;또한 아래 SQL 쿼리를 통해서 원래 데이터를 생성할 수도 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-sql highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthorID&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthID&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CONCAT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AuthFirst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;CASE&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;WHEN&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthMid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;IS&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;NULL&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;THEN&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;ELSE&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CONCAT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthMid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;END&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthLast&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;CONCAT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AuthStNum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthStreet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;AuthCity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;', '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthStProv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;' '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;AuthPostal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;', '&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthCountry&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;AuthAddress&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Authors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;올바른 테이블 설계는 개별 특성을 자체 컬럼에 할당함. 한 컬럼에 여러 특성이 포함되지 않도록 해야 함&lt;/li&gt;
  &lt;li&gt;일부 app에서는 주소나 전화번호 같은 컬럼의 일부를 걸러내기 위해 최소 수준의 데이터 조각으로 분할해야 함&lt;/li&gt;
  &lt;li&gt;보고서나 목록을 뽑으려고 특성들을 재결합 할때는 SQL의 문자열 연결 기능을 사용함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;better-way-5---왜-계산-데이터를-저장하면-좋지-않은지-이해하자&quot;&gt;&lt;font color=&quot;skyblue&quot;&gt;Better way 5 - 왜 계산 데이터를 저장하면 좋지 않은지 이해하자&lt;/font&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;계산 컬럼을 현행화 하는 가장 원시적인 방법은 계산에 사용되는 원천 컬럼이 있는 테이블에 트리거를 추가하는 것
    &lt;ul&gt;
      &lt;li&gt;트리거는 대상 테이블에 데이터가 입력, 갱신, 삭제 될 때 수행하는 코드&lt;/li&gt;
      &lt;li&gt;하지만 트리거는 정확하게 작성하기 어렵고, 비용도 비싸다는 단점이 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;트리거보다 나은 선택은 테이블을 생성할 때 계산 컬럼을 정의하는 방법이 있음 (몇몇 DB 시스템에서 제공)
    &lt;ul&gt;
      &lt;li&gt;이렇게 하면 트리거를 작성할 때 필요한 복잡한 코드를 작성하지 않아도 된다는 장점&lt;/li&gt;
      &lt;li&gt;SQL Server에서는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;AS&lt;/code&gt; 키워드 다음에 수행할 계산을 정의하는 표현식을 붙일 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;결정적 (Dterministic) 함수: 특정 값 집합이 입력되면 언제나 동일한 결과를 반환&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;비결정적 (Nondterministic) 함수: 특정 값 집합이 입력되더라도 매번 다른 값을 반환할 수 있음&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;테이블을 정의할 때, 계산 컬럼을 정의할 수 있지만, 성능을 고려해야 함&lt;/li&gt;
  &lt;li&gt;트리거를 사용하여 계산 컬럼을 일반 컬럼처럼 정의할 수 있지만, 작성해야 할 코드가 복잡함&lt;/li&gt;
  &lt;li&gt;계산 컬럼은 DB 시스템에 추가적인 부하를 일으키므로 득과 실을 따져서 사용해야 함&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;better-way-6---참조-무결성을-보호하려면-외래키를-정의하자&quot;&gt;&lt;font color=&quot;skyblue&quot;&gt;Better way 6 - 참조 무결성을 보호하려면 외래키를 정의하자&lt;/font&gt;&lt;/h3&gt;

&lt;figure&gt;
    &lt;img src=&quot;https://thebook.io/img/006882/055.jpg&quot; /&gt;
    &lt;figcaption align=&quot;center&quot;&gt;그림 1-8 전형적인 Sales Orders 데이터베이스의 테이블 설계&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;DB 스키마를 제대로 설계하려면 관련된 부모 테이블의 기본키를 포함하도록 테이블에 외래키를 정의하는 것이 좋음&lt;/li&gt;
  &lt;li&gt;Orders 테이블은 Customers 테이블의 기본키를 가리키는 CustomerID 컬럼을 정의하여 고객 정보 식별 가능&lt;/li&gt;
  &lt;li&gt;위 사진에서 각 관계선 끝에 위치한 열쇠 기호는 한 테이블의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;기본키와 관계&lt;/code&gt;를 맺고 있음을 의미&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;반대편 끝에 있는 무한대 기호는 두 번째 테이블의 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;외래키와 일대다 관계&lt;/code&gt;를 맺고 있음을 의미&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;선언적 참조 무결성 (Declarative Referential Integrity, DRI)을 정의하여 DB 시스템은 테이블 간 관계를 알고 있음
    &lt;ul&gt;
      &lt;li&gt;일대다 관계에서 ‘다’에 해당하는 테이블에 데이터를 입력, 변경하거나 ‘일’에 해당하는 테이블의 데이터를 변경, 삭제할 때, 데이터베이스 시스템이 데이터 무결성을 강화하는데 도움을 줌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;일부 DB 시스템에서는 참조 무결성 제약 조건을 정의하면 자동으로 외래키 컬럼에 인덱스를 만듦
    &lt;ul&gt;
      &lt;li&gt;조인을 수행할 떄 성능 향상 효과가 있음&lt;/li&gt;
      &lt;li&gt;외래키 컬럼에 자동으로 인덱스가 만들어지지 않는다면, 따로라도 만드는게 좋음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;better-way-7---테이블-간-관계를-명확히-하자&quot;&gt;&lt;font color=&quot;skyblue&quot;&gt;Better way 7 - 테이블 간 관계를 명확히 하자&lt;/font&gt;&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: SQL 코딩의 기술 책 리뷰&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Fri, 07 Oct 2022 13:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/SQL-%EC%BD%94%EB%94%A9%EC%9D%98-%EA%B8%B0%EC%88%A0-1%EA%B0%95/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/SQL-%EC%BD%94%EB%94%A9%EC%9D%98-%EA%B8%B0%EC%88%A0-1%EA%B0%95/</guid>
        
        <category>SQL</category>
        
        <category>데이터베이스</category>
        
        <category>SQL 코딩의 기술</category>
        
        
        <category>Review - IT Book</category>
        
        <category>SQL 코딩의 기술</category>
        
      </item>
    
      <item>
        <title>밑시딥1 3강. 신경망</title>
        <description>&lt;p&gt;이번 글에서는 밑바닥부터 시작하는 딥러닝1 책의 3강에 대한 리뷰를 시작합니다. &lt;br /&gt;
앞서 딥러닝의 기초 개념인 퍼셉트론을 학습했는데, 이어서 딥러닝의 중요한 개념인 신경망에 대해 학습합니다. &lt;br /&gt;
퍼셉트론과 달리 신경망이 어떻게 동작하는지, 그리고 활성화 함수는 무엇인지 등에 대해서도 배울 예정입니다. &lt;br /&gt;
마지막으로는, 손글씨 숫자 이미지 데이터로 유명한 MNIST 데이터를 활용하여 결과도 확인해봅니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Chapter&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Title&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Main Topics&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;헬로 파이썬&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;파이썬 기초 문법 소개, numpy, matplotlib&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;퍼셉트론&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;AND, NAND, OR 게이트&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;3강&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;신경망&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;활성화 함수, 다차원 배열 계산, 출력층 설계, MNIST&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;신경망 학습&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;손실 함수, 경사 하강법&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;오차역전파법&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;역전파, 활성화 함수 구현&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;학습 관련 기술들&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;매개변수 갱신, 배치 정규화, 하이퍼파라미터 값 찾기&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;합성곱 신경망 (CNN)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;합성곱 계층, 풀링 계층, CNN 구현&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;딥러닝 (Deep learning)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;초기 역사, 딥러닝 활용&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Appendix&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Softmax with loss 계층의 계산 그래프&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yes24.com/Product/Goods/34970929&quot;&gt;밑바닥부터 시작하는 딥러닝 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/WegraLee/deep-learning-from-scratch&quot;&gt;밑바닥부터 시작하는 딥러닝 1 Github 링크&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;참고 - 다크 모드가 아닌 화이트 모드로 보시면 자료를 편하게 확인 가능합니다!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;chapter-3-신경망&quot;&gt;&lt;font color=&quot;orange&quot;&gt;Chapter 3. 신경망&lt;/font&gt;&lt;/h2&gt;

&lt;h3 id=&quot;31-퍼셉트론에서-신경망으로&quot;&gt;3.1 퍼셉트론에서 신경망으로&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;(Multi-layer) 퍼셉트론으로 복잡한 함수를 표현할 수 있음&lt;/li&gt;
  &lt;li&gt;But, 가중치를 설정하는 작업은 여전히 사람이 해야 함&lt;/li&gt;
  &lt;li&gt;So, 매개변수의 적절한 값을 데이터로부터 자동으로 학습할 수 있도록 하는 것이 &lt;strong&gt;신경망&lt;/strong&gt;의 특징&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;311-신경망의-예&quot;&gt;3.1.1 신경망의 예&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sean-parkk.github.io/assets/images/DLscratch/3/Untitled.png&quot; width=&quot;300&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;신경망은 &lt;strong&gt;입력층 (0층), 은닉층 (1층), 출력층 (2층)&lt;/strong&gt;으로 구성됨&lt;/li&gt;
  &lt;li&gt;여기서 은닉층의 뉴런은 (입력층과 출력층과 달리) 사람 눈에 보이지 않음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;312-퍼셉트론-복습&quot;&gt;3.1.2 퍼셉트론 복습&lt;/h4&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;일반적인 퍼셉트론&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;편향을 명시한 퍼셉트론&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://sean-parkk.github.io/assets/images/DLscratch/3/Untitled%201.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://sean-parkk.github.io/assets/images/DLscratch/3/Untitled%202.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[y = \left\{
    \begin{array}\\
        0 &amp;amp; (b + w_1x_1 + w_2x_2 &amp;lt;= 0) \\
        1 &amp;amp; (b + w_1x_1 + w_2x_2 &amp;gt; 0) \\
    \end{array}
\right.\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$b$는 &lt;strong&gt;편향&lt;/strong&gt;을 뜻하며, 뉴런이 얼마나 쉽게 활성화 되는지를 제어&lt;/li&gt;
  &lt;li&gt;$w_1, w_2$는 각 신호의 &lt;strong&gt;가중치&lt;/strong&gt;를 뜻하며, 각 신호의 영향력을 제어&lt;/li&gt;
  &lt;li&gt;&amp;lt;편향을 명시한 퍼셉트론&amp;gt; 그림에서는 $x_1, x_2, 1$에 각 신호의 가중치를 곱한 후, 다음 뉴런에 전달&lt;/li&gt;
  &lt;li&gt;다음 뉴런에서는 이 신호들의 값을 더해서 활성화 될지 여부를 결정
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;이때, 조건 분기의 동작 (0 이상이면 1 출력, 그렇지 않으면 0 출력)을 다음과 같이 $h(x)$로 정의&lt;/li&gt;
  &lt;li&gt;입력 신호의 총합이 $h(x)$ 함수를 거쳐서 변환되어, 그 값이 y의 출력됨&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[y = h(b + w_1x_1 + w_2x_2)\]

\[h(x) = \left\{
    \begin{array}\\
        0 &amp;amp; (x &amp;lt;= 0) \\
        1 &amp;amp; (x &amp;gt; 0) \\
    \end{array}
\right.\]

&lt;h4 id=&quot;313-활성화-함수의-등장&quot;&gt;3.1.3 활성화 함수의 등장&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;위의 $h(x)$ 함수처럼 입력 신호의 총합을 출력 신호로 변환하는 함수를 &lt;strong&gt;활성화 (Activation) 함수&lt;/strong&gt; 라고 정의&lt;/li&gt;
  &lt;li&gt;위에서 정의한 식을 조금 풀어서 적으면 다음과 같이 표현 가능&lt;/li&gt;
  &lt;li&gt;이 식에서 $a$는 가중치가 달린 입력 신호와 편향의 총합을 계산 한 값&lt;/li&gt;
  &lt;li&gt;즉, 가중치 신호를 조합한 결과가 a라는 노드가 되고, $h(x)$를 통과하여 y라는 노드로 변환&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[a = b + w_1x_1 + w_2x_2\]

\[y = h(a)\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/post-images%2Fdscwinterstudy%2Fd1593000-38e9-11ea-b942-cb9b82d31200%2F%ED%99%9C%EC%84%B1%ED%99%94-%ED%95%A8%EC%88%98%EC%9D%98-%EC%B2%98%EB%A6%AC-%EA%B3%BC%EC%A0%95.PNG&quot; width=&quot;300&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-활성화-함수&quot;&gt;3.2 활성화 함수&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;위에서 이야기 했던 $h(x)$ 함수는 임계값을 경계로 출력이 바뀌는데, 이를 &lt;strong&gt;계단 (Step) 함수&lt;/strong&gt;라고 정의&lt;/li&gt;
  &lt;li&gt;따라서 퍼셉트론에서는 활성화 함수로 계단 함수를 이용&lt;/li&gt;
  &lt;li&gt;물론 신경망에서는 계단 함수 외에 다른 함수를 사용하고 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;321-시그모이드-sigmoid-함수&quot;&gt;3.2.1 시그모이드 (Sigmoid) 함수&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[h(x) = \frac{1}{1 + exp(-x)} = \frac{1}{1 + e^{-x}}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;신경망에서는 활성화 함수로 시그모이드 함수를 이용하여 신호를 변환하고, 변환된 신호를 다음 뉴런에 전달&lt;/li&gt;
  &lt;li&gt;퍼셉트론과 신경망의 주된 차이는 활성화 함수라고 할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;322-계단-함수-구현하기&quot;&gt;3.2.2 계단 함수 구현하기&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;계단 함수는 입력이 0을 넘으면 1을 출력하고, 그 외에는 0을 출력하는 함수&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;warnings&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;warnings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;filterwarnings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;action&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ignore'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;step_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;array_step_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;step_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 1
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;step_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# 0
&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array_step_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# [1 1]
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array_step_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [1 0]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;323-계단-함수의-그래프&quot;&gt;3.2.3 계단 함수의 그래프&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;아래 그림에서 볼 수 있듯, 계단 함수는 0을 경계로 출력이 0에서 1로 또는 1에서 0으로 바뀜&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib.pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;array_step_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Step Function graph&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221006_1.png&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;324-시그모이드-함수-구현하기&quot;&gt;3.2.4 시그모이드 함수 구현하기&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# array([0.26894142, 0.73105858, 0.88079708])
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 브로드캐스트 적용
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [2 3 4]
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [1.  0.5  0.33333333]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Sigmoid Function graph&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221006_2.png&quot; /&gt;
&lt;/p&gt;

&lt;h4 id=&quot;325-시그모이드-함수와-계단-함수-비교&quot;&gt;3.2.5 시그모이드 함수와 계단 함수 비교&lt;/h4&gt;

&lt;ol&gt;
  &lt;li&gt;[차이점] &lt;strong&gt;매끄러움&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;이 매끄러움이 신경망 학습에서 매우 중요한 역할을 하게 될 것&lt;/li&gt;
      &lt;li&gt;시그모이드 함수는 부드러운 곡선이며, 입력에 따라서 출력이 연속적으로 변화&lt;/li&gt;
      &lt;li&gt;한편, 계단 함수는 0을 경계로 출력이 갑자기 바뀜&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[차이점] &lt;strong&gt;출력값의 범위&lt;/strong&gt;가 다름
    &lt;ul&gt;
      &lt;li&gt;계단 함수는 0과 1 중 하나의 값만 출력&lt;/li&gt;
      &lt;li&gt;한편, 시그모이드 함수는 실수도 출력 가능&lt;/li&gt;
      &lt;li&gt;즉, 퍼셉트론에서는 뉴런 사이에 0이나 1만 흘렀지만, 신경망에서는 연속적인 실수가 흐름&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[공통점] 큰 관점에서 보면 &lt;strong&gt;같은 모양&lt;/strong&gt;을 보임
    &lt;ul&gt;
      &lt;li&gt;입력이 작을 때는 0에 가깝고, 입력이 클 때는 1에 가까워짐&lt;/li&gt;
      &lt;li&gt;즉, 두 함수 모두 입력이 중요하면 큰 값을 출력하고, 입력이 중요하지 않으면 작은 값을 출력&lt;/li&gt;
      &lt;li&gt;또한 입력이 아무리 작거나, 커도 출력은 0과 1 사이의 값을 가짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;[공통점] &lt;strong&gt;비선형 함수&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;시그모이드 함수는 곡선, 계단 함수는 구부러진 직선 형태의 비선형 함수 꼴을 보임&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;326-비선형-함수&quot;&gt;3.2.6 비선형 함수&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$f(x) = ax + b$처럼 직선 형태를 띄는 것을 &lt;strong&gt;선형 함수&lt;/strong&gt;라고 정의&lt;/li&gt;
  &lt;li&gt;But, 위에서 봤던 시그모이드 함수나 계단 함수처럼 직선 1개로 그릴 수 없는 함수를 &lt;strong&gt;비선형 함수&lt;/strong&gt;라고 정의&lt;/li&gt;
  &lt;li&gt;신경망의 활성화 함수를 선형 함수로 하는 경우에, 깊은 층이 의미가 없어지므로 비선형 함수를 사용해야 함
    &lt;ul&gt;
      &lt;li&gt;은닉층이 없는 네트워크&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;327-relu-rectified-linear-unit-함수&quot;&gt;3.2.7 ReLU (Rectified Linear Unit) 함수&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;ReLU 함수는 입력이 0을 넘으면 그 입력을 그대로 출력하고, 0 이하면 0을 출력하는 함수&lt;/li&gt;
  &lt;li&gt;이번 장에서는 시그모이드 함수를 사용하지만, 후반부에서는 ReLU 함수를 활성화 함수로 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[h(x) = \left\{
    \begin{array}\\
        x &amp;amp; (x &amp;gt; 0) \\
        0 &amp;amp; (x &amp;lt;= 0) \\
    \end{array}
\right.\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;maximum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 두 입력 중 더 큰 값을 선택해서 반환
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;arange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;5.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ReLU Function graph&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221006_3.png&quot; /&gt;
&lt;/p&gt;

&lt;h3 id=&quot;33-다차원-배열의-계산&quot;&gt;3.3 다차원 배열의 계산&lt;/h3&gt;

&lt;h4 id=&quot;331-다차원-배열&quot;&gt;3.3.1 다차원 배열&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;다차원 배열도 기본은 &lt;strong&gt;숫자의 집합&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;숫자가 한 줄로, 직사각형으로, N차원으로 나열된 것 등을 모두 &lt;strong&gt;다차원 배열&lt;/strong&gt;이라고 정의&lt;/li&gt;
  &lt;li&gt;2차원 배열은 &lt;strong&gt;행렬 (Matrix)&lt;/strong&gt;이라고 부르고, 가로 방향을 행, 세로 방향을 열이라고 정의&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# 1 (4,)
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ndim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# 2 (3, 2)
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;332-행렬의-곱&quot;&gt;3.3.2 행렬의 곱&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s1.md5.ltd/image/076c83bc7deb215d880f4d90dad15b13.png&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;행렬의 곱은 numpy에서 &lt;strong&gt;np.dot()&lt;/strong&gt; 함수를 통해서 확인 가능&lt;/li&gt;
  &lt;li&gt;주의해야 할 것은 np.dot(A, B)와 np.dot(B, A)는 달라질 수도 있음&lt;/li&gt;
  &lt;li&gt;행렬의 곱을 수행하기 위해서는 &lt;strong&gt;행렬 A의 열 수와 행렬 B의 행 수가 같아야 함&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [[19 22] [43 50]] 
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [[23 34] [31 46]]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (3, 2) (2,) 
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# [23 53 83]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;333-신경망에서-행렬의-곱&quot;&gt;3.3.3 신경망에서 행렬의 곱&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxODA5MjdfNjMg/MDAxNTM4MDE0NDkwMjAw.mMfOmVBBpA8Xvb1mlM8XWFlezAzFdF4B8HV0vkp_S24g.YbV20K7bYmvoIP8H_8X1blDyWeOlfGUxTF0dnE9ISyMg.PNG.cheeryun/fig_3-14.png?type=w800&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (2,) (2, 3) 
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# [ 5 11 17]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;34-3층-신경망-구현하기&quot;&gt;3.4 3층 신경망 구현하기&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxODA2MTBfMjQ3/MDAxNTI4NjEzNzA1OTEx.8LP6DXgs8QCQTeni1VRi9BueT5Uv_DHKpRYBPqi2tC4g.TQmRFx4Qp_1j5kqfyxxLFB1zBo7yIeMTskaQjwX73Pkg.PNG.ssdyka/fig_3-15.png?type=w2&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;입력층 (0층)은 2개, 첫 번째 은닉층 (1층)은 3개, 두 번째 은닉층 (2층)은 2개, 출력층 (3층)은 2개의 뉴런&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;341-표기법-설명&quot;&gt;3.4.1 표기법 설명&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxODA2MTBfNzIg/MDAxNTI4NjEzNzA2MzM0.j9QOoSA1XXHdyvrUWy0ZP_f_nWneMFSd1xIFpSRDxHsg.XXxHvb4p00Wu9Kzn_n-nB_lKfE2iRFVax14IwQhUejcg.PNG.ssdyka/fig_3-16.png?type=w2&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;342-각-층의-신호-전달-구현하기&quot;&gt;3.4.2 각 층의 신호 전달 구현하기&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxODA2MTBfMjEx/MDAxNTI4NjEzNzA2NzUw.4uqh_gqJn64Ensn63s0fY1jpbspD5oilVYSu-ejMgs8g.Di4xipmNSMo-duyfkJNJAzfkqqS72dWbGSGxzhqcHjQg.PNG.ssdyka/fig_3-17.png?type=w2&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[a_1^{(1)} = w_{11}^{(1)}x_1 + w_{12}^{(1)}x_2 + b_1^{(1)}\]

\[A^{(1)} = XW^{(1)} + B^{(1)}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;위 그림에서는 편향 $b$을 뜻하는 뉴런 1이 추가&lt;/li&gt;
  &lt;li&gt;편향은 오른쪽 아래 인덱스가 하나 밖에 없음 (앞 층의 편향 뉴런이 하나 뿐이기 때문)&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (2,) (2, 3) (3,) 
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B1&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A1 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;                &lt;span class=&quot;c1&quot;&gt;# A1 = [0.3 0.7 1.1]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxODA2MTBfMTUy/MDAxNTI4NjEzNzA3MTgy.iInlQtedOrR3cr83cHlHh5iBC97Rd3CAn8lsfil2pJwg.VY7Bft0dgHkae7sJ4UrgO9CchUygWMF7DhqdSlMvzm0g.PNG.ssdyka/fig_3-18.png?type=w2&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;은닉층에서 가중치 합을 a로 표기하고, 활성화 함수 $h()$로 변환된 신호를 $z$로 표기&lt;/li&gt;
  &lt;li&gt;여기서 활성화 함수는 시그모이드 함수를 활용&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;Z1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A1 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A1 = [0.3 0.7 1.1]
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Z1 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Z1 = [0.57444252 0.66818777 0.75026011]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxODA2MTBfMjg4/MDAxNTI4NjEzNzA3NTY4.cwhPzaPClfzWeATMD4HaoIlB8fISmeCpZ0F0dOMNYgwg.VCGI5jJ2ZPizDF-7smAQrHaU4-lt5D7edc3wvJXctBUg.PNG.ssdyka/fig_3-19.png?type=w2&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이번에는 1층의 출력값이었던 Z1이 입력이 된다는 점 빼고는 모두 동일&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (3,) (3, 2) (2,) 
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B2&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Z2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A2 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A2 = [0.51615984 1.21402696]
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Z2 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Z2 = [0.62624937 0.7710107 ]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://mblogthumb-phinf.pstatic.net/MjAxODA2MTBfMTgg/MDAxNTI4NjEzNzA3ODk1.OB5KMOlT1nVHC2soDuhlijxo7UTW6zxjoZ3MH1IiIVwg.r8GLKspRWCKdsp3xfel5Y34yJuK5K6AdmOQcBzX_iKgg.PNG.ssdyka/fig_3-20.png?type=w2&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;출력층의 활성화 함수는 입력을 그대로 출력해주는 항등 함수로 정의&lt;/li&gt;
  &lt;li&gt;출력층의 활성화 함수를 $\sigma()$로 표시하여 은닉층의 활성화 함수인 $h()$와 다름을 명시&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;identity_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (2,) (2, 2) (2,) 
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Z2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B3&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;identity_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;A3 = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A3&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A3 = [0.31682708 0.69627909]
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Y = &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Y&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# Y = [0.31682708 0.69627909]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;343-구현-정리&quot;&gt;3.4.3 구현 정리&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;신경망 구현 관례에 따라 가중치만 W1 같이 대문자로 표현, 그 외 편향과 중간 결과는 소문자로 표현&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;# 가중치와 편향을 초기화하고, dic에 할당
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;init_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;    
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 입력 신호를 출력으로 변환하는 처리 과정 구현
# 신호가 순방향 (입력 -&amp;gt; 출력)으로 전달되므로 forward (순전파)로 정의
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;identity_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;init_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [0.31682708 0.69627909]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;35-출력층-설계하기&quot;&gt;3.5 출력층 설계하기&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;신경망은 분류와 회귀 모두 이용 가능
    &lt;ul&gt;
      &lt;li&gt;분류: 데이터가 어느 class에 속하는지를 찾는 유형&lt;/li&gt;
      &lt;li&gt;회귀: 데이터에서 연속적인 수치를 예측하는 유형&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;일반적으로 회귀에는 항등 함수, 분류에는 시그모이드 및 소프트맥스 함수를 활성화 함수로 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;351-항등-함수와-소프트맥스-함수-구현하기&quot;&gt;3.5.1 항등 함수와 소프트맥스 함수 구현하기&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;항등 (identity) 함수&lt;/strong&gt;는 입력을 그대로 출력하므로, 출력층에서 이를 사용하면 입력 신호가 그대로 출력&lt;/li&gt;
  &lt;li&gt;분류에서 사용하는 &lt;strong&gt;소프트맥스 (softmax) 함수&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;n은 출력층의 뉴런 수&lt;/li&gt;
      &lt;li&gt;$y_k$는 그 중 $k$번째 출력을 뜻함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;소프트맥스 함수의 출력은 모든 입력 신호로부터 화살표를 받고 있음&lt;/li&gt;
  &lt;li&gt;그 이유는 출력층의 각 뉴런이 모든 입력 신호에서 영향을 받았기 때문&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

\[y_k = \frac{exp(a_k)}{\sum_{i=1}^{n} exp(a_i)}\]

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;항등 함수&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;소프트맥스 함수&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://sean-parkk.github.io/assets/images/DLscratch/3/Untitled%208.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https://sean-parkk.github.io/assets/images/DLscratch/3/Untitled%209.png&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;exp_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sum_exp_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum_exp_a&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [0.01821127 0.24519181 0.73659691]
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;exp_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sum_exp_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;exp_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sum_exp_a&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;352-소프트맥스-함수-구현-시-주의점&quot;&gt;3.5.2 소프트맥스 함수 구현 시 주의점&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;위 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;softmax&lt;/code&gt; 함수는 컴퓨터로 계산할 때, &lt;strong&gt;오버플로&lt;/strong&gt;라는 결함이 존재&lt;/li&gt;
  &lt;li&gt;지수 함수를 사용할 때, 너무 큰 값을 쉽게 내뱉는다는 한계&lt;/li&gt;
  &lt;li&gt;따라서 소프트맥스 함수를 개선&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/images%2Fu_jinju%2Fpost%2Fcf944335-0e81-4e9d-9335-a7e0541d2ffc%2Fimage.png&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;C라는 임의의 정수를 분자와 분모 양쪽에 곱하기&lt;/li&gt;
  &lt;li&gt;C를 지수 함수 exp() 안으로 옮겨 logC로 만들기&lt;/li&gt;
  &lt;li&gt;logC를 C’이라는 새로운 기호로 바꾸기&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;이를 통해 알 수 있는 것
    &lt;ul&gt;
      &lt;li&gt;소프트맥스의 지수 함수를 계산할 때, 어떤 정수를 더하거나 빼도 결과는 바뀌지 않음&lt;/li&gt;
      &lt;li&gt;C’에 어떤 값을 대입해도 상관 없지만, 오버플로를 막을 목적으로는 입력 신호 중 최댓값을 이용&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1010&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;990&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# 소프트맥스 함수의 계산
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;#  array([nan, nan, nan])
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# nan만 나왔던 위 결과와 달리 정상 값이 나옴
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# [  0 -10 -20]
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# array([9.99954600e-01, 4.53978686e-05, 2.06106005e-09])
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;353-소프트맥스-함수의-특징&quot;&gt;3.5.3 소프트맥스 함수의 특징&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;소프트맥스 함수의 출력은 0에서 1 사이의 실수&lt;/li&gt;
  &lt;li&gt;소프트맥스 함수 출력의 총합은 1&lt;/li&gt;
  &lt;li&gt;이러한 성질들을 통해 함수의 출력 값을 &lt;strong&gt;확률&lt;/strong&gt;로 해석 가능&lt;/li&gt;
  &lt;li&gt;소프트맥스 함수를 적용해도 각 원소의 대소 관계는 변하지 않음&lt;/li&gt;
  &lt;li&gt;신경망을 이용한 분류에서는 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식&lt;/li&gt;
  &lt;li&gt;기계학습 문제 풀이는 &lt;strong&gt;학습과 추론&lt;/strong&gt;의 단계로 이뤄짐
    &lt;ul&gt;
      &lt;li&gt;학습 (Train): 데이터를 통해서 모델이 학습하는 단계&lt;/li&gt;
      &lt;li&gt;추론 (Inference): 학습한 모델로 미지의 데이터에 대해 추론 (분류)을 수행&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;2.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;4.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# y[0], y[1], y[2]의 확률값
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;# [0.01821127 0.24519181 0.73659691] 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# 1.0
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;354-출력층의-뉴런-수-정하기&quot;&gt;3.5.4 출력층의 뉴런 수 정하기&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;출력층의 뉴런 수는 풀려는 문제에 맞게 적절히 정해야 함&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;분류&lt;/strong&gt;에서는 분류하고 싶은 클래스 수로 설정하는 것이 일반적
    &lt;ul&gt;
      &lt;li&gt;0부터 9 중 하나로 분류하는 문제에서의 출력층 뉴런은 10개로 설정&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;36-손글씨-숫자-mnist-인식&quot;&gt;3.6 손글씨 숫자 (MNIST) 인식&lt;/h3&gt;

&lt;h4 id=&quot;361-mnist-데이터셋&quot;&gt;3.6.1 MNIST 데이터셋&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;MNIST라는 데이터셋은 손글씨 숫자 이미지 집합
    &lt;ul&gt;
      &lt;li&gt;0부터 9까지의 숫자 이미지로 구성&lt;/li&gt;
      &lt;li&gt;훈련 이미지는 60,000장, 시험 이미지는 10,000장&lt;/li&gt;
      &lt;li&gt;MNIST 이미지 데이터는 28 * 28 크기의 회색조 이미지 (1채널)&lt;/li&gt;
      &lt;li&gt;각 픽셀은 0에서 255까지의 값을 취함&lt;/li&gt;
      &lt;li&gt;각 이미지에는 그 이미지가 실제 의미하는 숫자가 레이블로 붙어 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'/Users/paul/Desktop/github/deep-learning-from-scratch-master/'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset.mnist&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_mnist&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
    &lt;span class=&quot;nf&quot;&gt;load_mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (60000, 784)
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# (60000,)
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# (10000, 784)
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   &lt;span class=&quot;c1&quot;&gt;# (10000,)
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;img_show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pil_img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fromarray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;uint8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pil_img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;               &lt;span class=&quot;c1&quot;&gt;# 5
&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;# (784,)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# 형상을 원래 이미지의 크기로 변형
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;           &lt;span class=&quot;c1&quot;&gt;# (28, 28)
&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;img_show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;362-신경망의-추론-처리&quot;&gt;3.6.2 신경망의 추론 처리&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;MNIST 데이터셋은 입력층 뉴런을 784개, 출력층 뉴런을 10개로 구성
    &lt;ul&gt;
      &lt;li&gt;입력층 뉴런이 784개인 이유는 이미지 크기가 28 * 28 = 784&lt;/li&gt;
      &lt;li&gt;출력층 뉴런이 10개인 이유는 0부터 9까지 숫자를 구분하기 때문&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;은닉층은 총 두 개로, 첫 번째 은닉층에는 50개의 뉴런, 두 번째 은닉층에는 100개의 뉴런 배치 (임의로 설정)&lt;/li&gt;
  &lt;li&gt;입력 이미지 데이터에 대한 전처리 작업으로 정규화를 수행
    &lt;ul&gt;
      &lt;li&gt;정규화 (Normalization): 데이터를 특정 범위로 변환하는 처리&lt;/li&gt;
      &lt;li&gt;전처리 (Pre-processing): 신경망의 입력 데이터에 특정 변환을 가하는 것&lt;/li&gt;
      &lt;li&gt;백색화 (Whitening): 전체 데이터를 균일하게 분포시키는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; \
        &lt;span class=&quot;nf&quot;&gt;load_mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;one_hot_label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t_test&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;init_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# 가중치와 편향 매개변수가 dictionary 변수로 저장되어 있는 pickle 파일
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;github_url&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;ch03/sample_weight.pkl&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'rb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pickle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'W3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'b3'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b3&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;init_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; 
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# dict_keys(['b2', 'W1', 'b1', 'W2', 'W3', 'b3'])
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;accuracy_cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 확률이 가장 높은 원소의 인덱스 (clas)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;accuracy_cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 가장 마지막 데이터의 예측 확률값과 class
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# [4.2882856e-04 2.0043008e-06 2.5405665e-03 2.0168895e-06 5.5917690e-04
#  3.1262048e-04 9.9614757e-01 4.3499364e-07 6.3756829e-06 3.7751408e-07] 6 
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy:&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# Accuracy:0.9352
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;363-배치-처리&quot;&gt;3.6.3 배치 처리&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;아래의 결과에서 확인할 수 있듯, 다차원 배열의 대응하는 차원의 원소 수가 일치&lt;/li&gt;
  &lt;li&gt;가장 마지막 최종 결과는 원소가 10개인 1차원 배열 y가 출력&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sean-parkk.github.io/assets/images/DLscratch/3/Untitled%2010.png&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이미지 100장을 묶어서 나온 결과를 보면 다음과 같음&lt;/li&gt;
  &lt;li&gt;하나로 묶은 입력 데이터를 &lt;strong&gt;배치 (batch)&lt;/strong&gt;라고 정의&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://sean-parkk.github.io/assets/images/DLscratch/3/Untitled%2011.png&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;c1&quot;&gt;### 개별로 predict 한 결과가 아닌, 배치 단위로 predict하여 결과 확인
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;init_network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 배치 크기
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# x[0:100], x[100:200], ...과 같은 형태
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;network&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;argmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;accuracy_cnt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 가장 마지막 데이터의 예측 확률값과 class의 shape
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# (100, 10) (100,)
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Accuracy:&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_cnt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Accuracy:0.9352
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;37-정리&quot;&gt;3.7 정리&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;신경망에서는 활성화 함수로 시그모이드, ReLU 함수 같은 비선형 함수를 활용&lt;/li&gt;
  &lt;li&gt;numpy의 다차원 배열을 잘 사용하면, 신경망을 효율적으로 구현할 수 있음&lt;/li&gt;
  &lt;li&gt;출력층의 활성화 함수로 회귀에서는 항등 함수, 분류에서는 소프트맥스 함수를 사용&lt;/li&gt;
  &lt;li&gt;분류에서는 출력층의 뉴런 수를 분류하려는 클래스 수와 같게 설정&lt;/li&gt;
  &lt;li&gt;입력 데이터를 묶은 것을 배치라고 하고, 추론 처리를 이 배치 단위로 진행하면 결과를 빨리 얻을 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;추가-학습---tensorflow를-활용한-mnist-실습&quot;&gt;추가 학습 - tensorflow를 활용한 MNIST 실습&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 1. 데이터 불러오기
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mnist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;load_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 2. 간단한 데이터 전처리
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;255.0&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 3. 모델 구성
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;models&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 4. 모델 컴파일
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'adam'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sparse_categorical_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
              &lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 5. 모델 훈련
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 6 훈련 과정 시각화
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;history&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Model accuracy &amp;amp; loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Epoch'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Value'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Accuracy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Loss'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'upper left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# 7. 정확도 평가
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;evaluate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'테스트 정확도:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;Epoch 1/10
1875/1875 [==========================] - 4s 2ms/step - loss: 0.2236 - accuracy: 0.9347
Epoch 2/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0909 - accuracy: 0.9724
Epoch 3/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0602 - accuracy: 0.9815
Epoch 4/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0435 - accuracy: 0.9867
Epoch 5/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0316 - accuracy: 0.9900
Epoch 6/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0241 - accuracy: 0.9922
Epoch 7/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0195 - accuracy: 0.9936
Epoch 8/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0151 - accuracy: 0.9952
Epoch 9/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0131 - accuracy: 0.9956
Epoch 10/10
1875/1875 [==========================] - 3s 2ms/step - loss: 0.0112 - accuracy: 0.9964
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221006_4.png&quot; /&gt;
&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;313/313 [==========================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9795
테스트 정확도: 0.9794999957084656
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 밑바닥부터 시작하는 딥러닝1 책 리뷰 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/Deep-learning-from-scratch/blob/main/%EB%B0%91%EC%8B%9C%EB%94%A51-3%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D.ipynb&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Thu, 06 Oct 2022 22:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-3%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-3%EA%B0%95-%EC%8B%A0%EA%B2%BD%EB%A7%9D/</guid>
        
        <category>AI</category>
        
        <category>밑시딥1</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Review - IT Book</category>
        
        <category>밑바닥부터 시작하는 딥러닝1</category>
        
      </item>
    
      <item>
        <title>밑시딥1 2강. 퍼셉트론</title>
        <description>&lt;p&gt;이번 글에서는 본격적으로 밑바닥부터 시작하는 딥러닝1 책에 대한 리뷰를 시작합니다. &lt;br /&gt;
딥러닝의 가장 기초 개념이라고 할 수 있는 퍼셉트론이 무엇인지에 대해 학습합니다. &lt;br /&gt;
또한 AND, NAND, OR 게이트 등을 통해서 퍼셉트론의 구조와 동작 원리도 함께 배울 수 있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Chapter&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Title&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Main Topics&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;헬로 파이썬&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;파이썬 기초 문법 소개, numpy, matplotlib&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;2강&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;퍼셉트론&lt;/span&gt;&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;span style=&quot;color:red&quot;&gt;AND, NAND, OR 게이트&lt;/span&gt;&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;신경망&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;활성화 함수, 다차원 배열 계산, 출력층 설계, MNIST&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;신경망 학습&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;손실 함수, 경사 하강법&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;오차역전파법&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;역전파, 활성화 함수 구현&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;학습 관련 기술들&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;매개변수 갱신, 배치 정규화, 하이퍼파라미터 값 찾기&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;합성곱 신경망 (CNN)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;합성곱 계층, 풀링 계층, CNN 구현&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8강&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;딥러닝 (Deep learning)&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;초기 역사, 딥러닝 활용&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Appendix&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Softmax with loss 계층의 계산 그래프&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;-&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.yes24.com/Product/Goods/34970929&quot;&gt;밑바닥부터 시작하는 딥러닝 1&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/WegraLee/deep-learning-from-scratch&quot;&gt;밑바닥부터 시작하는 딥러닝 1 Github 링크&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;chapter-2-퍼셉트론&quot;&gt;&lt;font color=&quot;orange&quot;&gt;Chapter 2. 퍼셉트론&lt;/font&gt;&lt;/h2&gt;

&lt;h3 id=&quot;21-퍼셉트론이란&quot;&gt;2.1 퍼셉트론이란?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;신경망 (딥러닝)의 기원이 되는 알고리즘인 &lt;strong&gt;퍼셉트론&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;퍼셉트론의 구조를 배우는 것은 신경망 및 딥러닝을 배우는데 기초가 될 것&lt;/li&gt;
  &lt;li&gt;퍼셉트론은 다수의 신호 (흐름이 있는 것)를 입력으로 받아서 하나의 신호를 출력하는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://blog.kakaocdn.net/dn/GrSdp/btqyt6tY8Dl/kwVynuK95LJEKufPg93fE0/img.png&quot; width=&quot;300&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;이 그림은 입력으로 2개의 신호를 받은 퍼셉트론의 예&lt;/li&gt;
  &lt;li&gt;그림에서 원은 &lt;strong&gt;뉴런&lt;/strong&gt; 혹은 &lt;strong&gt;노드&lt;/strong&gt;라고 부름&lt;/li&gt;
  &lt;li&gt;입력 신호가 뉴런에 보내질 때는 각각 고유한 가중치가 곱해짐
    &lt;ul&gt;
      &lt;li&gt;$x_1, x_2$는 입력 신호, $y$는 출력 신호, $w_1, w_2$는 가중치&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;뉴런에서 보내온 신호의 총합이 정해진 한계를 넘어설 때만 1을 출력 (뉴런 활성화)
    &lt;ul&gt;
      &lt;li&gt;한계값을 &lt;strong&gt;임계값&lt;/strong&gt;이라고 하고, $\theta$로 표현&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;퍼셉트론은 복수의 입력 신호 각각에 고유한 가중치 부여&lt;/li&gt;
  &lt;li&gt;가중치는 각 신호가 결과에 주는 영향력을 조절하는 요소로 작용 (신호가 클수록 그만큼 중요하다는 것)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://blog.kakaocdn.net/dn/b2gwyi/btqysNhyAXw/1rmfWOZukyoM3g2qkdFCp1/img.png&quot; width=&quot;300&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;22-단순한-논리-회로&quot;&gt;2.2 단순한 논리 회로&lt;/h3&gt;

&lt;h4 id=&quot;221-and-게이트&quot;&gt;2.2.1 AND 게이트&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;AND 게이트는 입력이 둘이고, 출력은 하나&lt;/li&gt;
  &lt;li&gt;AND 게이트는 두 입력이 모두 1일 때만, 1을 출력하고 그 외에는 0을 출력&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;222-nand-게이트와-or-게이트&quot;&gt;2.2.2 NAND 게이트와 OR 게이트&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;NAND 게이트는 Not AND를 의미하고, AND 게이트의 출력을 뒤집은 것&lt;/li&gt;
  &lt;li&gt;OR 게이트는 입력 신호 중 하나 이상이 1이면 출력이 1이 되는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/20563D534DFE0F9F18&quot; width=&quot;400&quot; height=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;23-퍼셉트론-구현하기&quot;&gt;2.3 퍼셉트론 구현하기&lt;/h3&gt;

&lt;h4 id=&quot;231-간단한-구현부터&quot;&gt;2.3.1 간단한 구현부터&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;AND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -&amp;gt; &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;(0, 0) -&amp;gt; 0
(1, 0) -&amp;gt; 0
(0, 1) -&amp;gt; 0
(1, 1) -&amp;gt; 1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;232-가중치와-편향-도입&quot;&gt;2.3.2 가중치와 편향 도입&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;기존에 퍼셉트론 동작 원리의 식에서 $\theta$를 $-b$로 치환하면 다음의 식 확인 가능&lt;/li&gt;
  &lt;li&gt;여기에서 $b$를 &lt;strong&gt;편향 (bias)&lt;/strong&gt;이라 하고, $w_1, w_2$는 그대로 가중치&lt;/li&gt;
  &lt;li&gt;퍼셉트론은 입력 신호에 가중치를 곱한 값과 편향을 더해서, 출력값을 결정&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99B057465A65F39704&quot; width=&quot;300&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;233-가중치와-편향-구하기&quot;&gt;2.3.3 가중치와 편향 구하기&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$w_1, w_2$ (가중치)는 각 입력 신호가 결과에 주는 영향력 (중요도)을 조절하는 매개변수&lt;/li&gt;
  &lt;li&gt;$b$ (편향)는 뉴런이 얼마나 쉽게 활성화 (결과를 1로 출력) 하느냐를 조정하는 매개변수
    &lt;ul&gt;
      &lt;li&gt;$b$가 -0.1이면, 각 입력 신호에 가중치를 곱한 값들의 합이 0.1을 초과하면 활성화&lt;/li&gt;
      &lt;li&gt;$b$가 -20.0이면, 각 입력 신호에 가중치를 곱한 값들이 20을 넘어야 활성화&lt;/li&gt;
      &lt;li&gt;이처럼 편향의 값은 뉴런이 얼마나 쉽게 활성화 되는지 결정할 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;AND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -&amp;gt; &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;(0, 0) -&amp;gt; 0
(1, 0) -&amp;gt; 0
(0, 1) -&amp;gt; 0
(1, 1) -&amp;gt; 1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;NAND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.7&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NAND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -&amp;gt; &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;(0, 0) -&amp;gt; 1
(1, 0) -&amp;gt; 1
(0, 1) -&amp;gt; 1
(1, 1) -&amp;gt; 0
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;OR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tmp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;OR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -&amp;gt; &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;(0, 0) -&amp;gt; 0
(1, 0) -&amp;gt; 1
(0, 1) -&amp;gt; 1
(1, 1) -&amp;gt; 1
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;기존에 퍼셉트론 동작 원리의 식에서 $\theta$를 $-b$로 치환하면 다음의 식 확인 가능&lt;/li&gt;
  &lt;li&gt;여기에서 $b$를 &lt;strong&gt;편향 (bias)&lt;/strong&gt;이라 하고, $w_1, w_2$는 그대로 가중치&lt;/li&gt;
  &lt;li&gt;퍼셉트론은 입력 신호에 가중치를 곱한 값과 편향을 더해서, 출력값을 결정&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://t1.daumcdn.net/cfile/tistory/99B057465A65F39704&quot; width=&quot;300&quot; height=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;24-퍼셉트론의-한계&quot;&gt;2.4 퍼셉트론의 한계&lt;/h3&gt;

&lt;h4 id=&quot;241-도전-xor-게이트&quot;&gt;2.4.1 도전! XOR 게이트&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;XOR 게이트는 배타적 논리합이라는 논리 회로&lt;/li&gt;
  &lt;li&gt;$x_1, x_2$ 중에서 하나가 1일 때만 1을 출력&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;242-선형과-비선형&quot;&gt;2.4.2 선형과 비선형&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;선형식 (직선)으로 XOR 게이트를 표현하는 것은 사실상 불가&lt;/li&gt;
  &lt;li&gt;하지만 비선형 (곡선)으로는 다음과 같이 표현 가능&lt;/li&gt;
  &lt;li&gt;즉, 퍼셉트론은 직선 하나로 나눈 영역만 표현할 수 있다는 한계가 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/post-images%2Fdscwinterstudy%2F71488010-3a21-11ea-8734-d1dac55eae87%2F2-8%EA%B3%A1%EC%84%A0%EC%9D%B4%EB%9D%BC%EB%A9%B4-%EB%82%98%EB%88%8C-%EC%88%98-%EC%9E%88%EB%8B%A4..png&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;25-다층-퍼셉트론이-출동한다면&quot;&gt;2.5 다층 퍼셉트론이 출동한다면&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;단일 퍼셉트론으로는 XOR 게이트를 구현할 수 없었음&lt;/li&gt;
  &lt;li&gt;하지만 층을 쌓아서 올리는 &lt;strong&gt;다층 퍼셉트론 (Multi-layer perceptron)&lt;/strong&gt;으로는 구현 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;251-기존-게이트-조합하기&quot;&gt;2.5.1 기존 게이트 조합하기&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://cdn1.byjus.com/wp-content/uploads/2020/06/xor-equivalent-circuit.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;252-xor-게이트-구현하기&quot;&gt;2.5.2 XOR 게이트 구현하기&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;XOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;OR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;NAND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;AND&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;s2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;XOR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; -&amp;gt; &quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;(0, 0) -&amp;gt; 0
(1, 0) -&amp;gt; 1
(0, 1) -&amp;gt; 1
(1, 1) -&amp;gt; 0
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;XOR 게이트는 다음과 같은 다층 구조의 네트워크&lt;/li&gt;
  &lt;li&gt;이처럼 층이 여러 개인 퍼셉트론을 &lt;strong&gt;다층 퍼셉트론&lt;/strong&gt;이라고 함
    &lt;ul&gt;
      &lt;li&gt;0층의 두 뉴런이 입력 신호를 받아서 1층의 뉴런으로 신호를 보냄&lt;/li&gt;
      &lt;li&gt;1층의 뉴런이 2층의 뉴런으로 신호를 보내고, 2층의 뉴런은 y를 출력&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;단층 퍼셉트론으로는 표현하지 못한 것을 층을 하나 더 늘려서 구현&lt;/li&gt;
  &lt;li&gt;퍼셉트론은 층을 깊게 쌓아서 더 다양한 것들을 표현할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://velog.velcdn.com/post-images%2Fdscwinterstudy%2F754c7c20-3a21-11ea-8734-d1dac55eae87%2F2-13XOR%EC%9D%98-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0.png&quot; width=&quot;500&quot; height=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;26-nand에서-컴퓨터까지&quot;&gt;2.6 NAND에서 컴퓨터까지&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;NAND 게이트의 조합만으로 컴퓨터가 수행하는 일을 재현할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;27-정리&quot;&gt;2.7 정리&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;퍼셉트론은 입출력을 갖춘 알고리즘&lt;/li&gt;
  &lt;li&gt;퍼셉트론은 입력을 주면, 정해진 규칙에 따른 값을 출력&lt;/li&gt;
  &lt;li&gt;퍼셉트론은 가중치와 편향을 매개변수로 설정&lt;/li&gt;
  &lt;li&gt;퍼셉트론으로 AND, OR 같은 논리회로 표현 가능&lt;/li&gt;
  &lt;li&gt;XOR 게이트는 단층 퍼셉트론 (직선)은 불가능하지만, 다층 퍼셉트론 (곡선)으로 구현 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 밑바닥부터 시작하는 딥러닝1 책 리뷰 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/Deep-learning-from-scratch/blob/main/%EB%B0%91%EC%8B%9C%EB%94%A51-2%EA%B0%95-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0.ipynb&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Wed, 05 Oct 2022 10:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-2%EA%B0%95-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EB%B0%91%EC%8B%9C%EB%94%A51-2%EA%B0%95-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0/</guid>
        
        <category>AI</category>
        
        <category>밑시딥1</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        
        <category>Review - IT Book</category>
        
        <category>밑바닥부터 시작하는 딥러닝1</category>
        
      </item>
    
      <item>
        <title>프로그래머스 인공지능 데브코스 3주차 정리 및 후기</title>
        <description>&lt;p&gt;이번 글에서는 프로그래머스 인공지능 데브코스의 3주차 강의에 대한 정리입니다. &lt;br /&gt;
지금까지 기본적인 파이썬의 자료구조와 알고리즘 그리고 크롤링과 기초 수학 등을 학습했습니다. &lt;br /&gt;
이번 주에는 파이썬에서 데이터를 다루는데 필요한 기초 패키지인 Numpy와 Pandas를 학습합니다. &lt;br /&gt;
파이썬으로 데이터를 다뤄보신 분들이라면, 익히 들어보셨을 해당 패키지들의 기본적인 내용을 보겠습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-numpy-실습&quot;&gt;1. Numpy 실습&lt;/h2&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Numpy array shape
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# shape_of_A = (3, 3)
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy one, zero array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Numpy random array
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.56&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scale&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.67&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 평균 1.56, 표준편차 0.67
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 10부터 20까지 임의의 정수를 담은 배열
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy indexing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;answer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# arr의 (2, 2, 3, 3) 번째 요소 값 읽기
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy changing certain element
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# arr의 (1, 2, 2, 3, 4) 번째 요소 값을 0으로 변경
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy addition (+)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;arr_B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr_B&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A와 B 배열의 합
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy multiply (*)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;arr_B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;ones&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr_B&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A와 B 배열의 곱
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;지금까지의 연산은 shape이 같은 두 배열 사이에서 적용된 것들&lt;/li&gt;
  &lt;li&gt;즉, 두 배열 사이에 shape이 다르다면, 연산이 제대로 동작하지 않음&lt;/li&gt;
  &lt;li&gt;그런데 Numpy에서 같은 shape이 아니더라도 연산이 가능한 경우가 있음&lt;/li&gt;
  &lt;li&gt;Numpy에서 배열의 shape이 다르더라도 자동으로 맞춰 연산하는 것을 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;브로드캐스팅&lt;/code&gt;이라고 함&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Numpy dot
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;B&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;C&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A와 B의 행렬 곱 연산
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy 1d array slicing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;57&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# arr의 42번 인덱스부터 56번 인덱스까지 슬라이싱
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;35&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# arr의 35번 인덱스부터 49번 인덱스까지 요소 값을 1로 변경
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy 2d array slicing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# arr의 (3, 7)에서 (6, 9)까지 슬라이싱
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;# arr의 (2, 3)부터 (5, 7)까지 요소 값을 0으로 변경
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy 2d array가 주어질 때, (y1, x1)에서 (y2, x2)까지 요소에 2를 곱한 배열 반환
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Numpy 내적 연산
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;solution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;    

&lt;span class=&quot;c1&quot;&gt;### Numpy bool 인덱싱
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 10보다 크고, 20보다 작거나 같은 요소 추출
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy 관계 연산
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# A 안에 52 또는 1인 요소와 같은 위치에 True, 다른 곳은 False
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy 배열 만들기
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;np_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# np.array()를 이용하여 numpy.ndarray 타입의 배열 만들기
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy matmul() 함수
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dot_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;matmul_result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### Numpy 브로드캐스팅 스칼라
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 행렬 a에 모든 요소 별로 2를 곱함
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### Numpy any(), all()
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;result1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;any&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# arr에 52 또는 1이 있는지 확인
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;result2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 해당 범위의 요소가 모두 20 이상의 수를 가지고 있는지 확인
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;numpy의-npdot과-npmatmul의-차이&quot;&gt;Numpy의 np.dot()과 np.matmul()의 차이&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.dot.html&quot;&gt;numpy 패키지에서 dot 함수 documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.matmul.html&quot;&gt;numpy 패키지에서 matmul 함수 documentation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;dot과 matmul 함수는 2d array까지의 결과는 동일&lt;/li&gt;
  &lt;li&gt;But, 2차원보다 더 큰 nd array에서는 결과가 다르다는 것을 확인할 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;### 2d array dot, matmul
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;      &lt;span class=&quot;c1&quot;&gt;# [[16 18] [28 29]]
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;matmul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# [[16 18] [28 29]]
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# np.matmul()과 @ 연산은 동일
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;### 3d array dot, matmul
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;# shape: (2, 3, 3)
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;       &lt;span class=&quot;c1&quot;&gt;# shape: (2, 3, 2, 3)
&lt;/span&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;2-python으로-데이터-다루기---numpy&quot;&gt;2. Python으로 데이터 다루기 - Numpy&lt;/h2&gt;

&lt;h3 id=&quot;참고-git과-github-그리고-branch란-무엇인가&quot;&gt;[참고] Git과 Github 그리고 Branch란 무엇인가?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Git은 &lt;strong&gt;분산 버전관리 시스템 (Distributed version control system)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;원격 Repository와 로컬 Repository로 구분하여 협업하는데 사용&lt;/li&gt;
  &lt;li&gt;Git 저장소에서 파일의 상태를 주목하라
    &lt;ul&gt;
      &lt;li&gt;Local working directory (Unstaged area)&lt;/li&gt;
      &lt;li&gt;Local staging area&lt;/li&gt;
      &lt;li&gt;Local repository, Remote repository&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Branch: 코드의 흐름을 분산 - 가지치기 (Git-Flow)&lt;/li&gt;
  &lt;li&gt;Github: 가장 대표적인 원격 저장소&lt;/li&gt;
  &lt;li&gt;Pull Request를 만들어서 다른 사람들과 함께 협업하기&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;git init                             &lt;span class=&quot;c&quot;&gt;# 로컬 저장소 생성&lt;/span&gt;
git status                           &lt;span class=&quot;c&quot;&gt;# 저장소의 상태 확인&lt;/span&gt;
git add example.py                   &lt;span class=&quot;c&quot;&gt;# example.py를 unstaged에서 staged로 이동&lt;/span&gt;
git commit &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Add example.py&quot;&lt;/span&gt;       &lt;span class=&quot;c&quot;&gt;# commit message 작성&lt;/span&gt;
git log                              &lt;span class=&quot;c&quot;&gt;# commit 기록 확인 (author, commitor, date 등)&lt;/span&gt;

git branch &amp;lt;branch_name&amp;gt;             &lt;span class=&quot;c&quot;&gt;# 새로운 branch 생성&lt;/span&gt;
git checkout &amp;lt;branch_name&amp;gt;           &lt;span class=&quot;c&quot;&gt;# 현재 작업 중인 branch를 전환&lt;/span&gt;
git merge &amp;lt;branch_name&amp;gt;              &lt;span class=&quot;c&quot;&gt;# 현재 작업 중인 branch_name를 원하는 branch에 병합&lt;/span&gt;
git branch &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &amp;lt;branch_name&amp;gt;          &lt;span class=&quot;c&quot;&gt;# branch를 삭제&lt;/span&gt;

git remote add &amp;lt;별칭&amp;gt; &amp;lt;원격저장소 주소&amp;gt;   &lt;span class=&quot;c&quot;&gt;# 원격 저장소 설정&lt;/span&gt;
git push &amp;lt;원격_레포_이름&amp;gt; &amp;lt;branch_name&amp;gt; &lt;span class=&quot;c&quot;&gt;# 로컬 작업을 원격 레포지토리에 push&lt;/span&gt;
git clone &amp;lt;repo_uri&amp;gt;                 &lt;span class=&quot;c&quot;&gt;# 원격 저장소를 로컬에 저장하기&lt;/span&gt;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;numpy-연산-선형-대수&quot;&gt;Numpy 연산, 선형 대수&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Array의 Indexing, Slicing: python의 list와 유사&lt;/li&gt;
  &lt;li&gt;Array의 Boradcasting: (m * n &amp;amp; m * 1), (m * n &amp;amp; 1 * n), (m * 1 &amp;amp; 1 * n)&lt;/li&gt;
  &lt;li&gt;영벡터 (영행렬): np.zeros(dim)을 통해서 생성할 수 있음&lt;/li&gt;
  &lt;li&gt;일벡터 (일행렬): np.ones(dim)을 통해서 생성할 수 있음&lt;/li&gt;
  &lt;li&gt;대각행렬: np.diag(main_diagonal)을 통해서 생성할 수 있음&lt;/li&gt;
  &lt;li&gt;항등행렬: main diagonal이 1인 대각행렬. np.eye()를 통해서 생성할 수 있음&lt;/li&gt;
  &lt;li&gt;행렬곱 (dot product): 행렬 간의 곱 연산&lt;/li&gt;
  &lt;li&gt;트레이스: main diagonal의 합. np.trace()를 통해서 생성할 수 있음&lt;/li&gt;
  &lt;li&gt;행렬식 (determinant): 행렬을 대표하는 값들 중 하나. np.linalg.det()으로 계산 가능&lt;/li&gt;
  &lt;li&gt;역행렬: 행렬 A에 대해 AB = BA = 1를 만족하는 행렬 B = A^-1. np.linalg.inv()으로 계산 가능&lt;/li&gt;
  &lt;li&gt;고유값과 고유벡터 (eigenvalue, eigenvector): np.linalg.eig()으로 계산 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-python으로-데이터-다루기---pandas&quot;&gt;3. Python으로 데이터 다루기 - Pandas&lt;/h2&gt;

&lt;h3 id=&quot;pandas-시작하기&quot;&gt;Pandas 시작하기&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Table: 행과 열을 이용해서 데이터를 저장하고 관리하는 자료구조 (컨테이너)&lt;/li&gt;
  &lt;li&gt;Series: 1d labeled array, 인덱스를 지정해 줄 수 있음
    &lt;ul&gt;
      &lt;li&gt;Series는 &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;name&lt;/code&gt; 속성을 가지고 있음&lt;/li&gt;
      &lt;li&gt;처음 Series를 만들 때, 이름을 붙일 수 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DataFrame: 2d labeled table, 인덱스를 지정해 줄 수 있음
    &lt;ul&gt;
      &lt;li&gt;DataFrame의 각 column은 Series 형태임&lt;/li&gt;
      &lt;li&gt;head(): 처음 n개의 데이터 참조&lt;/li&gt;
      &lt;li&gt;tail(): 마지막 n개의 데이터 참조&lt;/li&gt;
      &lt;li&gt;loc[row, col]: 인덱스를 이용해서 가져오기&lt;/li&gt;
      &lt;li&gt;iloc[rowidx, colidx]: 숫자 인덱스를 이용해서 가져오기&lt;/li&gt;
      &lt;li&gt;split: 특정한 기준을 바탕으로 DataFrame 분할&lt;/li&gt;
      &lt;li&gt;apply: 통계함수 (sum, mean, median 등)를 적용해서 각 데이터 압축&lt;/li&gt;
      &lt;li&gt;combine: apply 된 결과를 바탕으로 새로운 Series 생성&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-python으로-시각화하기---matplotlib&quot;&gt;4. Python으로 시각화하기 - Matplotlib&lt;/h2&gt;

&lt;h3 id=&quot;matplotlib-시작하기&quot;&gt;Matplotlib 시작하기&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Matplotlib: 파이썬의 데이터 시각화 라이브러리
    &lt;ul&gt;
      &lt;li&gt;꺾은선 그래프, 산점도, 박스 그림, 막대 그래프, 히스토그램, 원형 그래프&lt;/li&gt;
      &lt;li&gt;박스 그림: Q1, Q2, Q3, min, max 정보를 담고 있음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Seaborn: Matplotlib을 기반으로 더 다양한 시각화 방법을 제공하는 라이브러리
    &lt;ul&gt;
      &lt;li&gt;커널밀도그림, 카운트그림, 캣그림, 스트립그림, 히트맵&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-python으로-시각화-프로젝트&quot;&gt;5. Python으로 시각화 프로젝트&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Mission 1. Netflix and Code
    &lt;ol&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/datasets/shivamb/netflix-shows&quot;&gt;넷플릭스 데이터&lt;/a&gt;를 활용&lt;/li&gt;
      &lt;li&gt;한국 작품은 총 얼마나 있는지?&lt;/li&gt;
      &lt;li&gt;가장 많은 작품이 올라간 국가는 어디이고, 얼마나 많은 작품이 있는지?&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221003_1.png&quot; /&gt;
&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mission 2. 가상화폐 가즈아!
    &lt;ol&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/datasets/rishidamarla/bitcoin-prices-20112015&quot;&gt;비트코인 데이터&lt;/a&gt;를 활용&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.kaggle.com/datasets/prasoonkottarathil/ethereum-historical-dataset?select=ETH_day.csv&quot;&gt;이더리움 데이터&lt;/a&gt;를 활용&lt;/li&gt;
      &lt;li&gt;비트코인의 2016.6 ~ 2017.6 기간의 5일 이동평균선 (5-MA, Moving Average) 비트코인 가격 그래프 그리기&lt;/li&gt;
      &lt;li&gt;이더리움의 2016.6 ~ 2017.6 기간의 5일 이동평균선 (5-MA, Moving Average) 이더리움 가격 그래프 그리기&lt;/li&gt;
      &lt;li&gt;비트코인과 이더리움 그래프를 함께 비교&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p align=&quot;left&quot;&gt;
    &lt;img src=&quot;../../assets/img/post_img/221003_2.png&quot; /&gt;
&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;6-3주차-돌아보기&quot;&gt;6. 3주차 돌아보기&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;기간: 2022. 10. 03 ~ 2022. 10. 08&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;9월 말 일자를 기준으로 퇴사를 하게 되어 이번 주는 데브코스 3주차이면서, 퇴사 후 첫번째 주차이기도 했다. 아무래도 회사에 있을 때보다는 시간적 그리고 심적 여유가 생긴 주차였다. 데브코스에서는 이번 주까지 해서 파이썬의 기초와 자료구조와 알고리즘, 크롤링, 기초 수학 그리고 pandas, numpy까지 학습을 마무리 했다. 아무래도 이 주제들은 과거에 접해보았거나, (끝은 없겠지만) 고민해보았던 주제여서 그나마 지금까지 따라가는데 어려움은 없었던 것 같다.&lt;/p&gt;

&lt;p&gt;지금까지 경험하지 못했던 시간적 여유 속에 참 역설적이지만 &lt;strong&gt;내 시간의 주인이 온전히 내가 된 것 같은 느낌&lt;/strong&gt;이었다. 대학교를 다닐 때는 수업과 과제 그리고 모임으로 내 시간을 스스로 통제하기 보다는 외부적 요인으로 통제 당했던 것 같다. 물론 회사를 다녔던 그 시기도 동일할 것이다. 그런데 지금은 그 시간 속에 내가 주체성을 가지고 통제하고 있는 것 같다. 지금껏 느껴보지 못했던 기분이라서 처음에는 조금 어색했지만, 성향상 가만히 있지 못하고 나름의 계획을 세우기 시작했다.&lt;/p&gt;

&lt;p&gt;학교 다니면서, 회사 다니면서 하고 싶었지만 하지 못했던 많은 것들을 조금씩 정리해두었는데, 이 시간들이 나에게는 그것들을 하기에 너무 좋은 기회라는 생각이 들었다. 완전한 (것 같으면서도 그렇지 않는) 자유 속에서 이 시간들을 아주 소중히, 그리고 뜻깊게 사용하고 싶다. 크고 작은 계획들을 세우고 기록하고 있는데, 잘 정리되고 마무리가 된다면 블로그에도 기록해두고 싶다.&lt;/p&gt;

&lt;p&gt;다음 주에는 데브코스에서는 flask와 AWS 등의 새로운 주제를 배울텐데, 또 새로운 것들을 배울 수 있는 기회에 감사하다!&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;출처: 프로그래머스 인공지능 데브코스 4기 3주차 강의 -&amp;gt; &lt;a href=&quot;https://github.com/Paul-scpark/AI-dev-course/tree/main/03%EC%A3%BC%EC%B0%A8&quot;&gt;강의 내용 정리 깃허브 링크&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
        <pubDate>Mon, 03 Oct 2022 10:00:00 +0900</pubDate>
        <link>https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-3%EC%A3%BC%EC%B0%A8/</link>
        <guid isPermaLink="true">https://paul-scpark.github.io/posts/%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%8D%B0%EB%B8%8C%EC%BD%94%EC%8A%A4-3%EC%A3%BC%EC%B0%A8/</guid>
        
        <category>AI</category>
        
        <category>Deep learning</category>
        
        <category>Machine learning</category>
        
        <category>프로그래머스</category>
        
        <category>인공지능 데브코스</category>
        
        <category>K-digital training</category>
        
        
        <category>Education</category>
        
        <category>프로그래머스 인공지능 데브코스 4기</category>
        
      </item>
    
  </channel>
</rss>